{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator, array_to_img\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "train_id = train_df.pop('id')\n",
    "    \n",
    "train_y = train_df.pop('species')\n",
    "species_encoder = preprocessing.LabelEncoder().fit(train_y)\n",
    "train_y = species_encoder.transform(train_y)\n",
    "\n",
    "numerical_scaler = preprocessing.StandardScaler().fit(train_df)\n",
    "train_numerical = numerical_scaler.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135d5ad53fa74e4aad7d9bbc6a3f4185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1584), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'datasets/images/images'\n",
    "train_img = []\n",
    "test_img = []\n",
    "# train_img_id = []\n",
    "# test_img_id = []\n",
    "desired_size = (96, 96)\n",
    "img_names = os.listdir(path)\n",
    "img_names.sort(key=lambda x:int(x.split('.')[0]))\n",
    "\n",
    "for img_name in tqdm_notebook(img_names):\n",
    "    img = cv2.imread(path+'/'+img_name, 0)\n",
    "    height, width = img.shape\n",
    "    if height < width:\n",
    "        delta_size = width - height\n",
    "        img = cv2.copyMakeBorder(img, delta_size//2, delta_size//2, \n",
    "                                    0, 0, cv2.BORDER_CONSTANT, value=[0])\n",
    "    elif height > width:\n",
    "        delta_size = height - width\n",
    "        img = cv2.copyMakeBorder(img, 0, 0, delta_size//2, delta_size//2,\n",
    "                                    cv2.BORDER_CONSTANT, value=[0])\n",
    "    img = resize(img, desired_size, mode='constant', preserve_range=True)\n",
    "    img = img/255.\n",
    "\n",
    "    img_name = int(img_name.split('.')[0])\n",
    "    if img_name in np.array(train_id):\n",
    "        train_img.append(img)\n",
    "#         train_img_id.append(img_name)\n",
    "    else:\n",
    "        test_img.append(img)\n",
    "#         test_img_id.append(img_name)\n",
    "    \n",
    "# list_img = np.array(list_img)\n",
    "# list_img_id = np.array(list_img_id).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.array(train_img)\n",
    "train_img = np.expand_dims(train_img, axis=-1)\n",
    "test_img = np.array(test_img)\n",
    "test_img = np.expand_dims(test_img, axis=-1)\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "# train_img_id = np.array(train_img_id)\n",
    "# test_img_id = np.array(test_img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, val_img, train_numerical, val_numerical, train_y, val_y = train_test_split(train_img, train_numerical, train_y,\n",
    "                                                                                     test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 123\n",
    "batch_size = 32\n",
    "\n",
    "data_gen_args = dict(rotation_range=20,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "val_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# train_generator = train_datagen.flow(train_img, train_y, batch_size=batch_size,\n",
    "#                                     seed=seed)\n",
    "# val_generator = val_datagen.flow(val_img, val_y, batch_size=batch_size,\n",
    "#                                 seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combined_generator(imgen, numerical_input):\n",
    "#     while True:\n",
    "#         for i in range(numerical_input.shape[0]):\n",
    "#             batch_img, batch_y = next(imgen)\n",
    "#             numerical_X = numerical_input[imgen.index_array]\n",
    "#             yield [batch_img, numerical_X], batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_generator(imgen, image_input, numerical_input, y):\n",
    "    batch_size = 32\n",
    "    seed = np.random.randint(1, 1000)\n",
    "    genX1 = imgen.flow(image_input, y, batch_size=batch_size, seed=seed)\n",
    "    genX2 = imgen.flow(image_input, numerical_input, batch_size=batch_size, seed=seed)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        X2i = genX2.next()\n",
    "        \n",
    "        yield [X1i[0], X2i[1]], X1i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = combined_generator(train_datagen, train_img, train_numerical, train_y)\n",
    "val_generator = combined_generator(val_datagen, val_img, val_numerical, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense, concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    image_input = Input((96, 96, 1), name='image')\n",
    "        \n",
    "    X = Conv2D(16, (3, 3), padding='same')(image_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D((2, 2))(X)\n",
    "    \n",
    "    X = Conv2D(32, (3, 3), padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D((2, 2))(X)\n",
    "    \n",
    "    X = Conv2D(64, (3, 3), padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D((2, 2))(X)\n",
    "    \n",
    "    X = Conv2D(128, (3, 3), padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D((2, 2))(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X1 = Dense(512, activation='relu')(X)\n",
    "    X1 = Dense(128, activation='relu')(X1)\n",
    "    numerical_input = Input((192, ), name='numerical')\n",
    "    X2 = Dense(128, activation='relu')(numerical_input)\n",
    "    X = concatenate([X1, X2])\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    out = Dense(99, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=[image_input, numerical_input], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              (None, 96, 96, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 96, 96, 16)   160         image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 96, 96, 16)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 48, 48, 16)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 48, 48, 32)   4640        max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 48, 48, 32)   128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 24, 24, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 24, 24, 64)   18496       max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 24, 24, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 12, 12, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 12, 12, 128)  73856       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 12, 12, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 6, 6, 128)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4608)         0           max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 512)          2359808     flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "numerical (InputLayer)          (None, 192)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          65664       dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          24704       numerical[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           dense_19[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          32896       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 99)           12771       dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,593,955\n",
      "Trainable params: 2,593,475\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = get_model()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = Adam(lr=0.01)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=30, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('NN_v5.model', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "24/24 [==============================] - ETA: 52s - loss: 5.4087 - acc: 0.0000e+ - ETA: 16s - loss: 5.3436 - acc: 0.0104   - ETA: 9s - loss: 5.1844 - acc: 0.0250 - ETA: 6s - loss: 5.1965 - acc: 0.022 - ETA: 4s - loss: 5.1086 - acc: 0.020 - ETA: 3s - loss: 5.0555 - acc: 0.017 - ETA: 2s - loss: 5.0164 - acc: 0.014 - ETA: 1s - loss: 4.9626 - acc: 0.014 - ETA: 1s - loss: 4.9133 - acc: 0.016 - ETA: 0s - loss: 4.8731 - acc: 0.023 - ETA: 0s - loss: 4.8328 - acc: 0.023 - ETA: 0s - loss: 4.7996 - acc: 0.023 - 3s 145ms/step - loss: 4.7788 - acc: 0.0234 - val_loss: 4.2375 - val_acc: 0.1024\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.23751, saving model to NN_v5.model\n",
      "Epoch 2/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.5871 - acc: 0.0000e+0 - ETA: 0s - loss: 4.3151 - acc: 0.0417    - ETA: 0s - loss: 4.2820 - acc: 0.050 - ETA: 0s - loss: 4.2924 - acc: 0.058 - ETA: 0s - loss: 4.2467 - acc: 0.069 - ETA: 0s - loss: 4.2371 - acc: 0.079 - ETA: 0s - loss: 4.2060 - acc: 0.084 - ETA: 0s - loss: 4.1964 - acc: 0.083 - ETA: 0s - loss: 4.1721 - acc: 0.085 - ETA: 0s - loss: 4.1373 - acc: 0.086 - ETA: 0s - loss: 4.1336 - acc: 0.086 - ETA: 0s - loss: 4.1075 - acc: 0.099 - 1s 44ms/step - loss: 4.0778 - acc: 0.1046 - val_loss: 3.7821 - val_acc: 0.1566\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.23751 to 3.78213, saving model to NN_v5.model\n",
      "Epoch 3/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 3.4968 - acc: 0.312 - ETA: 0s - loss: 3.6821 - acc: 0.208 - ETA: 0s - loss: 3.7205 - acc: 0.193 - ETA: 0s - loss: 3.6424 - acc: 0.209 - ETA: 0s - loss: 3.6476 - acc: 0.197 - ETA: 0s - loss: 3.6833 - acc: 0.181 - ETA: 0s - loss: 3.6693 - acc: 0.184 - ETA: 0s - loss: 3.6512 - acc: 0.182 - ETA: 0s - loss: 3.6228 - acc: 0.183 - ETA: 0s - loss: 3.6151 - acc: 0.181 - ETA: 0s - loss: 3.6040 - acc: 0.183 - ETA: 0s - loss: 3.5985 - acc: 0.182 - ETA: 0s - loss: 3.5792 - acc: 0.184 - ETA: 0s - loss: 3.5654 - acc: 0.191 - ETA: 0s - loss: 3.5653 - acc: 0.191 - ETA: 0s - loss: 3.5324 - acc: 0.201 - ETA: 0s - loss: 3.5080 - acc: 0.207 - ETA: 0s - loss: 3.4766 - acc: 0.209 - 1s 56ms/step - loss: 3.4474 - acc: 0.2140 - val_loss: 2.9196 - val_acc: 0.3675\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.78213 to 2.91964, saving model to NN_v5.model\n",
      "Epoch 4/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.8680 - acc: 0.312 - ETA: 0s - loss: 3.0736 - acc: 0.291 - ETA: 0s - loss: 3.1121 - acc: 0.268 - ETA: 0s - loss: 3.0556 - acc: 0.291 - ETA: 0s - loss: 3.0694 - acc: 0.299 - ETA: 0s - loss: 3.0422 - acc: 0.293 - ETA: 0s - loss: 3.0625 - acc: 0.288 - ETA: 0s - loss: 3.0221 - acc: 0.300 - ETA: 0s - loss: 2.9908 - acc: 0.309 - ETA: 0s - loss: 3.0113 - acc: 0.304 - ETA: 0s - loss: 3.0025 - acc: 0.302 - ETA: 0s - loss: 2.9770 - acc: 0.303 - ETA: 0s - loss: 2.9478 - acc: 0.310 - ETA: 0s - loss: 2.9413 - acc: 0.308 - ETA: 0s - loss: 2.9326 - acc: 0.307 - ETA: 0s - loss: 2.8942 - acc: 0.325 - ETA: 0s - loss: 2.8505 - acc: 0.335 - ETA: 0s - loss: 2.8177 - acc: 0.337 - ETA: 0s - loss: 2.7933 - acc: 0.347 - 1s 58ms/step - loss: 2.7522 - acc: 0.3589 - val_loss: 2.2487 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.91964 to 2.24871, saving model to NN_v5.model\n",
      "Epoch 5/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.3784 - acc: 0.406 - ETA: 0s - loss: 2.4754 - acc: 0.447 - ETA: 0s - loss: 2.4310 - acc: 0.431 - ETA: 0s - loss: 2.3371 - acc: 0.458 - ETA: 0s - loss: 2.3123 - acc: 0.473 - ETA: 0s - loss: 2.2863 - acc: 0.472 - ETA: 0s - loss: 2.2698 - acc: 0.482 - ETA: 0s - loss: 2.2294 - acc: 0.481 - ETA: 0s - loss: 2.2230 - acc: 0.483 - ETA: 0s - loss: 2.2245 - acc: 0.479 - ETA: 0s - loss: 2.2304 - acc: 0.471 - ETA: 0s - loss: 2.2179 - acc: 0.468 - ETA: 0s - loss: 2.2039 - acc: 0.468 - ETA: 0s - loss: 2.1875 - acc: 0.472 - ETA: 0s - loss: 2.1851 - acc: 0.470 - ETA: 0s - loss: 2.1959 - acc: 0.463 - ETA: 0s - loss: 2.1667 - acc: 0.471 - ETA: 0s - loss: 2.1390 - acc: 0.483 - ETA: 0s - loss: 2.1058 - acc: 0.488 - ETA: 0s - loss: 2.0837 - acc: 0.495 - 1s 57ms/step - loss: 2.0638 - acc: 0.5008 - val_loss: 1.4944 - val_acc: 0.7349\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.24871 to 1.49445, saving model to NN_v5.model\n",
      "Epoch 6/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.0451 - acc: 0.500 - ETA: 0s - loss: 1.9118 - acc: 0.500 - ETA: 0s - loss: 1.8659 - acc: 0.515 - ETA: 0s - loss: 1.8955 - acc: 0.506 - ETA: 0s - loss: 1.8358 - acc: 0.526 - ETA: 0s - loss: 1.8146 - acc: 0.526 - ETA: 0s - loss: 1.8498 - acc: 0.511 - ETA: 0s - loss: 1.8785 - acc: 0.510 - ETA: 0s - loss: 1.8414 - acc: 0.521 - ETA: 0s - loss: 1.7859 - acc: 0.542 - ETA: 0s - loss: 1.7517 - acc: 0.549 - ETA: 0s - loss: 1.7420 - acc: 0.548 - ETA: 0s - loss: 1.7264 - acc: 0.560 - ETA: 0s - loss: 1.7261 - acc: 0.560 - ETA: 0s - loss: 1.7195 - acc: 0.564 - ETA: 0s - loss: 1.7223 - acc: 0.568 - ETA: 0s - loss: 1.7114 - acc: 0.566 - ETA: 0s - loss: 1.6946 - acc: 0.567 - ETA: 0s - loss: 1.6714 - acc: 0.569 - ETA: 0s - loss: 1.6666 - acc: 0.574 - ETA: 0s - loss: 1.6549 - acc: 0.581 - 1s 60ms/step - loss: 1.6435 - acc: 0.5829 - val_loss: 1.2057 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.49445 to 1.20573, saving model to NN_v5.model\n",
      "Epoch 7/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3309 - acc: 0.656 - ETA: 0s - loss: 1.3190 - acc: 0.656 - ETA: 0s - loss: 1.2680 - acc: 0.679 - ETA: 0s - loss: 1.2813 - acc: 0.668 - ETA: 0s - loss: 1.3120 - acc: 0.666 - ETA: 0s - loss: 1.3039 - acc: 0.678 - ETA: 0s - loss: 1.3143 - acc: 0.668 - ETA: 0s - loss: 1.3022 - acc: 0.670 - ETA: 0s - loss: 1.2915 - acc: 0.675 - ETA: 0s - loss: 1.3096 - acc: 0.667 - ETA: 0s - loss: 1.3293 - acc: 0.656 - ETA: 0s - loss: 1.3079 - acc: 0.668 - ETA: 0s - loss: 1.2929 - acc: 0.671 - ETA: 0s - loss: 1.3072 - acc: 0.664 - ETA: 0s - loss: 1.3004 - acc: 0.666 - ETA: 0s - loss: 1.2903 - acc: 0.667 - ETA: 0s - loss: 1.2857 - acc: 0.666 - ETA: 0s - loss: 1.2786 - acc: 0.667 - ETA: 0s - loss: 1.2673 - acc: 0.673 - ETA: 0s - loss: 1.2524 - acc: 0.677 - ETA: 0s - loss: 1.2377 - acc: 0.681 - 2s 63ms/step - loss: 1.2352 - acc: 0.6819 - val_loss: 0.7380 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.20573 to 0.73800, saving model to NN_v5.model\n",
      "Epoch 8/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8348 - acc: 0.843 - ETA: 0s - loss: 0.9005 - acc: 0.812 - ETA: 0s - loss: 0.9174 - acc: 0.793 - ETA: 0s - loss: 0.9136 - acc: 0.786 - ETA: 0s - loss: 0.9077 - acc: 0.763 - ETA: 0s - loss: 0.9251 - acc: 0.765 - ETA: 0s - loss: 0.9277 - acc: 0.774 - ETA: 0s - loss: 0.9304 - acc: 0.781 - ETA: 0s - loss: 0.9282 - acc: 0.778 - ETA: 0s - loss: 0.9478 - acc: 0.763 - ETA: 0s - loss: 0.9400 - acc: 0.769 - ETA: 0s - loss: 0.9397 - acc: 0.767 - ETA: 0s - loss: 0.9470 - acc: 0.766 - ETA: 0s - loss: 0.9429 - acc: 0.765 - ETA: 0s - loss: 0.9288 - acc: 0.773 - ETA: 0s - loss: 0.9357 - acc: 0.770 - ETA: 0s - loss: 0.9239 - acc: 0.769 - ETA: 0s - loss: 0.9172 - acc: 0.773 - ETA: 0s - loss: 0.9266 - acc: 0.766 - ETA: 0s - loss: 0.9136 - acc: 0.771 - 1s 58ms/step - loss: 0.9134 - acc: 0.7691 - val_loss: 0.5455 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.73800 to 0.54551, saving model to NN_v5.model\n",
      "Epoch 9/300\n",
      "24/24 [==============================] - ETA: 1s - loss: 0.4277 - acc: 0.875 - ETA: 0s - loss: 0.5003 - acc: 0.875 - ETA: 0s - loss: 0.6192 - acc: 0.850 - ETA: 0s - loss: 0.6145 - acc: 0.864 - ETA: 0s - loss: 0.6046 - acc: 0.866 - ETA: 0s - loss: 0.6271 - acc: 0.851 - ETA: 0s - loss: 0.6420 - acc: 0.857 - ETA: 0s - loss: 0.6938 - acc: 0.846 - ETA: 0s - loss: 0.6884 - acc: 0.846 - ETA: 0s - loss: 0.6895 - acc: 0.846 - ETA: 0s - loss: 0.6883 - acc: 0.851 - ETA: 0s - loss: 0.6960 - acc: 0.850 - ETA: 0s - loss: 0.6894 - acc: 0.847 - ETA: 0s - loss: 0.6909 - acc: 0.843 - ETA: 0s - loss: 0.7003 - acc: 0.843 - ETA: 0s - loss: 0.7068 - acc: 0.840 - ETA: 0s - loss: 0.7175 - acc: 0.838 - ETA: 0s - loss: 0.7245 - acc: 0.834 - ETA: 0s - loss: 0.7260 - acc: 0.831 - ETA: 0s - loss: 0.7171 - acc: 0.835 - 1s 62ms/step - loss: 0.7226 - acc: 0.8282 - val_loss: 0.3995 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.54551 to 0.39955, saving model to NN_v5.model\n",
      "Epoch 10/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6873 - acc: 0.812 - ETA: 0s - loss: 0.6917 - acc: 0.833 - ETA: 0s - loss: 0.6534 - acc: 0.850 - ETA: 0s - loss: 0.6347 - acc: 0.859 - ETA: 0s - loss: 0.6002 - acc: 0.866 - ETA: 0s - loss: 0.5861 - acc: 0.867 - ETA: 0s - loss: 0.5856 - acc: 0.868 - ETA: 0s - loss: 0.5592 - acc: 0.871 - ETA: 0s - loss: 0.5457 - acc: 0.872 - ETA: 0s - loss: 0.5529 - acc: 0.864 - ETA: 0s - loss: 0.5652 - acc: 0.858 - ETA: 0s - loss: 0.5700 - acc: 0.854 - ETA: 0s - loss: 0.5676 - acc: 0.856 - ETA: 0s - loss: 0.5705 - acc: 0.857 - ETA: 0s - loss: 0.5790 - acc: 0.856 - ETA: 0s - loss: 0.5890 - acc: 0.852 - ETA: 0s - loss: 0.5856 - acc: 0.856 - ETA: 0s - loss: 0.5671 - acc: 0.864 - ETA: 0s - loss: 0.5641 - acc: 0.866 - ETA: 0s - loss: 0.5705 - acc: 0.863 - ETA: 0s - loss: 0.5738 - acc: 0.861 - 1s 58ms/step - loss: 0.5675 - acc: 0.8671 - val_loss: 0.2933 - val_acc: 0.9578\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.39955 to 0.29327, saving model to NN_v5.model\n",
      "Epoch 11/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6526 - acc: 0.750 - ETA: 0s - loss: 0.6394 - acc: 0.802 - ETA: 0s - loss: 0.6015 - acc: 0.820 - ETA: 0s - loss: 0.5632 - acc: 0.837 - ETA: 0s - loss: 0.5590 - acc: 0.838 - ETA: 0s - loss: 0.5323 - acc: 0.852 - ETA: 0s - loss: 0.5406 - acc: 0.847 - ETA: 0s - loss: 0.5307 - acc: 0.850 - ETA: 0s - loss: 0.5283 - acc: 0.846 - ETA: 0s - loss: 0.5201 - acc: 0.851 - ETA: 0s - loss: 0.5287 - acc: 0.848 - ETA: 0s - loss: 0.5265 - acc: 0.850 - ETA: 0s - loss: 0.5198 - acc: 0.856 - ETA: 0s - loss: 0.5230 - acc: 0.853 - ETA: 0s - loss: 0.5306 - acc: 0.854 - ETA: 0s - loss: 0.5228 - acc: 0.859 - ETA: 0s - loss: 0.5231 - acc: 0.858 - ETA: 0s - loss: 0.5137 - acc: 0.862 - ETA: 0s - loss: 0.5078 - acc: 0.867 - ETA: 0s - loss: 0.5045 - acc: 0.867 - ETA: 0s - loss: 0.4955 - acc: 0.870 - 1s 59ms/step - loss: 0.4861 - acc: 0.8750 - val_loss: 0.3040 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.29327\n",
      "Epoch 12/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9133 - acc: 0.833 - ETA: 0s - loss: 0.5235 - acc: 0.902 - ETA: 0s - loss: 0.4657 - acc: 0.911 - ETA: 0s - loss: 0.4370 - acc: 0.922 - ETA: 0s - loss: 0.4363 - acc: 0.920 - ETA: 0s - loss: 0.4521 - acc: 0.909 - ETA: 0s - loss: 0.4638 - acc: 0.897 - ETA: 0s - loss: 0.4692 - acc: 0.894 - ETA: 0s - loss: 0.4726 - acc: 0.892 - ETA: 0s - loss: 0.4629 - acc: 0.896 - ETA: 0s - loss: 0.4608 - acc: 0.897 - ETA: 0s - loss: 0.4627 - acc: 0.900 - ETA: 0s - loss: 0.4520 - acc: 0.903 - ETA: 0s - loss: 0.4542 - acc: 0.899 - ETA: 0s - loss: 0.4532 - acc: 0.895 - ETA: 0s - loss: 0.4552 - acc: 0.896 - ETA: 0s - loss: 0.4553 - acc: 0.895 - ETA: 0s - loss: 0.4624 - acc: 0.894 - ETA: 0s - loss: 0.4559 - acc: 0.894 - ETA: 0s - loss: 0.4586 - acc: 0.890 - ETA: 0s - loss: 0.4546 - acc: 0.890 - 1s 60ms/step - loss: 0.4469 - acc: 0.8928 - val_loss: 0.2762 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.29327 to 0.27617, saving model to NN_v5.model\n",
      "Epoch 13/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3376 - acc: 0.875 - ETA: 0s - loss: 0.3871 - acc: 0.899 - ETA: 0s - loss: 0.3961 - acc: 0.902 - ETA: 0s - loss: 0.3854 - acc: 0.913 - ETA: 0s - loss: 0.3594 - acc: 0.916 - ETA: 0s - loss: 0.3335 - acc: 0.927 - ETA: 0s - loss: 0.3408 - acc: 0.931 - ETA: 0s - loss: 0.3353 - acc: 0.932 - ETA: 0s - loss: 0.3243 - acc: 0.938 - ETA: 0s - loss: 0.3210 - acc: 0.935 - ETA: 0s - loss: 0.3196 - acc: 0.933 - ETA: 0s - loss: 0.3277 - acc: 0.927 - ETA: 0s - loss: 0.3329 - acc: 0.925 - ETA: 0s - loss: 0.3354 - acc: 0.924 - ETA: 0s - loss: 0.3332 - acc: 0.925 - ETA: 0s - loss: 0.3356 - acc: 0.922 - ETA: 0s - loss: 0.3309 - acc: 0.924 - ETA: 0s - loss: 0.3367 - acc: 0.922 - ETA: 0s - loss: 0.3353 - acc: 0.921 - ETA: 0s - loss: 0.3394 - acc: 0.916 - ETA: 0s - loss: 0.3397 - acc: 0.917 - 1s 59ms/step - loss: 0.3402 - acc: 0.9171 - val_loss: 0.1660 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.27617 to 0.16599, saving model to NN_v5.model\n",
      "Epoch 14/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2693 - acc: 0.968 - ETA: 0s - loss: 0.2559 - acc: 0.941 - ETA: 0s - loss: 0.2865 - acc: 0.933 - ETA: 0s - loss: 0.2656 - acc: 0.947 - ETA: 0s - loss: 0.3007 - acc: 0.931 - ETA: 0s - loss: 0.3194 - acc: 0.928 - ETA: 0s - loss: 0.3245 - acc: 0.926 - ETA: 0s - loss: 0.3253 - acc: 0.927 - ETA: 0s - loss: 0.3347 - acc: 0.925 - ETA: 0s - loss: 0.3424 - acc: 0.923 - ETA: 0s - loss: 0.3530 - acc: 0.918 - ETA: 0s - loss: 0.3521 - acc: 0.919 - ETA: 0s - loss: 0.3516 - acc: 0.918 - ETA: 0s - loss: 0.3507 - acc: 0.916 - ETA: 0s - loss: 0.3450 - acc: 0.919 - ETA: 0s - loss: 0.3353 - acc: 0.923 - ETA: 0s - loss: 0.3421 - acc: 0.920 - ETA: 0s - loss: 0.3406 - acc: 0.921 - ETA: 0s - loss: 0.3438 - acc: 0.920 - ETA: 0s - loss: 0.3387 - acc: 0.923 - 1s 58ms/step - loss: 0.3335 - acc: 0.9249 - val_loss: 0.1377 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.16599 to 0.13766, saving model to NN_v5.model\n",
      "Epoch 15/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3810 - acc: 0.906 - ETA: 0s - loss: 0.3372 - acc: 0.927 - ETA: 0s - loss: 0.2792 - acc: 0.947 - ETA: 0s - loss: 0.2951 - acc: 0.941 - ETA: 0s - loss: 0.2922 - acc: 0.944 - ETA: 0s - loss: 0.2733 - acc: 0.947 - ETA: 0s - loss: 0.2732 - acc: 0.942 - ETA: 0s - loss: 0.2650 - acc: 0.947 - ETA: 0s - loss: 0.2656 - acc: 0.947 - ETA: 0s - loss: 0.2657 - acc: 0.948 - ETA: 0s - loss: 0.2621 - acc: 0.947 - ETA: 0s - loss: 0.2659 - acc: 0.945 - ETA: 0s - loss: 0.2678 - acc: 0.944 - ETA: 0s - loss: 0.2616 - acc: 0.947 - ETA: 0s - loss: 0.2566 - acc: 0.949 - ETA: 0s - loss: 0.2527 - acc: 0.948 - ETA: 0s - loss: 0.2563 - acc: 0.946 - ETA: 0s - loss: 0.2528 - acc: 0.947 - ETA: 0s - loss: 0.2562 - acc: 0.945 - ETA: 0s - loss: 0.2556 - acc: 0.946 - 1s 59ms/step - loss: 0.2560 - acc: 0.9449 - val_loss: 0.1986 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13766\n",
      "Epoch 16/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2620 - acc: 0.937 - ETA: 0s - loss: 0.2110 - acc: 0.968 - ETA: 0s - loss: 0.1993 - acc: 0.960 - ETA: 0s - loss: 0.2519 - acc: 0.935 - ETA: 0s - loss: 0.2496 - acc: 0.930 - ETA: 0s - loss: 0.2566 - acc: 0.918 - ETA: 0s - loss: 0.2471 - acc: 0.928 - ETA: 0s - loss: 0.2408 - acc: 0.936 - ETA: 0s - loss: 0.2387 - acc: 0.933 - ETA: 0s - loss: 0.2403 - acc: 0.930 - ETA: 0s - loss: 0.2451 - acc: 0.928 - ETA: 0s - loss: 0.2483 - acc: 0.927 - ETA: 0s - loss: 0.2453 - acc: 0.930 - ETA: 0s - loss: 0.2557 - acc: 0.928 - ETA: 0s - loss: 0.2505 - acc: 0.932 - ETA: 0s - loss: 0.2425 - acc: 0.935 - ETA: 0s - loss: 0.2363 - acc: 0.936 - ETA: 0s - loss: 0.2406 - acc: 0.937 - ETA: 0s - loss: 0.2397 - acc: 0.938 - ETA: 0s - loss: 0.2403 - acc: 0.937 - ETA: 0s - loss: 0.2386 - acc: 0.938 - ETA: 0s - loss: 0.2455 - acc: 0.939 - 1s 61ms/step - loss: 0.2430 - acc: 0.9397 - val_loss: 0.1144 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13766 to 0.11439, saving model to NN_v5.model\n",
      "Epoch 17/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2084 - acc: 0.968 - ETA: 0s - loss: 0.2138 - acc: 0.968 - ETA: 0s - loss: 0.2111 - acc: 0.962 - ETA: 0s - loss: 0.2283 - acc: 0.947 - ETA: 0s - loss: 0.2381 - acc: 0.946 - ETA: 0s - loss: 0.2600 - acc: 0.941 - ETA: 0s - loss: 0.2493 - acc: 0.944 - ETA: 0s - loss: 0.2461 - acc: 0.948 - ETA: 0s - loss: 0.2391 - acc: 0.947 - ETA: 0s - loss: 0.2342 - acc: 0.951 - ETA: 0s - loss: 0.2244 - acc: 0.954 - ETA: 0s - loss: 0.2241 - acc: 0.953 - ETA: 0s - loss: 0.2216 - acc: 0.952 - ETA: 0s - loss: 0.2214 - acc: 0.948 - ETA: 0s - loss: 0.2171 - acc: 0.949 - ETA: 0s - loss: 0.2215 - acc: 0.947 - ETA: 0s - loss: 0.2248 - acc: 0.946 - ETA: 0s - loss: 0.2218 - acc: 0.946 - ETA: 0s - loss: 0.2167 - acc: 0.947 - 1s 60ms/step - loss: 0.2242 - acc: 0.9419 - val_loss: 0.1276 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11439\n",
      "Epoch 18/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.2426 - acc: 0.906 - ETA: 0s - loss: 0.1728 - acc: 0.968 - ETA: 0s - loss: 0.1838 - acc: 0.953 - ETA: 0s - loss: 0.1781 - acc: 0.950 - ETA: 1s - loss: 0.2046 - acc: 0.947 - ETA: 0s - loss: 0.2091 - acc: 0.947 - ETA: 0s - loss: 0.2035 - acc: 0.950 - ETA: 0s - loss: 0.2121 - acc: 0.949 - ETA: 0s - loss: 0.2112 - acc: 0.950 - ETA: 0s - loss: 0.2153 - acc: 0.949 - ETA: 0s - loss: 0.2161 - acc: 0.943 - ETA: 0s - loss: 0.2064 - acc: 0.947 - ETA: 0s - loss: 0.2180 - acc: 0.943 - ETA: 0s - loss: 0.2272 - acc: 0.938 - ETA: 0s - loss: 0.2248 - acc: 0.938 - ETA: 0s - loss: 0.2211 - acc: 0.938 - ETA: 0s - loss: 0.2196 - acc: 0.938 - ETA: 0s - loss: 0.2128 - acc: 0.941 - ETA: 0s - loss: 0.2171 - acc: 0.940 - ETA: 0s - loss: 0.2224 - acc: 0.937 - ETA: 0s - loss: 0.2223 - acc: 0.938 - 2s 64ms/step - loss: 0.2173 - acc: 0.9410 - val_loss: 0.1217 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11439\n",
      "Epoch 19/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1926 - acc: 0.937 - ETA: 0s - loss: 0.2372 - acc: 0.937 - ETA: 0s - loss: 0.2224 - acc: 0.937 - ETA: 0s - loss: 0.2058 - acc: 0.943 - ETA: 0s - loss: 0.1909 - acc: 0.947 - ETA: 0s - loss: 0.2058 - acc: 0.946 - ETA: 0s - loss: 0.2026 - acc: 0.950 - ETA: 0s - loss: 0.1918 - acc: 0.952 - ETA: 0s - loss: 0.1834 - acc: 0.956 - ETA: 0s - loss: 0.1954 - acc: 0.952 - ETA: 0s - loss: 0.1943 - acc: 0.951 - ETA: 0s - loss: 0.1991 - acc: 0.950 - ETA: 0s - loss: 0.2022 - acc: 0.951 - ETA: 0s - loss: 0.1994 - acc: 0.952 - ETA: 0s - loss: 0.2018 - acc: 0.951 - ETA: 0s - loss: 0.1977 - acc: 0.952 - ETA: 0s - loss: 0.1949 - acc: 0.955 - ETA: 0s - loss: 0.1947 - acc: 0.955 - ETA: 0s - loss: 0.1922 - acc: 0.956 - ETA: 0s - loss: 0.1957 - acc: 0.954 - ETA: 0s - loss: 0.1936 - acc: 0.954 - 1s 62ms/step - loss: 0.1942 - acc: 0.9553 - val_loss: 0.1093 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11439 to 0.10932, saving model to NN_v5.model\n",
      "Epoch 20/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0670 - acc: 1.000 - ETA: 0s - loss: 0.1717 - acc: 0.947 - ETA: 0s - loss: 0.1737 - acc: 0.950 - ETA: 0s - loss: 0.1735 - acc: 0.953 - ETA: 0s - loss: 0.1699 - acc: 0.959 - ETA: 0s - loss: 0.1651 - acc: 0.960 - ETA: 0s - loss: 0.1867 - acc: 0.952 - ETA: 0s - loss: 0.1885 - acc: 0.953 - ETA: 0s - loss: 0.1944 - acc: 0.949 - ETA: 0s - loss: 0.1969 - acc: 0.948 - ETA: 0s - loss: 0.1944 - acc: 0.947 - ETA: 0s - loss: 0.1895 - acc: 0.951 - ETA: 0s - loss: 0.1901 - acc: 0.952 - ETA: 0s - loss: 0.1872 - acc: 0.951 - ETA: 0s - loss: 0.1868 - acc: 0.952 - ETA: 0s - loss: 0.1819 - acc: 0.955 - ETA: 0s - loss: 0.1745 - acc: 0.957 - ETA: 0s - loss: 0.1711 - acc: 0.957 - ETA: 0s - loss: 0.1703 - acc: 0.956 - ETA: 0s - loss: 0.1703 - acc: 0.957 - 1s 60ms/step - loss: 0.1677 - acc: 0.9592 - val_loss: 0.1254 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10932\n",
      "Epoch 21/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1759 - acc: 0.968 - ETA: 0s - loss: 0.1919 - acc: 0.947 - ETA: 0s - loss: 0.2008 - acc: 0.943 - ETA: 0s - loss: 0.1775 - acc: 0.953 - ETA: 0s - loss: 0.1810 - acc: 0.946 - ETA: 0s - loss: 0.1790 - acc: 0.949 - ETA: 0s - loss: 0.1845 - acc: 0.951 - ETA: 0s - loss: 0.1672 - acc: 0.956 - ETA: 0s - loss: 0.1792 - acc: 0.954 - ETA: 0s - loss: 0.1781 - acc: 0.953 - ETA: 0s - loss: 0.1812 - acc: 0.950 - ETA: 0s - loss: 0.1760 - acc: 0.953 - ETA: 0s - loss: 0.1707 - acc: 0.954 - ETA: 0s - loss: 0.1642 - acc: 0.957 - ETA: 0s - loss: 0.1585 - acc: 0.960 - ETA: 0s - loss: 0.1545 - acc: 0.962 - ETA: 0s - loss: 0.1545 - acc: 0.960 - ETA: 0s - loss: 0.1527 - acc: 0.962 - ETA: 0s - loss: 0.1506 - acc: 0.962 - 1s 59ms/step - loss: 0.1478 - acc: 0.9644 - val_loss: 0.1010 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10932 to 0.10101, saving model to NN_v5.model\n",
      "Epoch 22/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2196 - acc: 0.937 - ETA: 0s - loss: 0.1795 - acc: 0.947 - ETA: 0s - loss: 0.1915 - acc: 0.950 - ETA: 0s - loss: 0.1818 - acc: 0.953 - ETA: 0s - loss: 0.1784 - acc: 0.955 - ETA: 0s - loss: 0.1658 - acc: 0.957 - ETA: 0s - loss: 0.1580 - acc: 0.958 - ETA: 0s - loss: 0.1458 - acc: 0.962 - ETA: 0s - loss: 0.1557 - acc: 0.956 - ETA: 0s - loss: 0.1537 - acc: 0.955 - ETA: 0s - loss: 0.1623 - acc: 0.953 - ETA: 0s - loss: 0.1570 - acc: 0.954 - ETA: 0s - loss: 0.1510 - acc: 0.957 - ETA: 0s - loss: 0.1479 - acc: 0.958 - ETA: 0s - loss: 0.1550 - acc: 0.957 - ETA: 0s - loss: 0.1521 - acc: 0.957 - ETA: 0s - loss: 0.1493 - acc: 0.958 - ETA: 0s - loss: 0.1489 - acc: 0.960 - ETA: 0s - loss: 0.1482 - acc: 0.960 - ETA: 0s - loss: 0.1451 - acc: 0.962 - 1s 60ms/step - loss: 0.1450 - acc: 0.9627 - val_loss: 0.1245 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10101\n",
      "Epoch 23/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0916 - acc: 0.968 - ETA: 0s - loss: 0.1583 - acc: 0.947 - ETA: 0s - loss: 0.1308 - acc: 0.960 - ETA: 0s - loss: 0.1515 - acc: 0.956 - ETA: 0s - loss: 0.1480 - acc: 0.958 - ETA: 0s - loss: 0.1428 - acc: 0.964 - ETA: 0s - loss: 0.1463 - acc: 0.964 - ETA: 0s - loss: 0.1425 - acc: 0.965 - ETA: 0s - loss: 0.1398 - acc: 0.965 - ETA: 0s - loss: 0.1338 - acc: 0.968 - ETA: 0s - loss: 0.1302 - acc: 0.968 - ETA: 0s - loss: 0.1268 - acc: 0.971 - ETA: 0s - loss: 0.1242 - acc: 0.970 - ETA: 0s - loss: 0.1246 - acc: 0.968 - ETA: 0s - loss: 0.1214 - acc: 0.970 - ETA: 0s - loss: 0.1240 - acc: 0.970 - ETA: 0s - loss: 0.1199 - acc: 0.972 - ETA: 0s - loss: 0.1195 - acc: 0.971 - ETA: 0s - loss: 0.1186 - acc: 0.971 - ETA: 0s - loss: 0.1189 - acc: 0.971 - ETA: 0s - loss: 0.1216 - acc: 0.970 - 1s 61ms/step - loss: 0.1212 - acc: 0.9713 - val_loss: 0.0774 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.10101 to 0.07743, saving model to NN_v5.model\n",
      "Epoch 24/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1722 - acc: 0.968 - ETA: 0s - loss: 0.1661 - acc: 0.958 - ETA: 0s - loss: 0.1352 - acc: 0.968 - ETA: 0s - loss: 0.1231 - acc: 0.968 - ETA: 0s - loss: 0.1250 - acc: 0.968 - ETA: 0s - loss: 0.1254 - acc: 0.968 - ETA: 0s - loss: 0.1354 - acc: 0.965 - ETA: 0s - loss: 0.1484 - acc: 0.959 - ETA: 0s - loss: 0.1469 - acc: 0.957 - ETA: 0s - loss: 0.1434 - acc: 0.958 - ETA: 0s - loss: 0.1372 - acc: 0.961 - ETA: 0s - loss: 0.1323 - acc: 0.961 - ETA: 0s - loss: 0.1286 - acc: 0.964 - ETA: 0s - loss: 0.1268 - acc: 0.966 - ETA: 0s - loss: 0.1235 - acc: 0.968 - ETA: 0s - loss: 0.1234 - acc: 0.966 - ETA: 0s - loss: 0.1214 - acc: 0.968 - ETA: 0s - loss: 0.1175 - acc: 0.969 - ETA: 0s - loss: 0.1220 - acc: 0.971 - ETA: 0s - loss: 0.1203 - acc: 0.972 - 1s 60ms/step - loss: 0.1196 - acc: 0.9735 - val_loss: 0.0696 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.07743 to 0.06960, saving model to NN_v5.model\n",
      "Epoch 25/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1076 - acc: 0.937 - ETA: 0s - loss: 0.0839 - acc: 0.979 - ETA: 0s - loss: 0.0867 - acc: 0.981 - ETA: 0s - loss: 0.0758 - acc: 0.984 - ETA: 0s - loss: 0.0862 - acc: 0.973 - ETA: 0s - loss: 0.0881 - acc: 0.976 - ETA: 0s - loss: 0.0848 - acc: 0.975 - ETA: 0s - loss: 0.0838 - acc: 0.975 - ETA: 0s - loss: 0.0810 - acc: 0.977 - ETA: 0s - loss: 0.0824 - acc: 0.976 - ETA: 0s - loss: 0.0837 - acc: 0.976 - ETA: 0s - loss: 0.0783 - acc: 0.979 - ETA: 0s - loss: 0.0779 - acc: 0.978 - ETA: 0s - loss: 0.0787 - acc: 0.979 - ETA: 0s - loss: 0.0787 - acc: 0.979 - ETA: 0s - loss: 0.0778 - acc: 0.980 - ETA: 0s - loss: 0.0769 - acc: 0.981 - ETA: 0s - loss: 0.0823 - acc: 0.979 - ETA: 0s - loss: 0.0845 - acc: 0.980 - ETA: 0s - loss: 0.0848 - acc: 0.979 - 1s 59ms/step - loss: 0.0850 - acc: 0.9792 - val_loss: 0.0632 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06960 to 0.06321, saving model to NN_v5.model\n",
      "Epoch 26/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0550 - acc: 1.000 - ETA: 0s - loss: 0.1442 - acc: 0.958 - ETA: 0s - loss: 0.1252 - acc: 0.968 - ETA: 0s - loss: 0.1241 - acc: 0.968 - ETA: 0s - loss: 0.1390 - acc: 0.959 - ETA: 0s - loss: 0.1296 - acc: 0.960 - ETA: 0s - loss: 0.1275 - acc: 0.961 - ETA: 0s - loss: 0.1256 - acc: 0.962 - ETA: 0s - loss: 0.1272 - acc: 0.963 - ETA: 0s - loss: 0.1419 - acc: 0.955 - ETA: 0s - loss: 0.1363 - acc: 0.956 - ETA: 0s - loss: 0.1323 - acc: 0.959 - ETA: 0s - loss: 0.1307 - acc: 0.959 - ETA: 0s - loss: 0.1287 - acc: 0.962 - ETA: 0s - loss: 0.1262 - acc: 0.962 - ETA: 0s - loss: 0.1234 - acc: 0.962 - ETA: 0s - loss: 0.1299 - acc: 0.959 - ETA: 0s - loss: 0.1368 - acc: 0.958 - ETA: 0s - loss: 0.1360 - acc: 0.959 - ETA: 0s - loss: 0.1331 - acc: 0.961 - 1s 60ms/step - loss: 0.1312 - acc: 0.9614 - val_loss: 0.0635 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.06321\n",
      "Epoch 27/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0188 - acc: 1.000 - ETA: 0s - loss: 0.0857 - acc: 0.979 - ETA: 0s - loss: 0.0922 - acc: 0.976 - ETA: 0s - loss: 0.0853 - acc: 0.981 - ETA: 0s - loss: 0.0931 - acc: 0.979 - ETA: 0s - loss: 0.0879 - acc: 0.982 - ETA: 0s - loss: 0.0841 - acc: 0.984 - ETA: 0s - loss: 0.0873 - acc: 0.986 - ETA: 0s - loss: 0.0865 - acc: 0.987 - ETA: 0s - loss: 0.0837 - acc: 0.988 - ETA: 0s - loss: 0.0856 - acc: 0.987 - ETA: 0s - loss: 0.0809 - acc: 0.988 - ETA: 0s - loss: 0.0795 - acc: 0.988 - ETA: 0s - loss: 0.0810 - acc: 0.987 - ETA: 0s - loss: 0.0786 - acc: 0.987 - ETA: 0s - loss: 0.0781 - acc: 0.987 - ETA: 0s - loss: 0.0815 - acc: 0.986 - ETA: 0s - loss: 0.0841 - acc: 0.987 - ETA: 0s - loss: 0.0863 - acc: 0.986 - ETA: 0s - loss: 0.0881 - acc: 0.985 - ETA: 0s - loss: 0.0948 - acc: 0.983 - 1s 60ms/step - loss: 0.0938 - acc: 0.9831 - val_loss: 0.0885 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.06321\n",
      "Epoch 28/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1339 - acc: 0.968 - ETA: 1s - loss: 0.1096 - acc: 0.968 - ETA: 1s - loss: 0.1193 - acc: 0.960 - ETA: 1s - loss: 0.1091 - acc: 0.968 - ETA: 1s - loss: 0.1165 - acc: 0.968 - ETA: 1s - loss: 0.1243 - acc: 0.968 - ETA: 0s - loss: 0.1190 - acc: 0.972 - ETA: 0s - loss: 0.1159 - acc: 0.975 - ETA: 0s - loss: 0.1084 - acc: 0.978 - ETA: 0s - loss: 0.1064 - acc: 0.977 - ETA: 0s - loss: 0.1033 - acc: 0.979 - ETA: 0s - loss: 0.1051 - acc: 0.976 - ETA: 0s - loss: 0.0996 - acc: 0.977 - ETA: 0s - loss: 0.1030 - acc: 0.977 - ETA: 0s - loss: 0.1063 - acc: 0.976 - ETA: 0s - loss: 0.1035 - acc: 0.976 - ETA: 0s - loss: 0.1028 - acc: 0.978 - ETA: 0s - loss: 0.1045 - acc: 0.976 - ETA: 0s - loss: 0.1009 - acc: 0.977 - ETA: 0s - loss: 0.1023 - acc: 0.978 - ETA: 0s - loss: 0.0992 - acc: 0.979 - 2s 64ms/step - loss: 0.0983 - acc: 0.9800 - val_loss: 0.0682 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.06321\n",
      "Epoch 29/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1392 - acc: 0.937 - ETA: 1s - loss: 0.1932 - acc: 0.927 - ETA: 1s - loss: 0.1643 - acc: 0.937 - ETA: 1s - loss: 0.1453 - acc: 0.943 - ETA: 0s - loss: 0.1290 - acc: 0.953 - ETA: 0s - loss: 0.1203 - acc: 0.955 - ETA: 0s - loss: 0.1125 - acc: 0.960 - ETA: 0s - loss: 0.1095 - acc: 0.961 - ETA: 0s - loss: 0.1005 - acc: 0.965 - ETA: 0s - loss: 0.0949 - acc: 0.968 - ETA: 0s - loss: 0.0939 - acc: 0.968 - ETA: 0s - loss: 0.0902 - acc: 0.971 - ETA: 0s - loss: 0.0887 - acc: 0.972 - ETA: 0s - loss: 0.0848 - acc: 0.974 - ETA: 0s - loss: 0.0832 - acc: 0.976 - ETA: 0s - loss: 0.0816 - acc: 0.977 - ETA: 0s - loss: 0.0796 - acc: 0.978 - ETA: 0s - loss: 0.0802 - acc: 0.977 - ETA: 0s - loss: 0.0781 - acc: 0.978 - ETA: 0s - loss: 0.0755 - acc: 0.979 - 1s 61ms/step - loss: 0.0747 - acc: 0.9805 - val_loss: 0.0605 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.06321 to 0.06046, saving model to NN_v5.model\n",
      "Epoch 30/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0809 - acc: 0.968 - ETA: 1s - loss: 0.0760 - acc: 0.968 - ETA: 0s - loss: 0.0868 - acc: 0.953 - ETA: 1s - loss: 0.0747 - acc: 0.962 - ETA: 1s - loss: 0.0770 - acc: 0.963 - ETA: 0s - loss: 0.0761 - acc: 0.964 - ETA: 0s - loss: 0.0872 - acc: 0.964 - ETA: 0s - loss: 0.0930 - acc: 0.965 - ETA: 0s - loss: 0.0913 - acc: 0.968 - ETA: 0s - loss: 0.0853 - acc: 0.971 - ETA: 0s - loss: 0.0857 - acc: 0.971 - ETA: 0s - loss: 0.0823 - acc: 0.973 - ETA: 0s - loss: 0.0790 - acc: 0.975 - ETA: 0s - loss: 0.0817 - acc: 0.975 - ETA: 0s - loss: 0.0798 - acc: 0.976 - ETA: 0s - loss: 0.0788 - acc: 0.976 - ETA: 0s - loss: 0.0754 - acc: 0.977 - ETA: 0s - loss: 0.0853 - acc: 0.972 - ETA: 0s - loss: 0.0846 - acc: 0.972 - ETA: 0s - loss: 0.0851 - acc: 0.971 - ETA: 0s - loss: 0.0827 - acc: 0.972 - 1s 62ms/step - loss: 0.0808 - acc: 0.9722 - val_loss: 0.0418 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.06046 to 0.04179, saving model to NN_v5.model\n",
      "Epoch 31/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0733 - acc: 0.968 - ETA: 0s - loss: 0.0644 - acc: 0.979 - ETA: 0s - loss: 0.0921 - acc: 0.975 - ETA: 0s - loss: 0.0857 - acc: 0.979 - ETA: 0s - loss: 0.0878 - acc: 0.982 - ETA: 0s - loss: 0.0881 - acc: 0.984 - ETA: 0s - loss: 0.0873 - acc: 0.982 - ETA: 0s - loss: 0.0906 - acc: 0.981 - ETA: 0s - loss: 0.0847 - acc: 0.983 - ETA: 0s - loss: 0.0830 - acc: 0.981 - ETA: 0s - loss: 0.0807 - acc: 0.980 - ETA: 0s - loss: 0.0773 - acc: 0.982 - ETA: 0s - loss: 0.0869 - acc: 0.981 - ETA: 0s - loss: 0.0840 - acc: 0.982 - ETA: 0s - loss: 0.0821 - acc: 0.983 - ETA: 0s - loss: 0.0813 - acc: 0.984 - ETA: 0s - loss: 0.0798 - acc: 0.985 - ETA: 0s - loss: 0.0790 - acc: 0.985 - ETA: 0s - loss: 0.0776 - acc: 0.985 - ETA: 0s - loss: 0.0764 - acc: 0.985 - ETA: 0s - loss: 0.0754 - acc: 0.986 - 1s 62ms/step - loss: 0.0754 - acc: 0.9870 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.04179\n",
      "Epoch 32/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0810 - acc: 1.000 - ETA: 0s - loss: 0.0654 - acc: 1.000 - ETA: 1s - loss: 0.0624 - acc: 1.000 - ETA: 1s - loss: 0.0704 - acc: 1.000 - ETA: 1s - loss: 0.0700 - acc: 0.994 - ETA: 1s - loss: 0.0740 - acc: 0.991 - ETA: 0s - loss: 0.0709 - acc: 0.992 - ETA: 0s - loss: 0.0711 - acc: 0.993 - ETA: 0s - loss: 0.0696 - acc: 0.993 - ETA: 0s - loss: 0.0669 - acc: 0.994 - ETA: 0s - loss: 0.0685 - acc: 0.992 - ETA: 0s - loss: 0.0660 - acc: 0.992 - ETA: 0s - loss: 0.0728 - acc: 0.988 - ETA: 0s - loss: 0.0743 - acc: 0.985 - ETA: 0s - loss: 0.0713 - acc: 0.985 - ETA: 0s - loss: 0.0719 - acc: 0.984 - ETA: 0s - loss: 0.0742 - acc: 0.983 - ETA: 0s - loss: 0.0772 - acc: 0.981 - ETA: 0s - loss: 0.0744 - acc: 0.981 - ETA: 0s - loss: 0.0730 - acc: 0.982 - 1s 61ms/step - loss: 0.0712 - acc: 0.9831 - val_loss: 0.0646 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04179\n",
      "Epoch 33/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0395 - acc: 1.000 - ETA: 0s - loss: 0.0597 - acc: 0.989 - ETA: 0s - loss: 0.0591 - acc: 0.992 - ETA: 0s - loss: 0.0573 - acc: 0.993 - ETA: 0s - loss: 0.0524 - acc: 0.994 - ETA: 0s - loss: 0.0488 - acc: 0.995 - ETA: 0s - loss: 0.0456 - acc: 0.996 - ETA: 0s - loss: 0.0424 - acc: 0.996 - ETA: 0s - loss: 0.0408 - acc: 0.996 - ETA: 0s - loss: 0.0421 - acc: 0.994 - ETA: 0s - loss: 0.0401 - acc: 0.994 - ETA: 0s - loss: 0.0387 - acc: 0.995 - ETA: 0s - loss: 0.0398 - acc: 0.995 - ETA: 0s - loss: 0.0407 - acc: 0.995 - ETA: 0s - loss: 0.0413 - acc: 0.996 - ETA: 0s - loss: 0.0409 - acc: 0.996 - ETA: 0s - loss: 0.0412 - acc: 0.996 - ETA: 0s - loss: 0.0451 - acc: 0.995 - ETA: 0s - loss: 0.0458 - acc: 0.995 - ETA: 0s - loss: 0.0462 - acc: 0.995 - ETA: 0s - loss: 0.0456 - acc: 0.995 - ETA: 0s - loss: 0.0464 - acc: 0.994 - 2s 78ms/step - loss: 0.0452 - acc: 0.9948 - val_loss: 0.0639 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.04179\n",
      "Epoch 34/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.1693 - acc: 0.906 - ETA: 1s - loss: 0.0947 - acc: 0.953 - ETA: 1s - loss: 0.0690 - acc: 0.968 - ETA: 1s - loss: 0.0618 - acc: 0.976 - ETA: 1s - loss: 0.0675 - acc: 0.975 - ETA: 1s - loss: 0.0778 - acc: 0.974 - ETA: 1s - loss: 0.0706 - acc: 0.977 - ETA: 1s - loss: 0.0694 - acc: 0.976 - ETA: 1s - loss: 0.0641 - acc: 0.979 - ETA: 0s - loss: 0.0611 - acc: 0.981 - ETA: 0s - loss: 0.0595 - acc: 0.983 - ETA: 0s - loss: 0.0587 - acc: 0.984 - ETA: 0s - loss: 0.0565 - acc: 0.985 - ETA: 0s - loss: 0.0549 - acc: 0.986 - ETA: 0s - loss: 0.0539 - acc: 0.987 - ETA: 0s - loss: 0.0551 - acc: 0.986 - ETA: 0s - loss: 0.0556 - acc: 0.987 - ETA: 0s - loss: 0.0576 - acc: 0.984 - ETA: 0s - loss: 0.0569 - acc: 0.985 - ETA: 0s - loss: 0.0549 - acc: 0.985 - ETA: 0s - loss: 0.0546 - acc: 0.986 - ETA: 0s - loss: 0.0556 - acc: 0.985 - ETA: 0s - loss: 0.0548 - acc: 0.986 - 2s 75ms/step - loss: 0.0544 - acc: 0.9870 - val_loss: 0.0653 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04179\n",
      "Epoch 35/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0219 - acc: 1.000 - ETA: 1s - loss: 0.0435 - acc: 1.000 - ETA: 1s - loss: 0.0424 - acc: 1.000 - ETA: 1s - loss: 0.0503 - acc: 1.000 - ETA: 1s - loss: 0.0506 - acc: 1.000 - ETA: 1s - loss: 0.0658 - acc: 0.995 - ETA: 1s - loss: 0.0665 - acc: 0.996 - ETA: 1s - loss: 0.0646 - acc: 0.996 - ETA: 1s - loss: 0.0604 - acc: 0.996 - ETA: 0s - loss: 0.0603 - acc: 0.997 - ETA: 0s - loss: 0.0582 - acc: 0.997 - ETA: 0s - loss: 0.0558 - acc: 0.997 - ETA: 0s - loss: 0.0534 - acc: 0.997 - ETA: 0s - loss: 0.0533 - acc: 0.997 - ETA: 0s - loss: 0.0518 - acc: 0.998 - ETA: 0s - loss: 0.0542 - acc: 0.996 - ETA: 0s - loss: 0.0571 - acc: 0.996 - ETA: 0s - loss: 0.0598 - acc: 0.996 - ETA: 0s - loss: 0.0598 - acc: 0.996 - ETA: 0s - loss: 0.0625 - acc: 0.997 - ETA: 0s - loss: 0.0615 - acc: 0.997 - ETA: 0s - loss: 0.0600 - acc: 0.997 - 2s 72ms/step - loss: 0.0601 - acc: 0.9974 - val_loss: 0.0525 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.04179\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 36/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1415 - acc: 0.937 - ETA: 0s - loss: 0.0998 - acc: 0.968 - ETA: 0s - loss: 0.0802 - acc: 0.975 - ETA: 0s - loss: 0.0698 - acc: 0.982 - ETA: 0s - loss: 0.0644 - acc: 0.982 - ETA: 0s - loss: 0.0592 - acc: 0.985 - ETA: 0s - loss: 0.0600 - acc: 0.985 - ETA: 0s - loss: 0.0543 - acc: 0.987 - ETA: 0s - loss: 0.0539 - acc: 0.986 - ETA: 0s - loss: 0.0535 - acc: 0.987 - ETA: 0s - loss: 0.0530 - acc: 0.987 - ETA: 0s - loss: 0.0568 - acc: 0.987 - ETA: 0s - loss: 0.0554 - acc: 0.987 - 1s 49ms/step - loss: 0.0548 - acc: 0.9883 - val_loss: 0.0627 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.04179\n",
      "Epoch 37/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1981 - acc: 0.958 - ETA: 1s - loss: 0.1046 - acc: 0.975 - ETA: 1s - loss: 0.0871 - acc: 0.981 - ETA: 1s - loss: 0.0821 - acc: 0.985 - ETA: 1s - loss: 0.0785 - acc: 0.987 - ETA: 1s - loss: 0.0766 - acc: 0.989 - ETA: 1s - loss: 0.0745 - acc: 0.990 - ETA: 1s - loss: 0.0728 - acc: 0.991 - ETA: 0s - loss: 0.0681 - acc: 0.992 - ETA: 0s - loss: 0.0627 - acc: 0.993 - ETA: 0s - loss: 0.0632 - acc: 0.991 - ETA: 0s - loss: 0.0608 - acc: 0.992 - ETA: 0s - loss: 0.0593 - acc: 0.992 - ETA: 0s - loss: 0.0574 - acc: 0.993 - ETA: 0s - loss: 0.0578 - acc: 0.991 - ETA: 0s - loss: 0.0573 - acc: 0.992 - ETA: 0s - loss: 0.0563 - acc: 0.992 - ETA: 0s - loss: 0.0555 - acc: 0.992 - ETA: 0s - loss: 0.0535 - acc: 0.993 - ETA: 0s - loss: 0.0542 - acc: 0.992 - ETA: 0s - loss: 0.0540 - acc: 0.992 - ETA: 0s - loss: 0.0547 - acc: 0.991 - 2s 70ms/step - loss: 0.0533 - acc: 0.9918 - val_loss: 0.0377 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04179 to 0.03771, saving model to NN_v5.model\n",
      "Epoch 38/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0107 - acc: 1.000 - ETA: 0s - loss: 0.0366 - acc: 1.000 - ETA: 0s - loss: 0.0348 - acc: 1.000 - ETA: 0s - loss: 0.0348 - acc: 1.000 - ETA: 0s - loss: 0.0463 - acc: 0.989 - ETA: 0s - loss: 0.0519 - acc: 0.991 - ETA: 0s - loss: 0.0575 - acc: 0.988 - ETA: 0s - loss: 0.0552 - acc: 0.989 - ETA: 0s - loss: 0.0594 - acc: 0.987 - ETA: 0s - loss: 0.0560 - acc: 0.988 - ETA: 0s - loss: 0.0530 - acc: 0.989 - ETA: 0s - loss: 0.0516 - acc: 0.990 - ETA: 0s - loss: 0.0497 - acc: 0.991 - ETA: 0s - loss: 0.0480 - acc: 0.992 - ETA: 0s - loss: 0.0488 - acc: 0.990 - ETA: 0s - loss: 0.0476 - acc: 0.991 - ETA: 0s - loss: 0.0480 - acc: 0.991 - ETA: 0s - loss: 0.0470 - acc: 0.992 - ETA: 0s - loss: 0.0474 - acc: 0.992 - ETA: 0s - loss: 0.0475 - acc: 0.991 - ETA: 0s - loss: 0.0468 - acc: 0.991 - 1s 62ms/step - loss: 0.0493 - acc: 0.9896 - val_loss: 0.0424 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.03771\n",
      "Epoch 39/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0702 - acc: 0.968 - ETA: 0s - loss: 0.0370 - acc: 0.989 - ETA: 0s - loss: 0.0345 - acc: 0.992 - ETA: 0s - loss: 0.0465 - acc: 0.993 - ETA: 0s - loss: 0.0436 - acc: 0.994 - ETA: 0s - loss: 0.0393 - acc: 0.995 - ETA: 0s - loss: 0.0482 - acc: 0.988 - ETA: 0s - loss: 0.0587 - acc: 0.986 - ETA: 0s - loss: 0.0572 - acc: 0.987 - ETA: 0s - loss: 0.0570 - acc: 0.985 - ETA: 0s - loss: 0.0550 - acc: 0.987 - ETA: 0s - loss: 0.0545 - acc: 0.988 - ETA: 0s - loss: 0.0531 - acc: 0.988 - ETA: 0s - loss: 0.0540 - acc: 0.987 - ETA: 0s - loss: 0.0536 - acc: 0.988 - ETA: 0s - loss: 0.0511 - acc: 0.989 - ETA: 0s - loss: 0.0521 - acc: 0.987 - ETA: 0s - loss: 0.0505 - acc: 0.988 - ETA: 0s - loss: 0.0540 - acc: 0.987 - ETA: 0s - loss: 0.0520 - acc: 0.988 - ETA: 0s - loss: 0.0509 - acc: 0.988 - ETA: 0s - loss: 0.0509 - acc: 0.989 - 2s 63ms/step - loss: 0.0496 - acc: 0.9896 - val_loss: 0.0527 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03771\n",
      "Epoch 40/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0112 - acc: 1.000 - ETA: 0s - loss: 0.0381 - acc: 1.000 - ETA: 0s - loss: 0.0355 - acc: 1.000 - ETA: 0s - loss: 0.0370 - acc: 1.000 - ETA: 0s - loss: 0.0426 - acc: 0.995 - ETA: 0s - loss: 0.0405 - acc: 0.996 - ETA: 0s - loss: 0.0403 - acc: 0.996 - ETA: 0s - loss: 0.0368 - acc: 0.996 - ETA: 0s - loss: 0.0355 - acc: 0.997 - ETA: 0s - loss: 0.0339 - acc: 0.997 - ETA: 0s - loss: 0.0324 - acc: 0.997 - ETA: 0s - loss: 0.0324 - acc: 0.997 - ETA: 0s - loss: 0.0320 - acc: 0.997 - ETA: 0s - loss: 0.0325 - acc: 0.998 - ETA: 0s - loss: 0.0321 - acc: 0.998 - ETA: 0s - loss: 0.0332 - acc: 0.998 - ETA: 0s - loss: 0.0341 - acc: 0.998 - ETA: 0s - loss: 0.0359 - acc: 0.996 - ETA: 0s - loss: 0.0364 - acc: 0.995 - ETA: 0s - loss: 0.0360 - acc: 0.995 - ETA: 0s - loss: 0.0356 - acc: 0.995 - 1s 62ms/step - loss: 0.0361 - acc: 0.9961 - val_loss: 0.0352 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03771 to 0.03524, saving model to NN_v5.model\n",
      "Epoch 41/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0517 - acc: 1.000 - ETA: 0s - loss: 0.0447 - acc: 0.989 - ETA: 0s - loss: 0.0362 - acc: 0.992 - ETA: 0s - loss: 0.0432 - acc: 0.993 - ETA: 0s - loss: 0.0492 - acc: 0.989 - ETA: 0s - loss: 0.0483 - acc: 0.991 - ETA: 0s - loss: 0.0440 - acc: 0.992 - ETA: 0s - loss: 0.0402 - acc: 0.993 - ETA: 0s - loss: 0.0394 - acc: 0.990 - ETA: 0s - loss: 0.0377 - acc: 0.991 - ETA: 0s - loss: 0.0401 - acc: 0.992 - ETA: 0s - loss: 0.0391 - acc: 0.992 - ETA: 0s - loss: 0.0381 - acc: 0.993 - ETA: 0s - loss: 0.0402 - acc: 0.991 - ETA: 0s - loss: 0.0400 - acc: 0.992 - ETA: 0s - loss: 0.0414 - acc: 0.992 - ETA: 0s - loss: 0.0408 - acc: 0.993 - ETA: 0s - loss: 0.0422 - acc: 0.991 - ETA: 0s - loss: 0.0409 - acc: 0.992 - ETA: 0s - loss: 0.0404 - acc: 0.992 - ETA: 0s - loss: 0.0406 - acc: 0.992 - ETA: 0s - loss: 0.0397 - acc: 0.993 - 2s 63ms/step - loss: 0.0388 - acc: 0.9935 - val_loss: 0.0615 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03524\n",
      "Epoch 42/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0496 - acc: 0.968 - ETA: 0s - loss: 0.0305 - acc: 0.989 - ETA: 0s - loss: 0.0356 - acc: 0.992 - ETA: 1s - loss: 0.0367 - acc: 0.993 - ETA: 0s - loss: 0.0329 - acc: 0.994 - ETA: 0s - loss: 0.0387 - acc: 0.995 - ETA: 0s - loss: 0.0389 - acc: 0.996 - ETA: 0s - loss: 0.0376 - acc: 0.996 - ETA: 0s - loss: 0.0389 - acc: 0.996 - ETA: 0s - loss: 0.0407 - acc: 0.997 - ETA: 0s - loss: 0.0391 - acc: 0.997 - ETA: 0s - loss: 0.0370 - acc: 0.997 - ETA: 0s - loss: 0.0364 - acc: 0.997 - ETA: 0s - loss: 0.0373 - acc: 0.997 - ETA: 0s - loss: 0.0357 - acc: 0.998 - ETA: 0s - loss: 0.0357 - acc: 0.998 - ETA: 0s - loss: 0.0352 - acc: 0.998 - ETA: 0s - loss: 0.0339 - acc: 0.998 - ETA: 0s - loss: 0.0338 - acc: 0.998 - ETA: 0s - loss: 0.0347 - acc: 0.997 - ETA: 0s - loss: 0.0343 - acc: 0.997 - ETA: 0s - loss: 0.0383 - acc: 0.995 - 2s 63ms/step - loss: 0.0371 - acc: 0.9961 - val_loss: 0.0647 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03524\n",
      "Epoch 43/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0312 - acc: 1.000 - ETA: 0s - loss: 0.0381 - acc: 0.989 - ETA: 0s - loss: 0.0387 - acc: 0.984 - ETA: 1s - loss: 0.0397 - acc: 0.987 - ETA: 0s - loss: 0.0368 - acc: 0.989 - ETA: 0s - loss: 0.0309 - acc: 0.992 - ETA: 0s - loss: 0.0288 - acc: 0.993 - ETA: 0s - loss: 0.0270 - acc: 0.993 - ETA: 0s - loss: 0.0272 - acc: 0.994 - ETA: 0s - loss: 0.0259 - acc: 0.994 - ETA: 0s - loss: 0.0279 - acc: 0.995 - ETA: 0s - loss: 0.0318 - acc: 0.995 - ETA: 0s - loss: 0.0317 - acc: 0.995 - ETA: 0s - loss: 0.0307 - acc: 0.996 - ETA: 0s - loss: 0.0309 - acc: 0.996 - ETA: 0s - loss: 0.0308 - acc: 0.996 - ETA: 0s - loss: 0.0309 - acc: 0.996 - ETA: 0s - loss: 0.0312 - acc: 0.996 - ETA: 0s - loss: 0.0339 - acc: 0.995 - ETA: 0s - loss: 0.0340 - acc: 0.995 - ETA: 0s - loss: 0.0335 - acc: 0.995 - 1s 60ms/step - loss: 0.0347 - acc: 0.9948 - val_loss: 0.0570 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03524\n",
      "Epoch 44/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0159 - acc: 1.000 - ETA: 0s - loss: 0.0148 - acc: 1.000 - ETA: 1s - loss: 0.0178 - acc: 1.000 - ETA: 1s - loss: 0.0357 - acc: 0.993 - ETA: 1s - loss: 0.0520 - acc: 0.984 - ETA: 0s - loss: 0.0470 - acc: 0.986 - ETA: 0s - loss: 0.0442 - acc: 0.989 - ETA: 0s - loss: 0.0403 - acc: 0.990 - ETA: 0s - loss: 0.0398 - acc: 0.991 - ETA: 0s - loss: 0.0400 - acc: 0.992 - ETA: 0s - loss: 0.0389 - acc: 0.992 - ETA: 0s - loss: 0.0409 - acc: 0.991 - ETA: 0s - loss: 0.0436 - acc: 0.989 - ETA: 0s - loss: 0.0452 - acc: 0.988 - ETA: 0s - loss: 0.0438 - acc: 0.989 - ETA: 0s - loss: 0.0422 - acc: 0.989 - ETA: 0s - loss: 0.0412 - acc: 0.990 - ETA: 0s - loss: 0.0414 - acc: 0.990 - ETA: 0s - loss: 0.0398 - acc: 0.991 - ETA: 0s - loss: 0.0386 - acc: 0.991 - ETA: 0s - loss: 0.0381 - acc: 0.991 - 1s 62ms/step - loss: 0.0408 - acc: 0.9896 - val_loss: 0.0594 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03524\n",
      "Epoch 45/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0140 - acc: 1.000 - ETA: 0s - loss: 0.1075 - acc: 0.968 - ETA: 1s - loss: 0.0867 - acc: 0.976 - ETA: 1s - loss: 0.0821 - acc: 0.975 - ETA: 1s - loss: 0.0716 - acc: 0.979 - ETA: 0s - loss: 0.0705 - acc: 0.982 - ETA: 0s - loss: 0.0651 - acc: 0.984 - ETA: 0s - loss: 0.0575 - acc: 0.987 - ETA: 0s - loss: 0.0543 - acc: 0.988 - ETA: 0s - loss: 0.0536 - acc: 0.989 - ETA: 0s - loss: 0.0517 - acc: 0.990 - ETA: 0s - loss: 0.0494 - acc: 0.991 - ETA: 0s - loss: 0.0486 - acc: 0.989 - ETA: 0s - loss: 0.0478 - acc: 0.990 - ETA: 0s - loss: 0.0480 - acc: 0.990 - ETA: 0s - loss: 0.0468 - acc: 0.991 - ETA: 0s - loss: 0.0450 - acc: 0.991 - ETA: 0s - loss: 0.0439 - acc: 0.992 - ETA: 0s - loss: 0.0427 - acc: 0.992 - ETA: 0s - loss: 0.0428 - acc: 0.992 - ETA: 0s - loss: 0.0416 - acc: 0.993 - 1s 62ms/step - loss: 0.0411 - acc: 0.9935 - val_loss: 0.0536 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03524\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 46/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0442 - acc: 1.000 - ETA: 1s - loss: 0.0587 - acc: 0.989 - ETA: 1s - loss: 0.0589 - acc: 0.984 - ETA: 1s - loss: 0.0502 - acc: 0.987 - ETA: 1s - loss: 0.0459 - acc: 0.989 - ETA: 1s - loss: 0.0548 - acc: 0.982 - ETA: 0s - loss: 0.0513 - acc: 0.984 - ETA: 0s - loss: 0.0467 - acc: 0.986 - ETA: 0s - loss: 0.0443 - acc: 0.988 - ETA: 0s - loss: 0.0475 - acc: 0.987 - ETA: 0s - loss: 0.0469 - acc: 0.985 - ETA: 0s - loss: 0.0446 - acc: 0.986 - ETA: 0s - loss: 0.0445 - acc: 0.985 - ETA: 0s - loss: 0.0427 - acc: 0.986 - ETA: 0s - loss: 0.0442 - acc: 0.987 - ETA: 0s - loss: 0.0430 - acc: 0.987 - ETA: 0s - loss: 0.0420 - acc: 0.988 - ETA: 0s - loss: 0.0406 - acc: 0.989 - ETA: 0s - loss: 0.0415 - acc: 0.989 - ETA: 0s - loss: 0.0404 - acc: 0.990 - ETA: 0s - loss: 0.0442 - acc: 0.989 - 1s 61ms/step - loss: 0.0428 - acc: 0.9896 - val_loss: 0.0409 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03524\n",
      "Epoch 47/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0728 - acc: 0.968 - ETA: 1s - loss: 0.0367 - acc: 0.989 - ETA: 1s - loss: 0.0382 - acc: 0.984 - ETA: 1s - loss: 0.0333 - acc: 0.987 - ETA: 1s - loss: 0.0299 - acc: 0.989 - ETA: 1s - loss: 0.0298 - acc: 0.991 - ETA: 0s - loss: 0.0332 - acc: 0.993 - ETA: 0s - loss: 0.0320 - acc: 0.993 - ETA: 0s - loss: 0.0362 - acc: 0.992 - ETA: 0s - loss: 0.0361 - acc: 0.992 - ETA: 0s - loss: 0.0357 - acc: 0.993 - ETA: 0s - loss: 0.0354 - acc: 0.993 - ETA: 0s - loss: 0.0368 - acc: 0.992 - ETA: 0s - loss: 0.0356 - acc: 0.992 - ETA: 0s - loss: 0.0357 - acc: 0.993 - ETA: 0s - loss: 0.0370 - acc: 0.991 - ETA: 0s - loss: 0.0361 - acc: 0.992 - ETA: 0s - loss: 0.0352 - acc: 0.992 - ETA: 0s - loss: 0.0372 - acc: 0.991 - ETA: 0s - loss: 0.0357 - acc: 0.991 - 2s 67ms/step - loss: 0.0346 - acc: 0.9922 - val_loss: 0.0474 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03524\n",
      "Epoch 48/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0232 - acc: 1.000 - ETA: 1s - loss: 0.0286 - acc: 1.000 - ETA: 1s - loss: 0.0273 - acc: 1.000 - ETA: 1s - loss: 0.0249 - acc: 1.000 - ETA: 1s - loss: 0.0224 - acc: 1.000 - ETA: 1s - loss: 0.0271 - acc: 1.000 - ETA: 0s - loss: 0.0281 - acc: 1.000 - ETA: 0s - loss: 0.0266 - acc: 1.000 - ETA: 0s - loss: 0.0255 - acc: 1.000 - ETA: 0s - loss: 0.0277 - acc: 1.000 - ETA: 0s - loss: 0.0262 - acc: 1.000 - ETA: 0s - loss: 0.0277 - acc: 1.000 - ETA: 0s - loss: 0.0314 - acc: 0.997 - ETA: 0s - loss: 0.0310 - acc: 0.998 - ETA: 0s - loss: 0.0309 - acc: 0.998 - ETA: 0s - loss: 0.0306 - acc: 0.998 - ETA: 0s - loss: 0.0296 - acc: 0.998 - ETA: 0s - loss: 0.0294 - acc: 0.998 - ETA: 0s - loss: 0.0292 - acc: 0.998 - ETA: 0s - loss: 0.0317 - acc: 0.995 - ETA: 0s - loss: 0.0313 - acc: 0.995 - 2s 66ms/step - loss: 0.0311 - acc: 0.9961 - val_loss: 0.0407 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03524\n",
      "Epoch 49/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0296 - acc: 0.968 - ETA: 0s - loss: 0.0450 - acc: 0.979 - ETA: 0s - loss: 0.0346 - acc: 0.984 - ETA: 0s - loss: 0.0378 - acc: 0.981 - ETA: 0s - loss: 0.0599 - acc: 0.968 - ETA: 0s - loss: 0.0588 - acc: 0.973 - ETA: 0s - loss: 0.0580 - acc: 0.972 - ETA: 0s - loss: 0.0536 - acc: 0.975 - ETA: 0s - loss: 0.0494 - acc: 0.978 - ETA: 0s - loss: 0.0467 - acc: 0.980 - ETA: 0s - loss: 0.0458 - acc: 0.979 - ETA: 0s - loss: 0.0411 - acc: 0.982 - ETA: 0s - loss: 0.0392 - acc: 0.983 - ETA: 0s - loss: 0.0378 - acc: 0.984 - ETA: 0s - loss: 0.0366 - acc: 0.985 - ETA: 0s - loss: 0.0364 - acc: 0.986 - ETA: 0s - loss: 0.0354 - acc: 0.986 - ETA: 0s - loss: 0.0347 - acc: 0.987 - ETA: 0s - loss: 0.0357 - acc: 0.986 - ETA: 0s - loss: 0.0348 - acc: 0.987 - ETA: 0s - loss: 0.0358 - acc: 0.987 - 1s 62ms/step - loss: 0.0368 - acc: 0.9883 - val_loss: 0.0513 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03524\n",
      "Epoch 50/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0436 - acc: 1.000 - ETA: 1s - loss: 0.0276 - acc: 1.000 - ETA: 1s - loss: 0.0246 - acc: 1.000 - ETA: 1s - loss: 0.0237 - acc: 1.000 - ETA: 1s - loss: 0.0243 - acc: 1.000 - ETA: 1s - loss: 0.0230 - acc: 1.000 - ETA: 0s - loss: 0.0248 - acc: 1.000 - ETA: 0s - loss: 0.0232 - acc: 1.000 - ETA: 0s - loss: 0.0229 - acc: 1.000 - ETA: 0s - loss: 0.0264 - acc: 0.997 - ETA: 0s - loss: 0.0271 - acc: 0.997 - ETA: 0s - loss: 0.0281 - acc: 0.995 - ETA: 0s - loss: 0.0276 - acc: 0.995 - ETA: 0s - loss: 0.0284 - acc: 0.996 - ETA: 0s - loss: 0.0283 - acc: 0.996 - ETA: 0s - loss: 0.0300 - acc: 0.996 - ETA: 0s - loss: 0.0316 - acc: 0.995 - ETA: 0s - loss: 0.0305 - acc: 0.995 - ETA: 0s - loss: 0.0304 - acc: 0.995 - ETA: 0s - loss: 0.0296 - acc: 0.995 - ETA: 0s - loss: 0.0293 - acc: 0.995 - 1s 61ms/step - loss: 0.0291 - acc: 0.9961 - val_loss: 0.0381 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03524\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 51/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0889 - acc: 0.937 - ETA: 0s - loss: 0.0411 - acc: 0.979 - ETA: 0s - loss: 0.0381 - acc: 0.984 - ETA: 0s - loss: 0.0312 - acc: 0.987 - ETA: 0s - loss: 0.0328 - acc: 0.989 - ETA: 0s - loss: 0.0380 - acc: 0.986 - ETA: 0s - loss: 0.0360 - acc: 0.988 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0355 - acc: 0.987 - ETA: 0s - loss: 0.0339 - acc: 0.988 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.990 - ETA: 0s - loss: 0.0366 - acc: 0.988 - ETA: 0s - loss: 0.0429 - acc: 0.985 - ETA: 0s - loss: 0.0411 - acc: 0.986 - ETA: 0s - loss: 0.0401 - acc: 0.987 - ETA: 0s - loss: 0.0403 - acc: 0.987 - ETA: 0s - loss: 0.0428 - acc: 0.985 - ETA: 0s - loss: 0.0436 - acc: 0.986 - ETA: 0s - loss: 0.0425 - acc: 0.986 - ETA: 0s - loss: 0.0447 - acc: 0.984 - 1s 60ms/step - loss: 0.0471 - acc: 0.9840 - val_loss: 0.0288 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03524 to 0.02881, saving model to NN_v5.model\n",
      "Epoch 52/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0120 - acc: 1.000 - ETA: 0s - loss: 0.0273 - acc: 1.000 - ETA: 0s - loss: 0.0308 - acc: 1.000 - ETA: 0s - loss: 0.0265 - acc: 1.000 - ETA: 0s - loss: 0.0232 - acc: 1.000 - ETA: 0s - loss: 0.0234 - acc: 1.000 - ETA: 0s - loss: 0.0237 - acc: 1.000 - ETA: 0s - loss: 0.0227 - acc: 1.000 - ETA: 0s - loss: 0.0229 - acc: 1.000 - ETA: 0s - loss: 0.0236 - acc: 1.000 - ETA: 0s - loss: 0.0232 - acc: 1.000 - ETA: 0s - loss: 0.0243 - acc: 1.000 - ETA: 0s - loss: 0.0259 - acc: 1.000 - ETA: 0s - loss: 0.0250 - acc: 1.000 - ETA: 0s - loss: 0.0233 - acc: 1.000 - ETA: 0s - loss: 0.0226 - acc: 1.000 - ETA: 0s - loss: 0.0219 - acc: 1.000 - ETA: 0s - loss: 0.0223 - acc: 1.000 - ETA: 0s - loss: 0.0217 - acc: 1.000 - ETA: 0s - loss: 0.0211 - acc: 1.000 - ETA: 0s - loss: 0.0214 - acc: 1.000 - 1s 61ms/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0535 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02881\n",
      "Epoch 53/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0074 - acc: 1.000 - ETA: 1s - loss: 0.0150 - acc: 1.000 - ETA: 0s - loss: 0.0236 - acc: 1.000 - ETA: 1s - loss: 0.0240 - acc: 1.000 - ETA: 0s - loss: 0.0297 - acc: 0.994 - ETA: 0s - loss: 0.0319 - acc: 0.995 - ETA: 0s - loss: 0.0361 - acc: 0.996 - ETA: 0s - loss: 0.0342 - acc: 0.996 - ETA: 0s - loss: 0.0314 - acc: 0.996 - ETA: 0s - loss: 0.0332 - acc: 0.997 - ETA: 0s - loss: 0.0313 - acc: 0.997 - ETA: 0s - loss: 0.0310 - acc: 0.997 - ETA: 0s - loss: 0.0323 - acc: 0.995 - ETA: 0s - loss: 0.0331 - acc: 0.995 - ETA: 0s - loss: 0.0321 - acc: 0.996 - ETA: 0s - loss: 0.0304 - acc: 0.996 - ETA: 0s - loss: 0.0300 - acc: 0.996 - ETA: 0s - loss: 0.0294 - acc: 0.996 - ETA: 0s - loss: 0.0293 - acc: 0.997 - ETA: 0s - loss: 0.0291 - acc: 0.997 - ETA: 0s - loss: 0.0296 - acc: 0.997 - 2s 63ms/step - loss: 0.0311 - acc: 0.9961 - val_loss: 0.0545 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02881\n",
      "Epoch 54/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0354 - acc: 0.968 - ETA: 1s - loss: 0.0244 - acc: 0.989 - ETA: 1s - loss: 0.0274 - acc: 0.992 - ETA: 1s - loss: 0.0252 - acc: 0.993 - ETA: 1s - loss: 0.0273 - acc: 0.994 - ETA: 0s - loss: 0.0259 - acc: 0.995 - ETA: 0s - loss: 0.0244 - acc: 0.996 - ETA: 0s - loss: 0.0249 - acc: 0.996 - ETA: 0s - loss: 0.0256 - acc: 0.996 - ETA: 0s - loss: 0.0239 - acc: 0.997 - ETA: 0s - loss: 0.0227 - acc: 0.997 - ETA: 0s - loss: 0.0254 - acc: 0.997 - ETA: 0s - loss: 0.0240 - acc: 0.997 - ETA: 0s - loss: 0.0229 - acc: 0.997 - ETA: 0s - loss: 0.0255 - acc: 0.996 - ETA: 0s - loss: 0.0265 - acc: 0.996 - ETA: 0s - loss: 0.0282 - acc: 0.996 - ETA: 0s - loss: 0.0273 - acc: 0.996 - ETA: 0s - loss: 0.0266 - acc: 0.997 - ETA: 0s - loss: 0.0259 - acc: 0.997 - ETA: 0s - loss: 0.0271 - acc: 0.997 - 2s 63ms/step - loss: 0.0311 - acc: 0.9961 - val_loss: 0.0286 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.02881 to 0.02861, saving model to NN_v5.model\n",
      "Epoch 55/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0217 - acc: 1.000 - ETA: 0s - loss: 0.0179 - acc: 1.000 - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 0s - loss: 0.0178 - acc: 1.000 - ETA: 0s - loss: 0.0207 - acc: 1.000 - ETA: 0s - loss: 0.0195 - acc: 1.000 - ETA: 0s - loss: 0.0241 - acc: 0.996 - ETA: 0s - loss: 0.0230 - acc: 0.996 - ETA: 0s - loss: 0.0218 - acc: 0.996 - ETA: 0s - loss: 0.0209 - acc: 0.997 - ETA: 0s - loss: 0.0222 - acc: 0.997 - ETA: 0s - loss: 0.0209 - acc: 0.997 - ETA: 0s - loss: 0.0210 - acc: 0.997 - ETA: 0s - loss: 0.0213 - acc: 0.997 - ETA: 0s - loss: 0.0218 - acc: 0.998 - ETA: 0s - loss: 0.0231 - acc: 0.998 - ETA: 0s - loss: 0.0243 - acc: 0.996 - ETA: 0s - loss: 0.0241 - acc: 0.996 - ETA: 0s - loss: 0.0239 - acc: 0.997 - ETA: 0s - loss: 0.0239 - acc: 0.997 - ETA: 0s - loss: 0.0239 - acc: 0.997 - 1s 60ms/step - loss: 0.0240 - acc: 0.9974 - val_loss: 0.0296 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02861\n",
      "Epoch 56/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0304 - acc: 1.000 - ETA: 0s - loss: 0.0207 - acc: 1.000 - ETA: 1s - loss: 0.0239 - acc: 1.000 - ETA: 1s - loss: 0.0306 - acc: 0.993 - ETA: 1s - loss: 0.0290 - acc: 0.994 - ETA: 0s - loss: 0.0263 - acc: 0.995 - ETA: 0s - loss: 0.0252 - acc: 0.996 - ETA: 0s - loss: 0.0271 - acc: 0.996 - ETA: 0s - loss: 0.0288 - acc: 0.996 - ETA: 0s - loss: 0.0266 - acc: 0.997 - ETA: 0s - loss: 0.0253 - acc: 0.997 - ETA: 0s - loss: 0.0255 - acc: 0.997 - ETA: 0s - loss: 0.0239 - acc: 0.997 - ETA: 0s - loss: 0.0259 - acc: 0.997 - ETA: 0s - loss: 0.0244 - acc: 0.998 - ETA: 0s - loss: 0.0234 - acc: 0.998 - ETA: 0s - loss: 0.0233 - acc: 0.998 - ETA: 0s - loss: 0.0258 - acc: 0.996 - ETA: 0s - loss: 0.0249 - acc: 0.997 - ETA: 0s - loss: 0.0267 - acc: 0.995 - ETA: 0s - loss: 0.0272 - acc: 0.994 - 2s 63ms/step - loss: 0.0266 - acc: 0.9948 - val_loss: 0.0516 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02861\n",
      "Epoch 57/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0066 - acc: 1.000 - ETA: 0s - loss: 0.0258 - acc: 1.000 - ETA: 0s - loss: 0.0239 - acc: 1.000 - ETA: 1s - loss: 0.0281 - acc: 1.000 - ETA: 0s - loss: 0.0283 - acc: 1.000 - ETA: 0s - loss: 0.0327 - acc: 1.000 - ETA: 0s - loss: 0.0301 - acc: 1.000 - ETA: 0s - loss: 0.0288 - acc: 1.000 - ETA: 0s - loss: 0.0276 - acc: 1.000 - ETA: 0s - loss: 0.0262 - acc: 1.000 - ETA: 0s - loss: 0.0269 - acc: 1.000 - ETA: 0s - loss: 0.0257 - acc: 1.000 - ETA: 0s - loss: 0.0254 - acc: 1.000 - ETA: 0s - loss: 0.0280 - acc: 1.000 - ETA: 0s - loss: 0.0320 - acc: 0.996 - ETA: 0s - loss: 0.0342 - acc: 0.994 - ETA: 0s - loss: 0.0330 - acc: 0.994 - ETA: 0s - loss: 0.0324 - acc: 0.995 - ETA: 0s - loss: 0.0315 - acc: 0.995 - ETA: 0s - loss: 0.0297 - acc: 0.995 - ETA: 0s - loss: 0.0288 - acc: 0.995 - 1s 61ms/step - loss: 0.0282 - acc: 0.9961 - val_loss: 0.0332 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02861\n",
      "Epoch 58/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0340 - acc: 1.000 - ETA: 1s - loss: 0.0308 - acc: 1.000 - ETA: 1s - loss: 0.0259 - acc: 1.000 - ETA: 1s - loss: 0.0231 - acc: 1.000 - ETA: 1s - loss: 0.0217 - acc: 1.000 - ETA: 0s - loss: 0.0219 - acc: 1.000 - ETA: 0s - loss: 0.0220 - acc: 1.000 - ETA: 0s - loss: 0.0230 - acc: 1.000 - ETA: 0s - loss: 0.0245 - acc: 1.000 - ETA: 0s - loss: 0.0241 - acc: 1.000 - ETA: 0s - loss: 0.0251 - acc: 1.000 - ETA: 0s - loss: 0.0374 - acc: 0.995 - ETA: 0s - loss: 0.0367 - acc: 0.995 - ETA: 0s - loss: 0.0355 - acc: 0.995 - ETA: 0s - loss: 0.0346 - acc: 0.996 - ETA: 0s - loss: 0.0355 - acc: 0.996 - ETA: 0s - loss: 0.0350 - acc: 0.996 - ETA: 0s - loss: 0.0342 - acc: 0.996 - ETA: 0s - loss: 0.0334 - acc: 0.996 - ETA: 0s - loss: 0.0330 - acc: 0.997 - ETA: 0s - loss: 0.0322 - acc: 0.997 - 2s 63ms/step - loss: 0.0316 - acc: 0.9974 - val_loss: 0.0494 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02861\n",
      "Epoch 59/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0305 - acc: 1.000 - ETA: 1s - loss: 0.0386 - acc: 0.989 - ETA: 1s - loss: 0.0307 - acc: 0.992 - ETA: 1s - loss: 0.0350 - acc: 0.993 - ETA: 1s - loss: 0.0530 - acc: 0.989 - ETA: 1s - loss: 0.0492 - acc: 0.991 - ETA: 1s - loss: 0.0449 - acc: 0.992 - ETA: 0s - loss: 0.0438 - acc: 0.993 - ETA: 0s - loss: 0.0453 - acc: 0.990 - ETA: 0s - loss: 0.0443 - acc: 0.991 - ETA: 0s - loss: 0.0421 - acc: 0.992 - ETA: 0s - loss: 0.0416 - acc: 0.990 - ETA: 0s - loss: 0.0395 - acc: 0.991 - ETA: 0s - loss: 0.0376 - acc: 0.991 - ETA: 0s - loss: 0.0365 - acc: 0.992 - ETA: 0s - loss: 0.0351 - acc: 0.992 - ETA: 0s - loss: 0.0382 - acc: 0.991 - ETA: 0s - loss: 0.0363 - acc: 0.991 - ETA: 0s - loss: 0.0359 - acc: 0.992 - ETA: 0s - loss: 0.0362 - acc: 0.992 - ETA: 0s - loss: 0.0355 - acc: 0.992 - 2s 66ms/step - loss: 0.0341 - acc: 0.9935 - val_loss: 0.0318 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02861\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 60/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0224 - acc: 1.000 - ETA: 0s - loss: 0.0491 - acc: 0.989 - ETA: 0s - loss: 0.0408 - acc: 0.992 - ETA: 1s - loss: 0.0360 - acc: 0.993 - ETA: 1s - loss: 0.0347 - acc: 0.994 - ETA: 0s - loss: 0.0323 - acc: 0.995 - ETA: 0s - loss: 0.0297 - acc: 0.996 - ETA: 0s - loss: 0.0288 - acc: 0.996 - ETA: 0s - loss: 0.0281 - acc: 0.996 - ETA: 0s - loss: 0.0266 - acc: 0.997 - ETA: 0s - loss: 0.0281 - acc: 0.994 - ETA: 0s - loss: 0.0275 - acc: 0.995 - ETA: 0s - loss: 0.0277 - acc: 0.995 - ETA: 0s - loss: 0.0261 - acc: 0.995 - ETA: 0s - loss: 0.0259 - acc: 0.996 - ETA: 0s - loss: 0.0249 - acc: 0.996 - ETA: 0s - loss: 0.0246 - acc: 0.996 - ETA: 0s - loss: 0.0240 - acc: 0.996 - ETA: 0s - loss: 0.0240 - acc: 0.996 - ETA: 0s - loss: 0.0238 - acc: 0.997 - ETA: 0s - loss: 0.0241 - acc: 0.997 - ETA: 0s - loss: 0.0246 - acc: 0.997 - 1s 61ms/step - loss: 0.0245 - acc: 0.9974 - val_loss: 0.0313 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02861\n",
      "Epoch 61/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0344 - acc: 1.000 - ETA: 1s - loss: 0.0281 - acc: 1.000 - ETA: 1s - loss: 0.0307 - acc: 1.000 - ETA: 1s - loss: 0.0259 - acc: 1.000 - ETA: 1s - loss: 0.0251 - acc: 1.000 - ETA: 0s - loss: 0.0266 - acc: 1.000 - ETA: 0s - loss: 0.0337 - acc: 0.996 - ETA: 0s - loss: 0.0319 - acc: 0.996 - ETA: 0s - loss: 0.0320 - acc: 0.996 - ETA: 0s - loss: 0.0316 - acc: 0.997 - ETA: 0s - loss: 0.0308 - acc: 0.997 - ETA: 0s - loss: 0.0302 - acc: 0.997 - ETA: 0s - loss: 0.0295 - acc: 0.997 - ETA: 0s - loss: 0.0280 - acc: 0.997 - ETA: 0s - loss: 0.0267 - acc: 0.998 - ETA: 0s - loss: 0.0270 - acc: 0.998 - ETA: 0s - loss: 0.0275 - acc: 0.998 - ETA: 0s - loss: 0.0269 - acc: 0.998 - ETA: 0s - loss: 0.0273 - acc: 0.998 - ETA: 0s - loss: 0.0262 - acc: 0.998 - ETA: 0s - loss: 0.0265 - acc: 0.998 - ETA: 0s - loss: 0.0280 - acc: 0.997 - 2s 63ms/step - loss: 0.0277 - acc: 0.9974 - val_loss: 0.0373 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02861\n",
      "Epoch 62/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0046 - acc: 1.000 - ETA: 0s - loss: 0.0125 - acc: 1.000 - ETA: 0s - loss: 0.0138 - acc: 1.000 - ETA: 0s - loss: 0.0200 - acc: 0.993 - ETA: 0s - loss: 0.0227 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.996 - ETA: 0s - loss: 0.0196 - acc: 0.996 - ETA: 0s - loss: 0.0209 - acc: 0.996 - ETA: 0s - loss: 0.0206 - acc: 0.997 - ETA: 0s - loss: 0.0208 - acc: 0.997 - ETA: 0s - loss: 0.0216 - acc: 0.997 - ETA: 0s - loss: 0.0247 - acc: 0.995 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0253 - acc: 0.994 - ETA: 0s - loss: 0.0252 - acc: 0.994 - ETA: 0s - loss: 0.0267 - acc: 0.993 - ETA: 0s - loss: 0.0267 - acc: 0.993 - ETA: 0s - loss: 0.0258 - acc: 0.993 - ETA: 0s - loss: 0.0250 - acc: 0.994 - ETA: 0s - loss: 0.0247 - acc: 0.994 - ETA: 0s - loss: 0.0247 - acc: 0.994 - 1s 60ms/step - loss: 0.0254 - acc: 0.9948 - val_loss: 0.0289 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02861\n",
      "Epoch 63/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0213 - acc: 1.000 - ETA: 0s - loss: 0.0429 - acc: 0.986 - ETA: 0s - loss: 0.0412 - acc: 0.979 - ETA: 0s - loss: 0.0363 - acc: 0.982 - ETA: 0s - loss: 0.0327 - acc: 0.985 - ETA: 0s - loss: 0.0310 - acc: 0.987 - ETA: 0s - loss: 0.0314 - acc: 0.988 - ETA: 0s - loss: 0.0296 - acc: 0.989 - ETA: 0s - loss: 0.0329 - acc: 0.987 - ETA: 0s - loss: 0.0306 - acc: 0.988 - ETA: 0s - loss: 0.0287 - acc: 0.989 - ETA: 0s - loss: 0.0288 - acc: 0.990 - ETA: 0s - loss: 0.0307 - acc: 0.988 - ETA: 0s - loss: 0.0297 - acc: 0.989 - ETA: 0s - loss: 0.0294 - acc: 0.990 - ETA: 0s - loss: 0.0290 - acc: 0.990 - ETA: 0s - loss: 0.0310 - acc: 0.989 - ETA: 0s - loss: 0.0303 - acc: 0.990 - ETA: 0s - loss: 0.0303 - acc: 0.990 - ETA: 0s - loss: 0.0292 - acc: 0.991 - ETA: 0s - loss: 0.0303 - acc: 0.991 - 1s 56ms/step - loss: 0.0311 - acc: 0.9918 - val_loss: 0.0366 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02861\n",
      "Epoch 64/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0266 - acc: 1.000 - ETA: 0s - loss: 0.0195 - acc: 1.000 - ETA: 0s - loss: 0.0336 - acc: 0.993 - ETA: 0s - loss: 0.0309 - acc: 0.994 - ETA: 0s - loss: 0.0311 - acc: 0.991 - ETA: 0s - loss: 0.0300 - acc: 0.992 - ETA: 0s - loss: 0.0276 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0273 - acc: 0.994 - ETA: 0s - loss: 0.0287 - acc: 0.994 - ETA: 0s - loss: 0.0290 - acc: 0.995 - ETA: 0s - loss: 0.0314 - acc: 0.995 - ETA: 0s - loss: 0.0296 - acc: 0.995 - ETA: 0s - loss: 0.0285 - acc: 0.996 - ETA: 0s - loss: 0.0289 - acc: 0.996 - ETA: 0s - loss: 0.0273 - acc: 0.996 - ETA: 0s - loss: 0.0281 - acc: 0.995 - ETA: 0s - loss: 0.0279 - acc: 0.995 - ETA: 0s - loss: 0.0287 - acc: 0.995 - 1s 55ms/step - loss: 0.0284 - acc: 0.9961 - val_loss: 0.0295 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02861\n",
      "Epoch 65/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0178 - acc: 1.000 - ETA: 0s - loss: 0.0247 - acc: 1.000 - ETA: 0s - loss: 0.0292 - acc: 1.000 - ETA: 0s - loss: 0.0253 - acc: 1.000 - ETA: 0s - loss: 0.0269 - acc: 1.000 - ETA: 0s - loss: 0.0275 - acc: 0.996 - ETA: 0s - loss: 0.0287 - acc: 0.996 - ETA: 0s - loss: 0.0270 - acc: 0.996 - ETA: 0s - loss: 0.0254 - acc: 0.997 - ETA: 0s - loss: 0.0247 - acc: 0.997 - ETA: 0s - loss: 0.0225 - acc: 0.997 - ETA: 0s - loss: 0.0223 - acc: 0.997 - ETA: 0s - loss: 0.0216 - acc: 0.998 - ETA: 0s - loss: 0.0207 - acc: 0.998 - ETA: 0s - loss: 0.0205 - acc: 0.998 - ETA: 0s - loss: 0.0206 - acc: 0.998 - ETA: 0s - loss: 0.0200 - acc: 0.998 - ETA: 0s - loss: 0.0194 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.995 - ETA: 0s - loss: 0.0220 - acc: 0.995 - 1s 54ms/step - loss: 0.0220 - acc: 0.9961 - val_loss: 0.0440 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02861\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0583 - acc: 0.968 - ETA: 0s - loss: 0.0527 - acc: 0.968 - ETA: 0s - loss: 0.0379 - acc: 0.981 - ETA: 0s - loss: 0.0336 - acc: 0.984 - ETA: 0s - loss: 0.0322 - acc: 0.986 - ETA: 0s - loss: 0.0290 - acc: 0.988 - ETA: 0s - loss: 0.0282 - acc: 0.987 - ETA: 0s - loss: 0.0283 - acc: 0.988 - ETA: 0s - loss: 0.0270 - acc: 0.989 - ETA: 0s - loss: 0.0294 - acc: 0.990 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0277 - acc: 0.991 - ETA: 0s - loss: 0.0292 - acc: 0.990 - ETA: 0s - loss: 0.0324 - acc: 0.987 - ETA: 0s - loss: 0.0317 - acc: 0.987 - ETA: 0s - loss: 0.0319 - acc: 0.988 - ETA: 0s - loss: 0.0316 - acc: 0.989 - ETA: 0s - loss: 0.0355 - acc: 0.988 - ETA: 0s - loss: 0.0354 - acc: 0.989 - 1s 53ms/step - loss: 0.0369 - acc: 0.9870 - val_loss: 0.0502 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02861\n",
      "Epoch 67/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0657 - acc: 0.968 - ETA: 0s - loss: 0.0290 - acc: 0.989 - ETA: 0s - loss: 0.0284 - acc: 0.993 - ETA: 0s - loss: 0.0292 - acc: 0.991 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0273 - acc: 0.993 - ETA: 0s - loss: 0.0269 - acc: 0.993 - ETA: 0s - loss: 0.0325 - acc: 0.991 - ETA: 0s - loss: 0.0313 - acc: 0.992 - ETA: 0s - loss: 0.0311 - acc: 0.992 - ETA: 0s - loss: 0.0294 - acc: 0.993 - ETA: 0s - loss: 0.0291 - acc: 0.993 - ETA: 0s - loss: 0.0315 - acc: 0.994 - ETA: 0s - loss: 0.0303 - acc: 0.994 - ETA: 0s - loss: 0.0303 - acc: 0.995 - ETA: 0s - loss: 0.0296 - acc: 0.995 - ETA: 0s - loss: 0.0304 - acc: 0.994 - ETA: 0s - loss: 0.0307 - acc: 0.993 - 1s 54ms/step - loss: 0.0301 - acc: 0.9935 - val_loss: 0.0311 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02861\n",
      "Epoch 68/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0426 - acc: 1.000 - ETA: 0s - loss: 0.0487 - acc: 0.989 - ETA: 0s - loss: 0.0338 - acc: 0.993 - ETA: 0s - loss: 0.0302 - acc: 0.994 - ETA: 0s - loss: 0.0266 - acc: 0.996 - ETA: 0s - loss: 0.0260 - acc: 0.996 - ETA: 0s - loss: 0.0263 - acc: 0.997 - ETA: 0s - loss: 0.0231 - acc: 0.997 - ETA: 0s - loss: 0.0226 - acc: 0.997 - ETA: 0s - loss: 0.0208 - acc: 0.998 - ETA: 0s - loss: 0.0207 - acc: 0.998 - ETA: 0s - loss: 0.0205 - acc: 0.998 - ETA: 0s - loss: 0.0203 - acc: 0.998 - ETA: 0s - loss: 0.0214 - acc: 0.997 - ETA: 0s - loss: 0.0219 - acc: 0.997 - 1s 54ms/step - loss: 0.0223 - acc: 0.9974 - val_loss: 0.0339 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02861\n",
      "Epoch 69/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0295 - acc: 1.000 - ETA: 0s - loss: 0.0215 - acc: 1.000 - ETA: 0s - loss: 0.0448 - acc: 0.993 - ETA: 0s - loss: 0.0502 - acc: 0.989 - ETA: 0s - loss: 0.0453 - acc: 0.991 - ETA: 0s - loss: 0.0387 - acc: 0.993 - ETA: 0s - loss: 0.0356 - acc: 0.993 - ETA: 0s - loss: 0.0333 - acc: 0.994 - ETA: 0s - loss: 0.0330 - acc: 0.994 - ETA: 0s - loss: 0.0364 - acc: 0.992 - ETA: 0s - loss: 0.0374 - acc: 0.991 - ETA: 0s - loss: 0.0361 - acc: 0.992 - ETA: 0s - loss: 0.0365 - acc: 0.992 - ETA: 0s - loss: 0.0348 - acc: 0.993 - ETA: 0s - loss: 0.0372 - acc: 0.992 - ETA: 0s - loss: 0.0417 - acc: 0.991 - ETA: 0s - loss: 0.0417 - acc: 0.990 - ETA: 0s - loss: 0.0405 - acc: 0.990 - 1s 55ms/step - loss: 0.0408 - acc: 0.9909 - val_loss: 0.0477 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02861\n",
      "Epoch 70/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0105 - acc: 1.000 - ETA: 0s - loss: 0.0201 - acc: 1.000 - ETA: 0s - loss: 0.0272 - acc: 0.993 - ETA: 0s - loss: 0.0270 - acc: 0.994 - ETA: 0s - loss: 0.0370 - acc: 0.995 - ETA: 0s - loss: 0.0353 - acc: 0.996 - ETA: 0s - loss: 0.0312 - acc: 0.996 - ETA: 0s - loss: 0.0301 - acc: 0.997 - ETA: 0s - loss: 0.0289 - acc: 0.997 - ETA: 0s - loss: 0.0275 - acc: 0.997 - ETA: 0s - loss: 0.0260 - acc: 0.997 - ETA: 0s - loss: 0.0258 - acc: 0.997 - ETA: 0s - loss: 0.0257 - acc: 0.998 - ETA: 0s - loss: 0.0253 - acc: 0.996 - ETA: 0s - loss: 0.0247 - acc: 0.996 - ETA: 0s - loss: 0.0253 - acc: 0.997 - ETA: 0s - loss: 0.0254 - acc: 0.997 - 1s 57ms/step - loss: 0.0252 - acc: 0.9974 - val_loss: 0.0362 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02861\n",
      "Epoch 71/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0688 - acc: 1.000 - ETA: 0s - loss: 0.0328 - acc: 1.000 - ETA: 0s - loss: 0.0264 - acc: 1.000 - ETA: 0s - loss: 0.0302 - acc: 1.000 - ETA: 0s - loss: 0.0257 - acc: 1.000 - ETA: 0s - loss: 0.0254 - acc: 1.000 - ETA: 0s - loss: 0.0247 - acc: 1.000 - ETA: 0s - loss: 0.0275 - acc: 1.000 - ETA: 0s - loss: 0.0246 - acc: 1.000 - ETA: 0s - loss: 0.0277 - acc: 0.997 - ETA: 0s - loss: 0.0274 - acc: 0.997 - ETA: 0s - loss: 0.0264 - acc: 0.997 - ETA: 0s - loss: 0.0275 - acc: 0.995 - ETA: 0s - loss: 0.0325 - acc: 0.994 - ETA: 0s - loss: 0.0310 - acc: 0.994 - ETA: 0s - loss: 0.0298 - acc: 0.994 - ETA: 0s - loss: 0.0290 - acc: 0.995 - ETA: 0s - loss: 0.0283 - acc: 0.995 - ETA: 0s - loss: 0.0277 - acc: 0.995 - ETA: 0s - loss: 0.0281 - acc: 0.995 - 1s 55ms/step - loss: 0.0275 - acc: 0.9961 - val_loss: 0.0279 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.02861 to 0.02792, saving model to NN_v5.model\n",
      "Epoch 72/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0956 - acc: 0.968 - ETA: 0s - loss: 0.0948 - acc: 0.968 - ETA: 0s - loss: 0.0625 - acc: 0.981 - ETA: 0s - loss: 0.0557 - acc: 0.984 - ETA: 0s - loss: 0.0490 - acc: 0.986 - ETA: 0s - loss: 0.0530 - acc: 0.984 - ETA: 0s - loss: 0.0478 - acc: 0.986 - ETA: 0s - loss: 0.0438 - acc: 0.987 - ETA: 0s - loss: 0.0380 - acc: 0.989 - ETA: 0s - loss: 0.0358 - acc: 0.990 - ETA: 0s - loss: 0.0331 - acc: 0.991 - ETA: 0s - loss: 0.0315 - acc: 0.992 - ETA: 0s - loss: 0.0330 - acc: 0.990 - ETA: 0s - loss: 0.0317 - acc: 0.991 - ETA: 0s - loss: 0.0310 - acc: 0.991 - ETA: 0s - loss: 0.0299 - acc: 0.992 - ETA: 0s - loss: 0.0301 - acc: 0.992 - ETA: 0s - loss: 0.0293 - acc: 0.992 - ETA: 0s - loss: 0.0291 - acc: 0.993 - 1s 54ms/step - loss: 0.0295 - acc: 0.9935 - val_loss: 0.0557 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02792\n",
      "Epoch 73/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0155 - acc: 1.000 - ETA: 0s - loss: 0.0579 - acc: 0.968 - ETA: 0s - loss: 0.0618 - acc: 0.975 - ETA: 0s - loss: 0.0549 - acc: 0.979 - ETA: 0s - loss: 0.0486 - acc: 0.982 - ETA: 0s - loss: 0.0460 - acc: 0.980 - ETA: 0s - loss: 0.0429 - acc: 0.982 - ETA: 0s - loss: 0.0409 - acc: 0.984 - ETA: 0s - loss: 0.0384 - acc: 0.985 - ETA: 0s - loss: 0.0349 - acc: 0.988 - ETA: 0s - loss: 0.0356 - acc: 0.986 - ETA: 0s - loss: 0.0346 - acc: 0.987 - ETA: 0s - loss: 0.0369 - acc: 0.986 - ETA: 0s - loss: 0.0349 - acc: 0.987 - ETA: 0s - loss: 0.0344 - acc: 0.987 - ETA: 0s - loss: 0.0336 - acc: 0.988 - ETA: 0s - loss: 0.0329 - acc: 0.989 - ETA: 0s - loss: 0.0323 - acc: 0.990 - ETA: 0s - loss: 0.0311 - acc: 0.990 - 1s 55ms/step - loss: 0.0303 - acc: 0.9909 - val_loss: 0.0317 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.02792\n",
      "Epoch 74/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0077 - acc: 1.000 - ETA: 0s - loss: 0.0137 - acc: 1.000 - ETA: 0s - loss: 0.0123 - acc: 1.000 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0186 - acc: 0.995 - ETA: 0s - loss: 0.0207 - acc: 0.996 - ETA: 0s - loss: 0.0211 - acc: 0.996 - ETA: 0s - loss: 0.0201 - acc: 0.997 - ETA: 0s - loss: 0.0212 - acc: 0.997 - ETA: 0s - loss: 0.0218 - acc: 0.997 - ETA: 0s - loss: 0.0214 - acc: 0.997 - ETA: 0s - loss: 0.0240 - acc: 0.995 - ETA: 0s - loss: 0.0249 - acc: 0.996 - ETA: 0s - loss: 0.0248 - acc: 0.996 - ETA: 0s - loss: 0.0240 - acc: 0.996 - ETA: 0s - loss: 0.0238 - acc: 0.996 - ETA: 0s - loss: 0.0247 - acc: 0.995 - ETA: 0s - loss: 0.0239 - acc: 0.995 - ETA: 0s - loss: 0.0237 - acc: 0.995 - 1s 57ms/step - loss: 0.0230 - acc: 0.9961 - val_loss: 0.0351 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.02792\n",
      "Epoch 75/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0132 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0221 - acc: 1.000 - ETA: 0s - loss: 0.0264 - acc: 1.000 - ETA: 0s - loss: 0.0243 - acc: 1.000 - ETA: 0s - loss: 0.0281 - acc: 1.000 - ETA: 0s - loss: 0.0269 - acc: 1.000 - ETA: 0s - loss: 0.0248 - acc: 1.000 - ETA: 0s - loss: 0.0228 - acc: 1.000 - ETA: 0s - loss: 0.0226 - acc: 1.000 - ETA: 0s - loss: 0.0219 - acc: 1.000 - ETA: 0s - loss: 0.0250 - acc: 0.997 - ETA: 0s - loss: 0.0251 - acc: 0.998 - ETA: 0s - loss: 0.0284 - acc: 0.996 - ETA: 0s - loss: 0.0281 - acc: 0.996 - ETA: 0s - loss: 0.0291 - acc: 0.995 - ETA: 0s - loss: 0.0311 - acc: 0.995 - ETA: 0s - loss: 0.0300 - acc: 0.995 - ETA: 0s - loss: 0.0297 - acc: 0.995 - ETA: 0s - loss: 0.0289 - acc: 0.995 - 1s 58ms/step - loss: 0.0284 - acc: 0.9961 - val_loss: 0.0545 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.02792\n",
      "Epoch 76/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0136 - acc: 1.000 - ETA: 0s - loss: 0.0223 - acc: 1.000 - ETA: 0s - loss: 0.0297 - acc: 1.000 - ETA: 0s - loss: 0.0286 - acc: 1.000 - ETA: 0s - loss: 0.0316 - acc: 1.000 - ETA: 0s - loss: 0.0287 - acc: 1.000 - ETA: 0s - loss: 0.0292 - acc: 1.000 - ETA: 0s - loss: 0.0270 - acc: 1.000 - ETA: 0s - loss: 0.0261 - acc: 1.000 - ETA: 0s - loss: 0.0252 - acc: 1.000 - ETA: 0s - loss: 0.0241 - acc: 1.000 - ETA: 0s - loss: 0.0235 - acc: 1.000 - ETA: 0s - loss: 0.0225 - acc: 1.000 - ETA: 0s - loss: 0.0212 - acc: 1.000 - ETA: 0s - loss: 0.0201 - acc: 1.000 - ETA: 0s - loss: 0.0207 - acc: 1.000 - ETA: 0s - loss: 0.0211 - acc: 1.000 - ETA: 0s - loss: 0.0205 - acc: 1.000 - ETA: 0s - loss: 0.0200 - acc: 1.000 - ETA: 0s - loss: 0.0196 - acc: 1.000 - 1s 59ms/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0356 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.02792\n",
      "Epoch 77/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0234 - acc: 1.000 - ETA: 0s - loss: 0.0170 - acc: 1.000 - ETA: 0s - loss: 0.0265 - acc: 1.000 - ETA: 0s - loss: 0.0334 - acc: 0.994 - ETA: 0s - loss: 0.0307 - acc: 0.995 - ETA: 0s - loss: 0.0281 - acc: 0.996 - ETA: 0s - loss: 0.0272 - acc: 0.996 - ETA: 0s - loss: 0.0258 - acc: 0.996 - ETA: 0s - loss: 0.0257 - acc: 0.997 - ETA: 0s - loss: 0.0252 - acc: 0.997 - ETA: 0s - loss: 0.0253 - acc: 0.997 - ETA: 0s - loss: 0.0257 - acc: 0.997 - ETA: 0s - loss: 0.0244 - acc: 0.997 - ETA: 0s - loss: 0.0227 - acc: 0.998 - ETA: 0s - loss: 0.0226 - acc: 0.998 - ETA: 0s - loss: 0.0241 - acc: 0.998 - ETA: 0s - loss: 0.0236 - acc: 0.998 - ETA: 0s - loss: 0.0239 - acc: 0.998 - ETA: 0s - loss: 0.0243 - acc: 0.998 - ETA: 0s - loss: 0.0236 - acc: 0.998 - 2s 67ms/step - loss: 0.0245 - acc: 0.9987 - val_loss: 0.0524 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.02792\n",
      "Epoch 78/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0341 - acc: 1.000 - ETA: 1s - loss: 0.0271 - acc: 1.000 - ETA: 1s - loss: 0.0224 - acc: 1.000 - ETA: 1s - loss: 0.0184 - acc: 1.000 - ETA: 1s - loss: 0.0166 - acc: 1.000 - ETA: 0s - loss: 0.0225 - acc: 1.000 - ETA: 0s - loss: 0.0303 - acc: 0.992 - ETA: 0s - loss: 0.0274 - acc: 0.993 - ETA: 0s - loss: 0.0256 - acc: 0.993 - ETA: 0s - loss: 0.0279 - acc: 0.994 - ETA: 0s - loss: 0.0270 - acc: 0.994 - ETA: 0s - loss: 0.0253 - acc: 0.995 - ETA: 0s - loss: 0.0254 - acc: 0.995 - ETA: 0s - loss: 0.0258 - acc: 0.995 - ETA: 0s - loss: 0.0254 - acc: 0.996 - ETA: 0s - loss: 0.0247 - acc: 0.996 - ETA: 0s - loss: 0.0237 - acc: 0.996 - ETA: 0s - loss: 0.0236 - acc: 0.996 - ETA: 0s - loss: 0.0230 - acc: 0.997 - ETA: 0s - loss: 0.0229 - acc: 0.997 - ETA: 0s - loss: 0.0226 - acc: 0.997 - 2s 67ms/step - loss: 0.0223 - acc: 0.9974 - val_loss: 0.0277 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.02792 to 0.02770, saving model to NN_v5.model\n",
      "Epoch 79/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0226 - acc: 1.000 - ETA: 1s - loss: 0.0282 - acc: 0.989 - ETA: 1s - loss: 0.0310 - acc: 0.992 - ETA: 1s - loss: 0.0302 - acc: 0.993 - ETA: 1s - loss: 0.0273 - acc: 0.994 - ETA: 1s - loss: 0.0278 - acc: 0.995 - ETA: 1s - loss: 0.0266 - acc: 0.996 - ETA: 1s - loss: 0.0251 - acc: 0.996 - ETA: 0s - loss: 0.0254 - acc: 0.996 - ETA: 0s - loss: 0.0265 - acc: 0.997 - ETA: 0s - loss: 0.0262 - acc: 0.997 - ETA: 0s - loss: 0.0252 - acc: 0.997 - ETA: 0s - loss: 0.0267 - acc: 0.997 - ETA: 0s - loss: 0.0274 - acc: 0.998 - ETA: 0s - loss: 0.0265 - acc: 0.998 - ETA: 0s - loss: 0.0243 - acc: 0.998 - ETA: 0s - loss: 0.0251 - acc: 0.996 - ETA: 0s - loss: 0.0244 - acc: 0.997 - ETA: 0s - loss: 0.0244 - acc: 0.997 - ETA: 0s - loss: 0.0240 - acc: 0.997 - 2s 67ms/step - loss: 0.0239 - acc: 0.9974 - val_loss: 0.0590 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.02770\n",
      "Epoch 80/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0612 - acc: 0.968 - ETA: 1s - loss: 0.0344 - acc: 0.984 - ETA: 1s - loss: 0.0315 - acc: 0.989 - ETA: 1s - loss: 0.0278 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.993 - ETA: 1s - loss: 0.0239 - acc: 0.994 - ETA: 1s - loss: 0.0219 - acc: 0.995 - ETA: 1s - loss: 0.0215 - acc: 0.996 - ETA: 1s - loss: 0.0203 - acc: 0.996 - ETA: 1s - loss: 0.0188 - acc: 0.996 - ETA: 0s - loss: 0.0196 - acc: 0.997 - ETA: 0s - loss: 0.0212 - acc: 0.997 - ETA: 0s - loss: 0.0209 - acc: 0.997 - ETA: 0s - loss: 0.0199 - acc: 0.997 - ETA: 0s - loss: 0.0192 - acc: 0.997 - ETA: 0s - loss: 0.0235 - acc: 0.996 - ETA: 0s - loss: 0.0230 - acc: 0.996 - ETA: 0s - loss: 0.0277 - acc: 0.993 - ETA: 0s - loss: 0.0289 - acc: 0.992 - ETA: 0s - loss: 0.0284 - acc: 0.992 - ETA: 0s - loss: 0.0285 - acc: 0.992 - ETA: 0s - loss: 0.0283 - acc: 0.993 - 2s 83ms/step - loss: 0.0274 - acc: 0.9935 - val_loss: 0.0265 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02770 to 0.02653, saving model to NN_v5.model\n",
      "Epoch 81/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0133 - acc: 1.000 - ETA: 1s - loss: 0.0119 - acc: 1.000 - ETA: 1s - loss: 0.0125 - acc: 1.000 - ETA: 1s - loss: 0.0195 - acc: 0.993 - ETA: 1s - loss: 0.0220 - acc: 0.994 - ETA: 1s - loss: 0.0212 - acc: 0.995 - ETA: 1s - loss: 0.0246 - acc: 0.996 - ETA: 1s - loss: 0.0271 - acc: 0.996 - ETA: 1s - loss: 0.0264 - acc: 0.996 - ETA: 1s - loss: 0.0266 - acc: 0.997 - ETA: 0s - loss: 0.0252 - acc: 0.997 - ETA: 0s - loss: 0.0249 - acc: 0.997 - ETA: 0s - loss: 0.0266 - acc: 0.995 - ETA: 0s - loss: 0.0259 - acc: 0.995 - ETA: 0s - loss: 0.0262 - acc: 0.996 - ETA: 0s - loss: 0.0253 - acc: 0.996 - ETA: 0s - loss: 0.0247 - acc: 0.996 - ETA: 0s - loss: 0.0245 - acc: 0.996 - ETA: 0s - loss: 0.0265 - acc: 0.994 - ETA: 0s - loss: 0.0260 - acc: 0.995 - ETA: 0s - loss: 0.0326 - acc: 0.993 - ETA: 0s - loss: 0.0319 - acc: 0.994 - 2s 78ms/step - loss: 0.0310 - acc: 0.9944 - val_loss: 0.0502 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.02653\n",
      "Epoch 82/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0258 - acc: 1.000 - ETA: 0s - loss: 0.0351 - acc: 0.989 - ETA: 1s - loss: 0.0309 - acc: 0.992 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0367 - acc: 0.989 - ETA: 1s - loss: 0.0340 - acc: 0.991 - ETA: 0s - loss: 0.0318 - acc: 0.992 - ETA: 0s - loss: 0.0290 - acc: 0.993 - ETA: 0s - loss: 0.0278 - acc: 0.993 - ETA: 0s - loss: 0.0267 - acc: 0.994 - ETA: 0s - loss: 0.0249 - acc: 0.994 - ETA: 0s - loss: 0.0236 - acc: 0.995 - ETA: 0s - loss: 0.0233 - acc: 0.995 - ETA: 0s - loss: 0.0227 - acc: 0.995 - ETA: 0s - loss: 0.0227 - acc: 0.996 - ETA: 0s - loss: 0.0219 - acc: 0.996 - ETA: 0s - loss: 0.0219 - acc: 0.996 - ETA: 0s - loss: 0.0224 - acc: 0.995 - ETA: 0s - loss: 0.0222 - acc: 0.995 - ETA: 0s - loss: 0.0224 - acc: 0.995 - ETA: 0s - loss: 0.0232 - acc: 0.995 - 2s 66ms/step - loss: 0.0230 - acc: 0.9961 - val_loss: 0.0598 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.02653\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0326 - acc: 1.000 - ETA: 1s - loss: 0.0311 - acc: 0.989 - ETA: 1s - loss: 0.0289 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.993 - ETA: 1s - loss: 0.0269 - acc: 0.994 - ETA: 0s - loss: 0.0253 - acc: 0.995 - ETA: 0s - loss: 0.0283 - acc: 0.996 - ETA: 0s - loss: 0.0257 - acc: 0.996 - ETA: 0s - loss: 0.0240 - acc: 0.996 - ETA: 0s - loss: 0.0231 - acc: 0.997 - ETA: 0s - loss: 0.0218 - acc: 0.997 - ETA: 0s - loss: 0.0223 - acc: 0.997 - ETA: 0s - loss: 0.0234 - acc: 0.997 - ETA: 0s - loss: 0.0230 - acc: 0.997 - ETA: 0s - loss: 0.0217 - acc: 0.998 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0214 - acc: 0.998 - ETA: 0s - loss: 0.0208 - acc: 0.998 - ETA: 0s - loss: 0.0207 - acc: 0.998 - ETA: 0s - loss: 0.0207 - acc: 0.998 - ETA: 0s - loss: 0.0200 - acc: 0.998 - 2s 63ms/step - loss: 0.0196 - acc: 0.9987 - val_loss: 0.0541 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.02653\n",
      "Epoch 84/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0343 - acc: 1.000 - ETA: 1s - loss: 0.0222 - acc: 1.000 - ETA: 1s - loss: 0.0245 - acc: 1.000 - ETA: 1s - loss: 0.0216 - acc: 1.000 - ETA: 1s - loss: 0.0227 - acc: 1.000 - ETA: 1s - loss: 0.0243 - acc: 1.000 - ETA: 0s - loss: 0.0257 - acc: 1.000 - ETA: 0s - loss: 0.0236 - acc: 1.000 - ETA: 0s - loss: 0.0231 - acc: 1.000 - ETA: 0s - loss: 0.0263 - acc: 1.000 - ETA: 0s - loss: 0.0244 - acc: 1.000 - ETA: 0s - loss: 0.0230 - acc: 1.000 - ETA: 0s - loss: 0.0231 - acc: 1.000 - ETA: 0s - loss: 0.0232 - acc: 1.000 - ETA: 0s - loss: 0.0248 - acc: 0.998 - ETA: 0s - loss: 0.0241 - acc: 0.998 - ETA: 0s - loss: 0.0273 - acc: 0.996 - ETA: 0s - loss: 0.0281 - acc: 0.996 - ETA: 0s - loss: 0.0270 - acc: 0.996 - ETA: 0s - loss: 0.0265 - acc: 0.997 - ETA: 0s - loss: 0.0262 - acc: 0.997 - 1s 62ms/step - loss: 0.0265 - acc: 0.9974 - val_loss: 0.0482 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.02653\n",
      "Epoch 85/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0113 - acc: 1.000 - ETA: 0s - loss: 0.0179 - acc: 1.000 - ETA: 0s - loss: 0.0250 - acc: 1.000 - ETA: 1s - loss: 0.0455 - acc: 0.987 - ETA: 0s - loss: 0.0387 - acc: 0.989 - ETA: 0s - loss: 0.0352 - acc: 0.991 - ETA: 0s - loss: 0.0327 - acc: 0.992 - ETA: 0s - loss: 0.0306 - acc: 0.993 - ETA: 0s - loss: 0.0345 - acc: 0.990 - ETA: 0s - loss: 0.0359 - acc: 0.988 - ETA: 0s - loss: 0.0370 - acc: 0.987 - ETA: 0s - loss: 0.0363 - acc: 0.988 - ETA: 0s - loss: 0.0364 - acc: 0.988 - ETA: 0s - loss: 0.0351 - acc: 0.989 - ETA: 0s - loss: 0.0337 - acc: 0.990 - ETA: 0s - loss: 0.0343 - acc: 0.989 - ETA: 0s - loss: 0.0335 - acc: 0.989 - ETA: 0s - loss: 0.0321 - acc: 0.990 - ETA: 0s - loss: 0.0325 - acc: 0.990 - ETA: 0s - loss: 0.0315 - acc: 0.991 - ETA: 0s - loss: 0.0312 - acc: 0.991 - ETA: 0s - loss: 0.0304 - acc: 0.991 - 1s 62ms/step - loss: 0.0294 - acc: 0.9922 - val_loss: 0.0369 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.02653\n",
      "Epoch 86/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0067 - acc: 1.000 - ETA: 0s - loss: 0.0135 - acc: 1.000 - ETA: 0s - loss: 0.0160 - acc: 1.000 - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 1s - loss: 0.0177 - acc: 1.000 - ETA: 0s - loss: 0.0199 - acc: 1.000 - ETA: 0s - loss: 0.0214 - acc: 1.000 - ETA: 0s - loss: 0.0203 - acc: 1.000 - ETA: 0s - loss: 0.0222 - acc: 1.000 - ETA: 0s - loss: 0.0213 - acc: 1.000 - ETA: 0s - loss: 0.0211 - acc: 1.000 - ETA: 0s - loss: 0.0215 - acc: 1.000 - ETA: 0s - loss: 0.0210 - acc: 1.000 - ETA: 0s - loss: 0.0223 - acc: 1.000 - ETA: 0s - loss: 0.0220 - acc: 1.000 - ETA: 0s - loss: 0.0236 - acc: 1.000 - ETA: 0s - loss: 0.0227 - acc: 1.000 - ETA: 0s - loss: 0.0223 - acc: 1.000 - ETA: 0s - loss: 0.0215 - acc: 1.000 - ETA: 0s - loss: 0.0209 - acc: 1.000 - ETA: 0s - loss: 0.0203 - acc: 1.000 - ETA: 0s - loss: 0.0198 - acc: 1.000 - 2s 63ms/step - loss: 0.0213 - acc: 0.9987 - val_loss: 0.0322 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02653\n",
      "Epoch 87/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0073 - acc: 1.000 - ETA: 0s - loss: 0.0113 - acc: 1.000 - ETA: 1s - loss: 0.0141 - acc: 1.000 - ETA: 1s - loss: 0.0148 - acc: 1.000 - ETA: 1s - loss: 0.0202 - acc: 0.994 - ETA: 1s - loss: 0.0185 - acc: 0.995 - ETA: 0s - loss: 0.0179 - acc: 0.996 - ETA: 0s - loss: 0.0235 - acc: 0.993 - ETA: 0s - loss: 0.0232 - acc: 0.993 - ETA: 0s - loss: 0.0229 - acc: 0.994 - ETA: 0s - loss: 0.0243 - acc: 0.994 - ETA: 0s - loss: 0.0255 - acc: 0.992 - ETA: 0s - loss: 0.0263 - acc: 0.993 - ETA: 0s - loss: 0.0261 - acc: 0.993 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0297 - acc: 0.990 - ETA: 0s - loss: 0.0293 - acc: 0.991 - ETA: 0s - loss: 0.0283 - acc: 0.991 - ETA: 0s - loss: 0.0291 - acc: 0.990 - ETA: 0s - loss: 0.0295 - acc: 0.991 - ETA: 0s - loss: 0.0304 - acc: 0.990 - ETA: 0s - loss: 0.0299 - acc: 0.990 - 2s 66ms/step - loss: 0.0290 - acc: 0.9909 - val_loss: 0.0473 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02653\n",
      "Epoch 88/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0284 - acc: 1.000 - ETA: 0s - loss: 0.0194 - acc: 1.000 - ETA: 1s - loss: 0.0152 - acc: 1.000 - ETA: 1s - loss: 0.0242 - acc: 0.993 - ETA: 1s - loss: 0.0230 - acc: 0.994 - ETA: 1s - loss: 0.0203 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.996 - ETA: 0s - loss: 0.0198 - acc: 0.996 - ETA: 0s - loss: 0.0211 - acc: 0.993 - ETA: 0s - loss: 0.0220 - acc: 0.991 - ETA: 0s - loss: 0.0220 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.990 - ETA: 0s - loss: 0.0256 - acc: 0.988 - ETA: 0s - loss: 0.0255 - acc: 0.989 - ETA: 0s - loss: 0.0245 - acc: 0.990 - ETA: 0s - loss: 0.0289 - acc: 0.989 - ETA: 0s - loss: 0.0281 - acc: 0.989 - ETA: 0s - loss: 0.0271 - acc: 0.990 - ETA: 0s - loss: 0.0278 - acc: 0.990 - ETA: 0s - loss: 0.0270 - acc: 0.991 - ETA: 0s - loss: 0.0262 - acc: 0.991 - ETA: 0s - loss: 0.0276 - acc: 0.990 - 2s 67ms/step - loss: 0.0276 - acc: 0.9909 - val_loss: 0.0310 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02653\n",
      "Epoch 89/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0459 - acc: 0.968 - ETA: 0s - loss: 0.0331 - acc: 0.989 - ETA: 1s - loss: 0.0337 - acc: 0.984 - ETA: 1s - loss: 0.0275 - acc: 0.987 - ETA: 1s - loss: 0.0257 - acc: 0.989 - ETA: 1s - loss: 0.0230 - acc: 0.991 - ETA: 0s - loss: 0.0219 - acc: 0.992 - ETA: 0s - loss: 0.0215 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0335 - acc: 0.991 - ETA: 0s - loss: 0.0311 - acc: 0.992 - ETA: 0s - loss: 0.0301 - acc: 0.992 - ETA: 0s - loss: 0.0326 - acc: 0.991 - ETA: 0s - loss: 0.0314 - acc: 0.991 - ETA: 0s - loss: 0.0304 - acc: 0.992 - ETA: 0s - loss: 0.0301 - acc: 0.992 - ETA: 0s - loss: 0.0296 - acc: 0.993 - ETA: 0s - loss: 0.0289 - acc: 0.993 - ETA: 0s - loss: 0.0281 - acc: 0.993 - ETA: 0s - loss: 0.0280 - acc: 0.994 - ETA: 0s - loss: 0.0286 - acc: 0.994 - ETA: 0s - loss: 0.0281 - acc: 0.994 - 2s 65ms/step - loss: 0.0272 - acc: 0.9948 - val_loss: 0.0406 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02653\n",
      "Epoch 90/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0204 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - ETA: 0s - loss: 0.0186 - acc: 1.000 - ETA: 0s - loss: 0.0176 - acc: 1.000 - ETA: 0s - loss: 0.0173 - acc: 1.000 - ETA: 0s - loss: 0.0164 - acc: 1.000 - ETA: 0s - loss: 0.0180 - acc: 1.000 - ETA: 0s - loss: 0.0210 - acc: 1.000 - ETA: 0s - loss: 0.0211 - acc: 1.000 - ETA: 0s - loss: 0.0211 - acc: 1.000 - ETA: 0s - loss: 0.0214 - acc: 1.000 - ETA: 0s - loss: 0.0207 - acc: 1.000 - ETA: 0s - loss: 0.0202 - acc: 1.000 - ETA: 0s - loss: 0.0199 - acc: 1.000 - ETA: 0s - loss: 0.0209 - acc: 0.998 - ETA: 0s - loss: 0.0208 - acc: 0.998 - ETA: 0s - loss: 0.0222 - acc: 0.996 - ETA: 0s - loss: 0.0230 - acc: 0.995 - ETA: 0s - loss: 0.0230 - acc: 0.995 - ETA: 0s - loss: 0.0258 - acc: 0.994 - ETA: 0s - loss: 0.0262 - acc: 0.994 - 2s 65ms/step - loss: 0.0265 - acc: 0.9948 - val_loss: 0.0346 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02653\n",
      "Epoch 91/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0631 - acc: 0.968 - ETA: 1s - loss: 0.0382 - acc: 0.984 - ETA: 1s - loss: 0.0343 - acc: 0.989 - ETA: 1s - loss: 0.0306 - acc: 0.992 - ETA: 1s - loss: 0.0276 - acc: 0.994 - ETA: 1s - loss: 0.0243 - acc: 0.995 - ETA: 1s - loss: 0.0226 - acc: 0.996 - ETA: 1s - loss: 0.0219 - acc: 0.996 - ETA: 0s - loss: 0.0224 - acc: 0.996 - ETA: 0s - loss: 0.0210 - acc: 0.997 - ETA: 0s - loss: 0.0259 - acc: 0.994 - ETA: 0s - loss: 0.0273 - acc: 0.995 - ETA: 0s - loss: 0.0269 - acc: 0.995 - ETA: 0s - loss: 0.0277 - acc: 0.995 - ETA: 0s - loss: 0.0272 - acc: 0.996 - ETA: 0s - loss: 0.0290 - acc: 0.994 - ETA: 0s - loss: 0.0287 - acc: 0.994 - ETA: 0s - loss: 0.0285 - acc: 0.995 - ETA: 0s - loss: 0.0286 - acc: 0.993 - ETA: 0s - loss: 0.0278 - acc: 0.994 - ETA: 0s - loss: 0.0297 - acc: 0.992 - ETA: 0s - loss: 0.0310 - acc: 0.993 - 2s 70ms/step - loss: 0.0317 - acc: 0.9935 - val_loss: 0.0327 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02653\n",
      "Epoch 92/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0132 - acc: 1.000 - ETA: 1s - loss: 0.0112 - acc: 1.000 - ETA: 1s - loss: 0.0112 - acc: 1.000 - ETA: 1s - loss: 0.0140 - acc: 1.000 - ETA: 0s - loss: 0.0156 - acc: 1.000 - ETA: 0s - loss: 0.0150 - acc: 1.000 - ETA: 0s - loss: 0.0193 - acc: 1.000 - ETA: 0s - loss: 0.0193 - acc: 1.000 - ETA: 0s - loss: 0.0187 - acc: 1.000 - ETA: 0s - loss: 0.0180 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0172 - acc: 1.000 - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 0s - loss: 0.0165 - acc: 1.000 - ETA: 0s - loss: 0.0160 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - ETA: 0s - loss: 0.0158 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0165 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - ETA: 0s - loss: 0.0162 - acc: 1.000 - 2s 63ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02653\n",
      "Epoch 93/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0198 - acc: 1.000 - ETA: 1s - loss: 0.0158 - acc: 1.000 - ETA: 1s - loss: 0.0134 - acc: 1.000 - ETA: 1s - loss: 0.0137 - acc: 1.000 - ETA: 1s - loss: 0.0150 - acc: 1.000 - ETA: 0s - loss: 0.0145 - acc: 1.000 - ETA: 0s - loss: 0.0164 - acc: 1.000 - ETA: 0s - loss: 0.0152 - acc: 1.000 - ETA: 0s - loss: 0.0149 - acc: 1.000 - ETA: 0s - loss: 0.0153 - acc: 1.000 - ETA: 0s - loss: 0.0146 - acc: 1.000 - ETA: 0s - loss: 0.0138 - acc: 1.000 - ETA: 0s - loss: 0.0175 - acc: 0.997 - ETA: 0s - loss: 0.0172 - acc: 0.998 - ETA: 0s - loss: 0.0192 - acc: 0.998 - ETA: 0s - loss: 0.0201 - acc: 0.998 - ETA: 0s - loss: 0.0202 - acc: 0.998 - ETA: 0s - loss: 0.0198 - acc: 0.998 - ETA: 0s - loss: 0.0206 - acc: 0.998 - ETA: 0s - loss: 0.0204 - acc: 0.998 - ETA: 0s - loss: 0.0216 - acc: 0.997 - 2s 66ms/step - loss: 0.0210 - acc: 0.9974 - val_loss: 0.0447 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02653\n",
      "Epoch 94/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0123 - acc: 1.000 - ETA: 0s - loss: 0.0365 - acc: 0.989 - ETA: 0s - loss: 0.0309 - acc: 0.992 - ETA: 1s - loss: 0.0274 - acc: 0.993 - ETA: 1s - loss: 0.0304 - acc: 0.994 - ETA: 0s - loss: 0.0289 - acc: 0.995 - ETA: 0s - loss: 0.0268 - acc: 0.996 - ETA: 0s - loss: 0.0265 - acc: 0.996 - ETA: 0s - loss: 0.0280 - acc: 0.996 - ETA: 0s - loss: 0.0262 - acc: 0.997 - ETA: 0s - loss: 0.0278 - acc: 0.997 - ETA: 0s - loss: 0.0263 - acc: 0.997 - ETA: 0s - loss: 0.0275 - acc: 0.995 - ETA: 0s - loss: 0.0276 - acc: 0.995 - ETA: 0s - loss: 0.0273 - acc: 0.996 - ETA: 0s - loss: 0.0266 - acc: 0.996 - ETA: 0s - loss: 0.0254 - acc: 0.996 - ETA: 0s - loss: 0.0259 - acc: 0.996 - ETA: 0s - loss: 0.0250 - acc: 0.996 - ETA: 0s - loss: 0.0242 - acc: 0.997 - ETA: 0s - loss: 0.0245 - acc: 0.995 - ETA: 0s - loss: 0.0237 - acc: 0.995 - 2s 64ms/step - loss: 0.0250 - acc: 0.9948 - val_loss: 0.0373 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02653\n",
      "Epoch 95/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0153 - acc: 1.000 - ETA: 0s - loss: 0.0265 - acc: 0.989 - ETA: 0s - loss: 0.0328 - acc: 0.984 - ETA: 1s - loss: 0.0325 - acc: 0.987 - ETA: 1s - loss: 0.0297 - acc: 0.989 - ETA: 0s - loss: 0.0264 - acc: 0.991 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0297 - acc: 0.993 - ETA: 0s - loss: 0.0279 - acc: 0.994 - ETA: 0s - loss: 0.0313 - acc: 0.992 - ETA: 0s - loss: 0.0331 - acc: 0.992 - ETA: 0s - loss: 0.0313 - acc: 0.993 - ETA: 0s - loss: 0.0308 - acc: 0.993 - ETA: 0s - loss: 0.0291 - acc: 0.994 - ETA: 0s - loss: 0.0286 - acc: 0.994 - ETA: 0s - loss: 0.0284 - acc: 0.994 - ETA: 0s - loss: 0.0273 - acc: 0.995 - ETA: 0s - loss: 0.0280 - acc: 0.993 - ETA: 0s - loss: 0.0278 - acc: 0.994 - ETA: 0s - loss: 0.0270 - acc: 0.994 - ETA: 0s - loss: 0.0262 - acc: 0.994 - 2s 64ms/step - loss: 0.0254 - acc: 0.9948 - val_loss: 0.0348 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02653\n",
      "Epoch 96/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0353 - acc: 0.968 - ETA: 1s - loss: 0.0213 - acc: 0.989 - ETA: 1s - loss: 0.0235 - acc: 0.992 - ETA: 1s - loss: 0.0252 - acc: 0.993 - ETA: 1s - loss: 0.0218 - acc: 0.994 - ETA: 0s - loss: 0.0211 - acc: 0.995 - ETA: 0s - loss: 0.0192 - acc: 0.996 - ETA: 0s - loss: 0.0254 - acc: 0.993 - ETA: 0s - loss: 0.0234 - acc: 0.993 - ETA: 0s - loss: 0.0290 - acc: 0.991 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0265 - acc: 0.992 - ETA: 0s - loss: 0.0285 - acc: 0.991 - ETA: 0s - loss: 0.0286 - acc: 0.991 - ETA: 0s - loss: 0.0289 - acc: 0.992 - ETA: 0s - loss: 0.0297 - acc: 0.992 - ETA: 0s - loss: 0.0285 - acc: 0.993 - ETA: 0s - loss: 0.0280 - acc: 0.993 - ETA: 0s - loss: 0.0267 - acc: 0.993 - ETA: 0s - loss: 0.0269 - acc: 0.994 - ETA: 0s - loss: 0.0258 - acc: 0.994 - ETA: 0s - loss: 0.0252 - acc: 0.994 - 2s 66ms/step - loss: 0.0249 - acc: 0.9948 - val_loss: 0.0332 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02653\n",
      "Epoch 97/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0290 - acc: 1.000 - ETA: 1s - loss: 0.0167 - acc: 1.000 - ETA: 1s - loss: 0.0219 - acc: 0.992 - ETA: 1s - loss: 0.0196 - acc: 0.993 - ETA: 1s - loss: 0.0174 - acc: 0.994 - ETA: 1s - loss: 0.0305 - acc: 0.991 - ETA: 1s - loss: 0.0281 - acc: 0.992 - ETA: 0s - loss: 0.0314 - acc: 0.989 - ETA: 0s - loss: 0.0292 - acc: 0.990 - ETA: 0s - loss: 0.0307 - acc: 0.989 - ETA: 0s - loss: 0.0307 - acc: 0.988 - ETA: 0s - loss: 0.0290 - acc: 0.988 - ETA: 0s - loss: 0.0280 - acc: 0.989 - ETA: 0s - loss: 0.0276 - acc: 0.990 - ETA: 0s - loss: 0.0272 - acc: 0.990 - ETA: 0s - loss: 0.0269 - acc: 0.991 - ETA: 0s - loss: 0.0270 - acc: 0.991 - ETA: 0s - loss: 0.0259 - acc: 0.992 - ETA: 0s - loss: 0.0254 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.992 - ETA: 0s - loss: 0.0333 - acc: 0.991 - 2s 68ms/step - loss: 0.0322 - acc: 0.9922 - val_loss: 0.0278 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02653\n",
      "Epoch 98/300\n",
      "24/24 [==============================] - ETA: 1s - loss: 0.0182 - acc: 1.000 - ETA: 1s - loss: 0.0143 - acc: 1.000 - ETA: 1s - loss: 0.0143 - acc: 1.000 - ETA: 1s - loss: 0.0143 - acc: 1.000 - ETA: 1s - loss: 0.0161 - acc: 1.000 - ETA: 1s - loss: 0.0199 - acc: 1.000 - ETA: 0s - loss: 0.0204 - acc: 1.000 - ETA: 0s - loss: 0.0194 - acc: 1.000 - ETA: 0s - loss: 0.0183 - acc: 1.000 - ETA: 0s - loss: 0.0171 - acc: 1.000 - ETA: 0s - loss: 0.0160 - acc: 1.000 - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 0s - loss: 0.0171 - acc: 1.000 - ETA: 0s - loss: 0.0170 - acc: 1.000 - ETA: 0s - loss: 0.0174 - acc: 1.000 - ETA: 0s - loss: 0.0204 - acc: 0.998 - ETA: 0s - loss: 0.0200 - acc: 0.998 - ETA: 0s - loss: 0.0204 - acc: 0.998 - ETA: 0s - loss: 0.0200 - acc: 0.998 - ETA: 0s - loss: 0.0194 - acc: 0.998 - ETA: 0s - loss: 0.0190 - acc: 0.998 - 2s 65ms/step - loss: 0.0191 - acc: 0.9987 - val_loss: 0.0193 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.02653 to 0.01934, saving model to NN_v5.model\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0182 - acc: 1.000 - ETA: 0s - loss: 0.0337 - acc: 0.989 - ETA: 0s - loss: 0.0307 - acc: 0.992 - ETA: 1s - loss: 0.0254 - acc: 0.993 - ETA: 0s - loss: 0.0270 - acc: 0.994 - ETA: 0s - loss: 0.0299 - acc: 0.995 - ETA: 0s - loss: 0.0279 - acc: 0.996 - ETA: 0s - loss: 0.0253 - acc: 0.996 - ETA: 0s - loss: 0.0246 - acc: 0.996 - ETA: 0s - loss: 0.0233 - acc: 0.997 - ETA: 0s - loss: 0.0228 - acc: 0.997 - ETA: 0s - loss: 0.0213 - acc: 0.997 - ETA: 0s - loss: 0.0207 - acc: 0.997 - ETA: 0s - loss: 0.0211 - acc: 0.998 - ETA: 0s - loss: 0.0205 - acc: 0.998 - ETA: 0s - loss: 0.0231 - acc: 0.996 - ETA: 0s - loss: 0.0223 - acc: 0.996 - ETA: 0s - loss: 0.0218 - acc: 0.996 - ETA: 0s - loss: 0.0210 - acc: 0.997 - ETA: 0s - loss: 0.0213 - acc: 0.997 - ETA: 0s - loss: 0.0207 - acc: 0.997 - 2s 64ms/step - loss: 0.0200 - acc: 0.9974 - val_loss: 0.0279 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.01934\n",
      "Epoch 100/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0252 - acc: 1.000 - ETA: 1s - loss: 0.0514 - acc: 0.989 - ETA: 1s - loss: 0.0586 - acc: 0.984 - ETA: 1s - loss: 0.0507 - acc: 0.987 - ETA: 1s - loss: 0.0490 - acc: 0.989 - ETA: 0s - loss: 0.0429 - acc: 0.991 - ETA: 0s - loss: 0.0379 - acc: 0.992 - ETA: 0s - loss: 0.0356 - acc: 0.993 - ETA: 0s - loss: 0.0342 - acc: 0.993 - ETA: 0s - loss: 0.0333 - acc: 0.994 - ETA: 0s - loss: 0.0326 - acc: 0.994 - ETA: 0s - loss: 0.0308 - acc: 0.995 - ETA: 0s - loss: 0.0301 - acc: 0.995 - ETA: 0s - loss: 0.0296 - acc: 0.995 - ETA: 0s - loss: 0.0282 - acc: 0.996 - ETA: 0s - loss: 0.0287 - acc: 0.996 - ETA: 0s - loss: 0.0278 - acc: 0.996 - ETA: 0s - loss: 0.0267 - acc: 0.996 - ETA: 0s - loss: 0.0262 - acc: 0.996 - ETA: 0s - loss: 0.0260 - acc: 0.997 - ETA: 0s - loss: 0.0253 - acc: 0.997 - ETA: 0s - loss: 0.0249 - acc: 0.997 - 2s 66ms/step - loss: 0.0246 - acc: 0.9974 - val_loss: 0.0499 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01934\n",
      "Epoch 101/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0104 - acc: 1.000 - ETA: 1s - loss: 0.0082 - acc: 1.000 - ETA: 1s - loss: 0.0073 - acc: 1.000 - ETA: 1s - loss: 0.0096 - acc: 1.000 - ETA: 1s - loss: 0.0121 - acc: 1.000 - ETA: 1s - loss: 0.0141 - acc: 1.000 - ETA: 1s - loss: 0.0124 - acc: 1.000 - ETA: 1s - loss: 0.0166 - acc: 1.000 - ETA: 1s - loss: 0.0156 - acc: 1.000 - ETA: 1s - loss: 0.0170 - acc: 1.000 - ETA: 0s - loss: 0.0170 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0173 - acc: 1.000 - ETA: 0s - loss: 0.0171 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0180 - acc: 0.998 - ETA: 0s - loss: 0.0179 - acc: 0.998 - ETA: 0s - loss: 0.0176 - acc: 0.998 - ETA: 0s - loss: 0.0169 - acc: 0.998 - ETA: 0s - loss: 0.0169 - acc: 0.998 - ETA: 0s - loss: 0.0175 - acc: 0.998 - ETA: 0s - loss: 0.0171 - acc: 0.998 - ETA: 0s - loss: 0.0176 - acc: 0.998 - 2s 72ms/step - loss: 0.0186 - acc: 0.9987 - val_loss: 0.0230 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.01934\n",
      "Epoch 102/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0149 - acc: 1.000 - ETA: 1s - loss: 0.0127 - acc: 1.000 - ETA: 1s - loss: 0.0126 - acc: 1.000 - ETA: 1s - loss: 0.0129 - acc: 1.000 - ETA: 1s - loss: 0.0130 - acc: 1.000 - ETA: 1s - loss: 0.0127 - acc: 1.000 - ETA: 1s - loss: 0.0146 - acc: 1.000 - ETA: 1s - loss: 0.0136 - acc: 1.000 - ETA: 1s - loss: 0.0209 - acc: 0.993 - ETA: 1s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0245 - acc: 0.991 - ETA: 0s - loss: 0.0230 - acc: 0.992 - ETA: 0s - loss: 0.0230 - acc: 0.992 - ETA: 0s - loss: 0.0221 - acc: 0.993 - ETA: 0s - loss: 0.0224 - acc: 0.993 - ETA: 0s - loss: 0.0228 - acc: 0.994 - ETA: 0s - loss: 0.0282 - acc: 0.993 - ETA: 0s - loss: 0.0274 - acc: 0.993 - ETA: 0s - loss: 0.0262 - acc: 0.993 - ETA: 0s - loss: 0.0261 - acc: 0.994 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0274 - acc: 0.993 - 2s 73ms/step - loss: 0.0290 - acc: 0.9922 - val_loss: 0.0264 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.01934\n",
      "Epoch 103/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0231 - acc: 1.000 - ETA: 1s - loss: 0.0210 - acc: 1.000 - ETA: 1s - loss: 0.0165 - acc: 1.000 - ETA: 1s - loss: 0.0140 - acc: 1.000 - ETA: 1s - loss: 0.0179 - acc: 1.000 - ETA: 1s - loss: 0.0175 - acc: 1.000 - ETA: 1s - loss: 0.0172 - acc: 1.000 - ETA: 1s - loss: 0.0185 - acc: 1.000 - ETA: 1s - loss: 0.0193 - acc: 1.000 - ETA: 0s - loss: 0.0180 - acc: 1.000 - ETA: 0s - loss: 0.0238 - acc: 0.997 - ETA: 0s - loss: 0.0238 - acc: 0.997 - ETA: 0s - loss: 0.0236 - acc: 0.997 - ETA: 0s - loss: 0.0237 - acc: 0.997 - ETA: 0s - loss: 0.0231 - acc: 0.997 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0208 - acc: 0.998 - ETA: 0s - loss: 0.0208 - acc: 0.998 - ETA: 0s - loss: 0.0220 - acc: 0.998 - ETA: 0s - loss: 0.0217 - acc: 0.998 - ETA: 0s - loss: 0.0216 - acc: 0.998 - ETA: 0s - loss: 0.0210 - acc: 0.998 - 2s 71ms/step - loss: 0.0207 - acc: 0.9987 - val_loss: 0.0308 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.01934\n",
      "Epoch 104/300\n",
      "24/24 [==============================] - ETA: 1s - loss: 0.0172 - acc: 1.000 - ETA: 1s - loss: 0.0166 - acc: 1.000 - ETA: 1s - loss: 0.0184 - acc: 1.000 - ETA: 1s - loss: 0.0160 - acc: 1.000 - ETA: 1s - loss: 0.0200 - acc: 1.000 - ETA: 1s - loss: 0.0202 - acc: 1.000 - ETA: 1s - loss: 0.0200 - acc: 1.000 - ETA: 1s - loss: 0.0200 - acc: 1.000 - ETA: 1s - loss: 0.0189 - acc: 1.000 - ETA: 0s - loss: 0.0235 - acc: 0.996 - ETA: 0s - loss: 0.0237 - acc: 0.997 - ETA: 0s - loss: 0.0246 - acc: 0.994 - ETA: 0s - loss: 0.0289 - acc: 0.992 - ETA: 0s - loss: 0.0286 - acc: 0.993 - ETA: 0s - loss: 0.0293 - acc: 0.993 - ETA: 0s - loss: 0.0282 - acc: 0.994 - ETA: 0s - loss: 0.0280 - acc: 0.994 - ETA: 0s - loss: 0.0278 - acc: 0.995 - ETA: 0s - loss: 0.0265 - acc: 0.995 - ETA: 0s - loss: 0.0256 - acc: 0.995 - ETA: 0s - loss: 0.0248 - acc: 0.995 - ETA: 0s - loss: 0.0264 - acc: 0.995 - 2s 69ms/step - loss: 0.0256 - acc: 0.9961 - val_loss: 0.0305 - val_acc: 0.9948\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.01934\n",
      "Epoch 105/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0240 - acc: 1.000 - ETA: 0s - loss: 0.0331 - acc: 1.000 - ETA: 0s - loss: 0.0339 - acc: 1.000 - ETA: 0s - loss: 0.0311 - acc: 1.000 - ETA: 0s - loss: 0.0447 - acc: 0.994 - ETA: 0s - loss: 0.0393 - acc: 0.995 - ETA: 0s - loss: 0.0397 - acc: 0.996 - ETA: 0s - loss: 0.0365 - acc: 0.996 - ETA: 0s - loss: 0.0366 - acc: 0.996 - ETA: 0s - loss: 0.0344 - acc: 0.997 - ETA: 0s - loss: 0.0323 - acc: 0.997 - ETA: 0s - loss: 0.0311 - acc: 0.997 - ETA: 0s - loss: 0.0294 - acc: 0.997 - ETA: 0s - loss: 0.0295 - acc: 0.997 - ETA: 0s - loss: 0.0293 - acc: 0.998 - ETA: 0s - loss: 0.0282 - acc: 0.998 - ETA: 0s - loss: 0.0277 - acc: 0.998 - ETA: 0s - loss: 0.0267 - acc: 0.998 - ETA: 0s - loss: 0.0273 - acc: 0.998 - ETA: 0s - loss: 0.0285 - acc: 0.997 - ETA: 0s - loss: 0.0277 - acc: 0.997 - 1s 60ms/step - loss: 0.0268 - acc: 0.9974 - val_loss: 0.0563 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.01934\n",
      "Epoch 106/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0431 - acc: 0.968 - ETA: 1s - loss: 0.0298 - acc: 0.989 - ETA: 1s - loss: 0.0250 - acc: 0.992 - ETA: 1s - loss: 0.0209 - acc: 0.993 - ETA: 1s - loss: 0.0232 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.995 - ETA: 1s - loss: 0.0204 - acc: 0.996 - ETA: 0s - loss: 0.0199 - acc: 0.996 - ETA: 0s - loss: 0.0210 - acc: 0.996 - ETA: 0s - loss: 0.0218 - acc: 0.997 - ETA: 0s - loss: 0.0203 - acc: 0.997 - ETA: 0s - loss: 0.0193 - acc: 0.997 - ETA: 0s - loss: 0.0190 - acc: 0.997 - ETA: 0s - loss: 0.0204 - acc: 0.997 - ETA: 0s - loss: 0.0204 - acc: 0.998 - ETA: 0s - loss: 0.0201 - acc: 0.998 - ETA: 0s - loss: 0.0199 - acc: 0.998 - ETA: 0s - loss: 0.0197 - acc: 0.998 - ETA: 0s - loss: 0.0184 - acc: 0.998 - ETA: 0s - loss: 0.0201 - acc: 0.998 - ETA: 0s - loss: 0.0234 - acc: 0.997 - 2s 65ms/step - loss: 0.0228 - acc: 0.9974 - val_loss: 0.0323 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.01934\n",
      "Epoch 107/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0381 - acc: 0.968 - ETA: 0s - loss: 0.0200 - acc: 0.989 - ETA: 0s - loss: 0.0191 - acc: 0.992 - ETA: 1s - loss: 0.0187 - acc: 0.993 - ETA: 1s - loss: 0.0169 - acc: 0.994 - ETA: 1s - loss: 0.0160 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - ETA: 0s - loss: 0.0172 - acc: 0.996 - ETA: 0s - loss: 0.0193 - acc: 0.997 - ETA: 0s - loss: 0.0235 - acc: 0.994 - ETA: 0s - loss: 0.0225 - acc: 0.995 - ETA: 0s - loss: 0.0230 - acc: 0.995 - ETA: 0s - loss: 0.0227 - acc: 0.995 - ETA: 0s - loss: 0.0228 - acc: 0.996 - ETA: 0s - loss: 0.0225 - acc: 0.996 - ETA: 0s - loss: 0.0240 - acc: 0.994 - ETA: 0s - loss: 0.0234 - acc: 0.995 - ETA: 0s - loss: 0.0226 - acc: 0.995 - ETA: 0s - loss: 0.0234 - acc: 0.995 - ETA: 0s - loss: 0.0250 - acc: 0.994 - 2s 65ms/step - loss: 0.0251 - acc: 0.9948 - val_loss: 0.0333 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.01934\n",
      "Epoch 108/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0028 - acc: 1.000 - ETA: 0s - loss: 0.0085 - acc: 1.000 - ETA: 1s - loss: 0.0086 - acc: 1.000 - ETA: 0s - loss: 0.0111 - acc: 1.000 - ETA: 1s - loss: 0.0106 - acc: 1.000 - ETA: 1s - loss: 0.0103 - acc: 1.000 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0146 - acc: 0.996 - ETA: 0s - loss: 0.0177 - acc: 0.996 - ETA: 0s - loss: 0.0164 - acc: 0.997 - ETA: 0s - loss: 0.0163 - acc: 0.997 - ETA: 0s - loss: 0.0157 - acc: 0.997 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0184 - acc: 0.995 - ETA: 0s - loss: 0.0200 - acc: 0.996 - ETA: 0s - loss: 0.0193 - acc: 0.996 - ETA: 0s - loss: 0.0197 - acc: 0.996 - ETA: 0s - loss: 0.0192 - acc: 0.996 - ETA: 0s - loss: 0.0194 - acc: 0.996 - ETA: 0s - loss: 0.0238 - acc: 0.995 - ETA: 0s - loss: 0.0231 - acc: 0.995 - ETA: 0s - loss: 0.0249 - acc: 0.994 - 2s 66ms/step - loss: 0.0257 - acc: 0.9935 - val_loss: 0.0547 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.01934\n",
      "Epoch 109/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0187 - acc: 1.000 - ETA: 1s - loss: 0.0232 - acc: 1.000 - ETA: 1s - loss: 0.0273 - acc: 0.992 - ETA: 1s - loss: 0.0238 - acc: 0.993 - ETA: 1s - loss: 0.0234 - acc: 0.994 - ETA: 1s - loss: 0.0210 - acc: 0.995 - ETA: 1s - loss: 0.0214 - acc: 0.996 - ETA: 0s - loss: 0.0226 - acc: 0.996 - ETA: 0s - loss: 0.0211 - acc: 0.996 - ETA: 0s - loss: 0.0203 - acc: 0.997 - ETA: 0s - loss: 0.0199 - acc: 0.997 - ETA: 0s - loss: 0.0208 - acc: 0.997 - ETA: 0s - loss: 0.0214 - acc: 0.997 - ETA: 0s - loss: 0.0207 - acc: 0.997 - ETA: 0s - loss: 0.0221 - acc: 0.998 - ETA: 0s - loss: 0.0239 - acc: 0.996 - ETA: 0s - loss: 0.0231 - acc: 0.996 - ETA: 0s - loss: 0.0231 - acc: 0.996 - ETA: 0s - loss: 0.0233 - acc: 0.996 - ETA: 0s - loss: 0.0224 - acc: 0.997 - ETA: 0s - loss: 0.0216 - acc: 0.997 - 2s 75ms/step - loss: 0.0231 - acc: 0.9961 - val_loss: 0.0476 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.01934\n",
      "Epoch 110/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0144 - acc: 1.000 - ETA: 1s - loss: 0.0182 - acc: 1.000 - ETA: 1s - loss: 0.0176 - acc: 1.000 - ETA: 1s - loss: 0.0215 - acc: 0.993 - ETA: 1s - loss: 0.0187 - acc: 0.994 - ETA: 1s - loss: 0.0186 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.996 - ETA: 0s - loss: 0.0171 - acc: 0.996 - ETA: 0s - loss: 0.0192 - acc: 0.996 - ETA: 0s - loss: 0.0177 - acc: 0.997 - ETA: 0s - loss: 0.0167 - acc: 0.997 - ETA: 0s - loss: 0.0162 - acc: 0.997 - ETA: 0s - loss: 0.0156 - acc: 0.997 - ETA: 0s - loss: 0.0157 - acc: 0.997 - ETA: 0s - loss: 0.0161 - acc: 0.998 - ETA: 0s - loss: 0.0173 - acc: 0.998 - ETA: 0s - loss: 0.0230 - acc: 0.994 - ETA: 0s - loss: 0.0236 - acc: 0.995 - ETA: 0s - loss: 0.0246 - acc: 0.995 - ETA: 0s - loss: 0.0252 - acc: 0.995 - ETA: 0s - loss: 0.0254 - acc: 0.995 - ETA: 0s - loss: 0.0249 - acc: 0.995 - 2s 69ms/step - loss: 0.0251 - acc: 0.9961 - val_loss: 0.0381 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.01934\n",
      "Epoch 111/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0071 - acc: 1.000 - ETA: 1s - loss: 0.0063 - acc: 1.000 - ETA: 1s - loss: 0.0076 - acc: 1.000 - ETA: 1s - loss: 0.0081 - acc: 1.000 - ETA: 1s - loss: 0.0068 - acc: 1.000 - ETA: 1s - loss: 0.0135 - acc: 0.994 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0165 - acc: 0.996 - ETA: 0s - loss: 0.0166 - acc: 0.996 - ETA: 0s - loss: 0.0163 - acc: 0.996 - ETA: 0s - loss: 0.0163 - acc: 0.997 - ETA: 0s - loss: 0.0169 - acc: 0.997 - ETA: 0s - loss: 0.0188 - acc: 0.997 - ETA: 0s - loss: 0.0249 - acc: 0.995 - ETA: 0s - loss: 0.0248 - acc: 0.995 - ETA: 0s - loss: 0.0275 - acc: 0.994 - ETA: 0s - loss: 0.0262 - acc: 0.994 - ETA: 0s - loss: 0.0272 - acc: 0.993 - ETA: 0s - loss: 0.0262 - acc: 0.993 - ETA: 0s - loss: 0.0255 - acc: 0.993 - ETA: 0s - loss: 0.0254 - acc: 0.994 - ETA: 0s - loss: 0.0261 - acc: 0.994 - ETA: 0s - loss: 0.0251 - acc: 0.994 - 2s 71ms/step - loss: 0.0247 - acc: 0.9948 - val_loss: 0.0316 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.01934\n",
      "Epoch 112/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0324 - acc: 1.000 - ETA: 0s - loss: 0.0290 - acc: 1.000 - ETA: 1s - loss: 0.0354 - acc: 0.992 - ETA: 1s - loss: 0.0335 - acc: 0.993 - ETA: 1s - loss: 0.0292 - acc: 0.994 - ETA: 1s - loss: 0.0257 - acc: 0.995 - ETA: 1s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0269 - acc: 0.993 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.994 - ETA: 0s - loss: 0.0248 - acc: 0.994 - ETA: 0s - loss: 0.0232 - acc: 0.995 - ETA: 0s - loss: 0.0217 - acc: 0.995 - ETA: 0s - loss: 0.0204 - acc: 0.995 - ETA: 0s - loss: 0.0202 - acc: 0.996 - ETA: 0s - loss: 0.0200 - acc: 0.996 - ETA: 0s - loss: 0.0199 - acc: 0.996 - ETA: 0s - loss: 0.0200 - acc: 0.996 - ETA: 0s - loss: 0.0205 - acc: 0.996 - ETA: 0s - loss: 0.0208 - acc: 0.997 - ETA: 0s - loss: 0.0205 - acc: 0.997 - ETA: 0s - loss: 0.0204 - acc: 0.997 - 2s 77ms/step - loss: 0.0204 - acc: 0.9974 - val_loss: 0.0327 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.01934\n",
      "Epoch 113/300\n",
      "24/24 [==============================] - ETA: 1s - loss: 0.0058 - acc: 1.000 - ETA: 1s - loss: 0.0054 - acc: 1.000 - ETA: 1s - loss: 0.0110 - acc: 1.000 - ETA: 1s - loss: 0.0101 - acc: 1.000 - ETA: 1s - loss: 0.0103 - acc: 1.000 - ETA: 1s - loss: 0.0102 - acc: 1.000 - ETA: 1s - loss: 0.0245 - acc: 0.996 - ETA: 1s - loss: 0.0237 - acc: 0.996 - ETA: 1s - loss: 0.0228 - acc: 0.996 - ETA: 0s - loss: 0.0226 - acc: 0.997 - ETA: 0s - loss: 0.0223 - acc: 0.997 - ETA: 0s - loss: 0.0209 - acc: 0.997 - ETA: 0s - loss: 0.0208 - acc: 0.997 - ETA: 0s - loss: 0.0203 - acc: 0.997 - ETA: 0s - loss: 0.0220 - acc: 0.998 - ETA: 0s - loss: 0.0209 - acc: 0.998 - ETA: 0s - loss: 0.0203 - acc: 0.998 - ETA: 0s - loss: 0.0194 - acc: 0.998 - ETA: 0s - loss: 0.0194 - acc: 0.998 - ETA: 0s - loss: 0.0206 - acc: 0.998 - ETA: 0s - loss: 0.0207 - acc: 0.998 - ETA: 0s - loss: 0.0202 - acc: 0.998 - 2s 75ms/step - loss: 0.0202 - acc: 0.9987 - val_loss: 0.0384 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.01934\n",
      "Epoch 114/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0353 - acc: 1.000 - ETA: 1s - loss: 0.0220 - acc: 1.000 - ETA: 1s - loss: 0.0269 - acc: 0.992 - ETA: 1s - loss: 0.0228 - acc: 0.993 - ETA: 1s - loss: 0.0198 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.996 - ETA: 0s - loss: 0.0224 - acc: 0.996 - ETA: 0s - loss: 0.0263 - acc: 0.993 - ETA: 0s - loss: 0.0257 - acc: 0.994 - ETA: 0s - loss: 0.0250 - acc: 0.994 - ETA: 0s - loss: 0.0238 - acc: 0.995 - ETA: 0s - loss: 0.0233 - acc: 0.995 - ETA: 0s - loss: 0.0266 - acc: 0.993 - ETA: 0s - loss: 0.0277 - acc: 0.994 - ETA: 0s - loss: 0.0266 - acc: 0.994 - ETA: 0s - loss: 0.0255 - acc: 0.994 - ETA: 0s - loss: 0.0245 - acc: 0.995 - ETA: 0s - loss: 0.0240 - acc: 0.995 - ETA: 0s - loss: 0.0235 - acc: 0.995 - ETA: 0s - loss: 0.0237 - acc: 0.995 - ETA: 0s - loss: 0.0234 - acc: 0.995 - 2s 65ms/step - loss: 0.0243 - acc: 0.9961 - val_loss: 0.0240 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.01934\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0109 - acc: 1.000 - ETA: 1s - loss: 0.0102 - acc: 1.000 - ETA: 1s - loss: 0.0235 - acc: 0.989 - ETA: 1s - loss: 0.0250 - acc: 0.991 - ETA: 1s - loss: 0.0298 - acc: 0.993 - ETA: 1s - loss: 0.0267 - acc: 0.994 - ETA: 1s - loss: 0.0292 - acc: 0.990 - ETA: 1s - loss: 0.0298 - acc: 0.991 - ETA: 1s - loss: 0.0307 - acc: 0.992 - ETA: 0s - loss: 0.0306 - acc: 0.990 - ETA: 0s - loss: 0.0320 - acc: 0.991 - ETA: 0s - loss: 0.0318 - acc: 0.992 - ETA: 0s - loss: 0.0308 - acc: 0.992 - ETA: 0s - loss: 0.0306 - acc: 0.993 - ETA: 0s - loss: 0.0307 - acc: 0.991 - ETA: 0s - loss: 0.0317 - acc: 0.992 - ETA: 0s - loss: 0.0331 - acc: 0.992 - ETA: 0s - loss: 0.0320 - acc: 0.992 - ETA: 0s - loss: 0.0324 - acc: 0.991 - ETA: 0s - loss: 0.0315 - acc: 0.992 - ETA: 0s - loss: 0.0308 - acc: 0.992 - ETA: 0s - loss: 0.0297 - acc: 0.992 - 2s 72ms/step - loss: 0.0291 - acc: 0.9931 - val_loss: 0.0432 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.01934\n",
      "Epoch 116/300\n",
      "24/24 [==============================] - ETA: 1s - loss: 0.0065 - acc: 1.000 - ETA: 1s - loss: 0.0204 - acc: 0.989 - ETA: 1s - loss: 0.0203 - acc: 0.992 - ETA: 0s - loss: 0.0212 - acc: 0.994 - ETA: 0s - loss: 0.0193 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.996 - ETA: 0s - loss: 0.0208 - acc: 0.996 - ETA: 0s - loss: 0.0239 - acc: 0.996 - ETA: 0s - loss: 0.0238 - acc: 0.997 - ETA: 0s - loss: 0.0231 - acc: 0.997 - ETA: 0s - loss: 0.0216 - acc: 0.997 - ETA: 0s - loss: 0.0226 - acc: 0.997 - ETA: 0s - loss: 0.0225 - acc: 0.997 - ETA: 0s - loss: 0.0246 - acc: 0.998 - ETA: 0s - loss: 0.0236 - acc: 0.998 - ETA: 0s - loss: 0.0232 - acc: 0.998 - ETA: 0s - loss: 0.0222 - acc: 0.998 - ETA: 0s - loss: 0.0213 - acc: 0.998 - ETA: 0s - loss: 0.0211 - acc: 0.998 - ETA: 0s - loss: 0.0207 - acc: 0.998 - ETA: 0s - loss: 0.0204 - acc: 0.998 - 1s 62ms/step - loss: 0.0207 - acc: 0.9987 - val_loss: 0.0394 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.01934\n",
      "Epoch 117/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0884 - acc: 0.968 - ETA: 1s - loss: 0.0329 - acc: 0.989 - ETA: 1s - loss: 0.0420 - acc: 0.984 - ETA: 1s - loss: 0.0407 - acc: 0.987 - ETA: 1s - loss: 0.0374 - acc: 0.989 - ETA: 1s - loss: 0.0345 - acc: 0.991 - ETA: 0s - loss: 0.0313 - acc: 0.992 - ETA: 0s - loss: 0.0297 - acc: 0.993 - ETA: 0s - loss: 0.0286 - acc: 0.993 - ETA: 0s - loss: 0.0270 - acc: 0.994 - ETA: 0s - loss: 0.0270 - acc: 0.994 - ETA: 0s - loss: 0.0283 - acc: 0.992 - ETA: 0s - loss: 0.0285 - acc: 0.993 - ETA: 0s - loss: 0.0280 - acc: 0.993 - ETA: 0s - loss: 0.0275 - acc: 0.994 - ETA: 0s - loss: 0.0262 - acc: 0.994 - ETA: 0s - loss: 0.0265 - acc: 0.994 - ETA: 0s - loss: 0.0255 - acc: 0.995 - ETA: 0s - loss: 0.0254 - acc: 0.995 - ETA: 0s - loss: 0.0252 - acc: 0.995 - ETA: 0s - loss: 0.0251 - acc: 0.995 - ETA: 0s - loss: 0.0248 - acc: 0.995 - 2s 68ms/step - loss: 0.0240 - acc: 0.9961 - val_loss: 0.0410 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.01934\n",
      "Epoch 118/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0056 - acc: 1.000 - ETA: 1s - loss: 0.0083 - acc: 1.000 - ETA: 1s - loss: 0.0115 - acc: 1.000 - ETA: 1s - loss: 0.0371 - acc: 0.992 - ETA: 1s - loss: 0.0320 - acc: 0.993 - ETA: 1s - loss: 0.0327 - acc: 0.994 - ETA: 1s - loss: 0.0344 - acc: 0.992 - ETA: 0s - loss: 0.0313 - acc: 0.993 - ETA: 0s - loss: 0.0299 - acc: 0.993 - ETA: 0s - loss: 0.0274 - acc: 0.994 - ETA: 0s - loss: 0.0260 - acc: 0.994 - ETA: 0s - loss: 0.0246 - acc: 0.995 - ETA: 0s - loss: 0.0245 - acc: 0.995 - ETA: 0s - loss: 0.0237 - acc: 0.995 - ETA: 0s - loss: 0.0230 - acc: 0.996 - ETA: 0s - loss: 0.0237 - acc: 0.996 - ETA: 0s - loss: 0.0230 - acc: 0.996 - ETA: 0s - loss: 0.0240 - acc: 0.995 - ETA: 0s - loss: 0.0231 - acc: 0.995 - ETA: 0s - loss: 0.0232 - acc: 0.995 - ETA: 0s - loss: 0.0229 - acc: 0.995 - ETA: 0s - loss: 0.0221 - acc: 0.995 - 2s 70ms/step - loss: 0.0226 - acc: 0.9961 - val_loss: 0.0498 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.01934\n",
      "Epoch 119/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0049 - acc: 1.000 - ETA: 1s - loss: 0.0216 - acc: 1.000 - ETA: 1s - loss: 0.0183 - acc: 1.000 - ETA: 1s - loss: 0.0162 - acc: 1.000 - ETA: 1s - loss: 0.0179 - acc: 1.000 - ETA: 1s - loss: 0.0166 - acc: 1.000 - ETA: 0s - loss: 0.0175 - acc: 1.000 - ETA: 0s - loss: 0.0175 - acc: 1.000 - ETA: 0s - loss: 0.0166 - acc: 1.000 - ETA: 0s - loss: 0.0204 - acc: 0.997 - ETA: 0s - loss: 0.0197 - acc: 0.997 - ETA: 0s - loss: 0.0189 - acc: 0.997 - ETA: 0s - loss: 0.0181 - acc: 0.997 - ETA: 0s - loss: 0.0183 - acc: 0.998 - ETA: 0s - loss: 0.0182 - acc: 0.998 - ETA: 0s - loss: 0.0179 - acc: 0.998 - ETA: 0s - loss: 0.0181 - acc: 0.998 - ETA: 0s - loss: 0.0180 - acc: 0.998 - ETA: 0s - loss: 0.0183 - acc: 0.998 - ETA: 0s - loss: 0.0182 - acc: 0.998 - ETA: 0s - loss: 0.0210 - acc: 0.997 - 2s 73ms/step - loss: 0.0204 - acc: 0.9974 - val_loss: 0.0465 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.01934\n",
      "Epoch 120/300\n",
      "24/24 [==============================] - ETA: 1s - loss: 0.0198 - acc: 1.000 - ETA: 1s - loss: 0.0138 - acc: 1.000 - ETA: 1s - loss: 0.0203 - acc: 1.000 - ETA: 1s - loss: 0.0188 - acc: 1.000 - ETA: 1s - loss: 0.0209 - acc: 1.000 - ETA: 1s - loss: 0.0189 - acc: 1.000 - ETA: 1s - loss: 0.0190 - acc: 1.000 - ETA: 1s - loss: 0.0182 - acc: 1.000 - ETA: 1s - loss: 0.0165 - acc: 1.000 - ETA: 1s - loss: 0.0169 - acc: 1.000 - ETA: 1s - loss: 0.0164 - acc: 1.000 - ETA: 1s - loss: 0.0176 - acc: 1.000 - ETA: 0s - loss: 0.0172 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0158 - acc: 1.000 - ETA: 0s - loss: 0.0153 - acc: 1.000 - ETA: 0s - loss: 0.0160 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - ETA: 0s - loss: 0.0158 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0155 - acc: 1.000 - ETA: 0s - loss: 0.0156 - acc: 1.000 - ETA: 0s - loss: 0.0156 - acc: 1.000 - 2s 83ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.0354 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.01934\n",
      "Epoch 121/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0112 - acc: 1.000 - ETA: 1s - loss: 0.0168 - acc: 1.000 - ETA: 1s - loss: 0.0149 - acc: 1.000 - ETA: 1s - loss: 0.0137 - acc: 1.000 - ETA: 1s - loss: 0.0139 - acc: 1.000 - ETA: 1s - loss: 0.0138 - acc: 1.000 - ETA: 0s - loss: 0.0138 - acc: 1.000 - ETA: 0s - loss: 0.0138 - acc: 1.000 - ETA: 0s - loss: 0.0175 - acc: 1.000 - ETA: 0s - loss: 0.0187 - acc: 1.000 - ETA: 0s - loss: 0.0183 - acc: 1.000 - ETA: 0s - loss: 0.0175 - acc: 1.000 - ETA: 0s - loss: 0.0176 - acc: 1.000 - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 0s - loss: 0.0164 - acc: 1.000 - ETA: 0s - loss: 0.0163 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0155 - acc: 1.000 - ETA: 0s - loss: 0.0158 - acc: 1.000 - ETA: 0s - loss: 0.0159 - acc: 1.000 - ETA: 0s - loss: 0.0155 - acc: 1.000 - 2s 64ms/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0454 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.01934\n",
      "Epoch 122/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0620 - acc: 0.968 - ETA: 1s - loss: 0.0420 - acc: 0.979 - ETA: 1s - loss: 0.0366 - acc: 0.984 - ETA: 1s - loss: 0.0321 - acc: 0.987 - ETA: 1s - loss: 0.0278 - acc: 0.989 - ETA: 1s - loss: 0.0261 - acc: 0.991 - ETA: 0s - loss: 0.0313 - acc: 0.992 - ETA: 0s - loss: 0.0300 - acc: 0.993 - ETA: 0s - loss: 0.0287 - acc: 0.993 - ETA: 0s - loss: 0.0281 - acc: 0.994 - ETA: 0s - loss: 0.0297 - acc: 0.995 - ETA: 0s - loss: 0.0281 - acc: 0.995 - ETA: 0s - loss: 0.0265 - acc: 0.995 - ETA: 0s - loss: 0.0252 - acc: 0.996 - ETA: 0s - loss: 0.0242 - acc: 0.996 - ETA: 0s - loss: 0.0233 - acc: 0.996 - ETA: 0s - loss: 0.0222 - acc: 0.996 - ETA: 0s - loss: 0.0223 - acc: 0.996 - ETA: 0s - loss: 0.0216 - acc: 0.997 - ETA: 0s - loss: 0.0228 - acc: 0.997 - ETA: 0s - loss: 0.0231 - acc: 0.997 - 2s 64ms/step - loss: 0.0229 - acc: 0.9974 - val_loss: 0.0391 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.01934\n",
      "Epoch 123/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0022 - acc: 1.000 - ETA: 0s - loss: 0.0105 - acc: 1.000 - ETA: 1s - loss: 0.0211 - acc: 1.000 - ETA: 1s - loss: 0.0209 - acc: 1.000 - ETA: 1s - loss: 0.0182 - acc: 1.000 - ETA: 1s - loss: 0.0190 - acc: 1.000 - ETA: 1s - loss: 0.0172 - acc: 1.000 - ETA: 1s - loss: 0.0159 - acc: 1.000 - ETA: 0s - loss: 0.0152 - acc: 1.000 - ETA: 0s - loss: 0.0199 - acc: 0.997 - ETA: 0s - loss: 0.0180 - acc: 0.997 - ETA: 0s - loss: 0.0228 - acc: 0.995 - ETA: 0s - loss: 0.0218 - acc: 0.995 - ETA: 0s - loss: 0.0212 - acc: 0.996 - ETA: 0s - loss: 0.0223 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.994 - ETA: 0s - loss: 0.0241 - acc: 0.993 - ETA: 0s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0242 - acc: 0.992 - ETA: 0s - loss: 0.0234 - acc: 0.992 - ETA: 0s - loss: 0.0226 - acc: 0.993 - 2s 70ms/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.0182 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.01934 to 0.01823, saving model to NN_v5.model\n",
      "Epoch 124/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0121 - acc: 1.000 - ETA: 0s - loss: 0.0168 - acc: 1.000 - ETA: 0s - loss: 0.0187 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0171 - acc: 1.000 - ETA: 0s - loss: 0.0163 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0151 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - ETA: 0s - loss: 0.0190 - acc: 0.997 - ETA: 0s - loss: 0.0182 - acc: 0.997 - ETA: 0s - loss: 0.0176 - acc: 0.997 - ETA: 0s - loss: 0.0174 - acc: 0.998 - ETA: 0s - loss: 0.0174 - acc: 0.998 - ETA: 0s - loss: 0.0169 - acc: 0.998 - ETA: 0s - loss: 0.0164 - acc: 0.998 - ETA: 0s - loss: 0.0164 - acc: 0.998 - ETA: 0s - loss: 0.0172 - acc: 0.998 - ETA: 0s - loss: 0.0167 - acc: 0.998 - ETA: 0s - loss: 0.0178 - acc: 0.998 - 2s 66ms/step - loss: 0.0173 - acc: 0.9987 - val_loss: 0.0607 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.01823\n",
      "Epoch 125/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0085 - acc: 1.000 - ETA: 1s - loss: 0.0076 - acc: 1.000 - ETA: 1s - loss: 0.0065 - acc: 1.000 - ETA: 1s - loss: 0.0112 - acc: 1.000 - ETA: 1s - loss: 0.0143 - acc: 1.000 - ETA: 1s - loss: 0.0140 - acc: 1.000 - ETA: 1s - loss: 0.0148 - acc: 1.000 - ETA: 1s - loss: 0.0149 - acc: 1.000 - ETA: 0s - loss: 0.0145 - acc: 1.000 - ETA: 0s - loss: 0.0136 - acc: 1.000 - ETA: 0s - loss: 0.0130 - acc: 1.000 - ETA: 0s - loss: 0.0133 - acc: 1.000 - ETA: 0s - loss: 0.0159 - acc: 0.997 - ETA: 0s - loss: 0.0152 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0143 - acc: 0.997 - ETA: 0s - loss: 0.0143 - acc: 0.997 - ETA: 0s - loss: 0.0140 - acc: 0.997 - ETA: 0s - loss: 0.0157 - acc: 0.996 - ETA: 0s - loss: 0.0150 - acc: 0.996 - ETA: 0s - loss: 0.0151 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.996 - 2s 72ms/step - loss: 0.0149 - acc: 0.9970 - val_loss: 0.0488 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.01823\n",
      "Epoch 126/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0083 - acc: 1.000 - ETA: 1s - loss: 0.0098 - acc: 1.000 - ETA: 1s - loss: 0.0106 - acc: 1.000 - ETA: 1s - loss: 0.0099 - acc: 1.000 - ETA: 1s - loss: 0.0102 - acc: 1.000 - ETA: 1s - loss: 0.0103 - acc: 1.000 - ETA: 1s - loss: 0.0111 - acc: 1.000 - ETA: 0s - loss: 0.0176 - acc: 0.996 - ETA: 0s - loss: 0.0171 - acc: 0.996 - ETA: 0s - loss: 0.0167 - acc: 0.997 - ETA: 0s - loss: 0.0165 - acc: 0.997 - ETA: 0s - loss: 0.0161 - acc: 0.997 - ETA: 0s - loss: 0.0162 - acc: 0.997 - ETA: 0s - loss: 0.0153 - acc: 0.998 - ETA: 0s - loss: 0.0164 - acc: 0.998 - ETA: 0s - loss: 0.0162 - acc: 0.998 - ETA: 0s - loss: 0.0159 - acc: 0.998 - ETA: 0s - loss: 0.0154 - acc: 0.998 - ETA: 0s - loss: 0.0148 - acc: 0.998 - ETA: 0s - loss: 0.0145 - acc: 0.998 - ETA: 0s - loss: 0.0161 - acc: 0.997 - 2s 64ms/step - loss: 0.0157 - acc: 0.9974 - val_loss: 0.0331 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.01823\n",
      "Epoch 127/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0246 - acc: 1.000 - ETA: 1s - loss: 0.0151 - acc: 1.000 - ETA: 1s - loss: 0.0234 - acc: 0.992 - ETA: 1s - loss: 0.0234 - acc: 0.993 - ETA: 1s - loss: 0.0203 - acc: 0.994 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.996 - ETA: 0s - loss: 0.0159 - acc: 0.996 - ETA: 0s - loss: 0.0149 - acc: 0.996 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0177 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0182 - acc: 0.996 - ETA: 0s - loss: 0.0176 - acc: 0.996 - ETA: 0s - loss: 0.0183 - acc: 0.996 - ETA: 0s - loss: 0.0175 - acc: 0.996 - ETA: 0s - loss: 0.0181 - acc: 0.997 - ETA: 0s - loss: 0.0181 - acc: 0.997 - ETA: 0s - loss: 0.0185 - acc: 0.997 - 2s 66ms/step - loss: 0.0188 - acc: 0.9974 - val_loss: 0.0252 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.01823\n",
      "Epoch 128/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0046 - acc: 1.000 - ETA: 1s - loss: 0.0102 - acc: 1.000 - ETA: 1s - loss: 0.0139 - acc: 1.000 - ETA: 1s - loss: 0.0199 - acc: 0.993 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 1s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.996 - ETA: 0s - loss: 0.0175 - acc: 0.996 - ETA: 0s - loss: 0.0168 - acc: 0.996 - ETA: 0s - loss: 0.0174 - acc: 0.997 - ETA: 0s - loss: 0.0167 - acc: 0.997 - ETA: 0s - loss: 0.0186 - acc: 0.995 - ETA: 0s - loss: 0.0213 - acc: 0.993 - ETA: 0s - loss: 0.0230 - acc: 0.991 - ETA: 0s - loss: 0.0219 - acc: 0.992 - ETA: 0s - loss: 0.0210 - acc: 0.993 - ETA: 0s - loss: 0.0221 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.993 - ETA: 0s - loss: 0.0210 - acc: 0.994 - ETA: 0s - loss: 0.0207 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - 2s 69ms/step - loss: 0.0197 - acc: 0.9948 - val_loss: 0.0333 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01823\n",
      "Epoch 129/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0181 - acc: 1.000 - ETA: 1s - loss: 0.0403 - acc: 1.000 - ETA: 1s - loss: 0.0388 - acc: 1.000 - ETA: 1s - loss: 0.0347 - acc: 1.000 - ETA: 1s - loss: 0.0341 - acc: 1.000 - ETA: 1s - loss: 0.0303 - acc: 1.000 - ETA: 1s - loss: 0.0275 - acc: 1.000 - ETA: 1s - loss: 0.0248 - acc: 1.000 - ETA: 0s - loss: 0.0237 - acc: 1.000 - ETA: 0s - loss: 0.0250 - acc: 0.997 - ETA: 0s - loss: 0.0231 - acc: 0.997 - ETA: 0s - loss: 0.0221 - acc: 0.997 - ETA: 0s - loss: 0.0211 - acc: 0.997 - ETA: 0s - loss: 0.0207 - acc: 0.997 - ETA: 0s - loss: 0.0201 - acc: 0.998 - ETA: 0s - loss: 0.0195 - acc: 0.998 - ETA: 0s - loss: 0.0194 - acc: 0.998 - ETA: 0s - loss: 0.0204 - acc: 0.998 - ETA: 0s - loss: 0.0202 - acc: 0.998 - ETA: 0s - loss: 0.0195 - acc: 0.998 - ETA: 0s - loss: 0.0190 - acc: 0.998 - 2s 76ms/step - loss: 0.0188 - acc: 0.9987 - val_loss: 0.0551 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.01823\n",
      "Epoch 130/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0080 - acc: 1.000 - ETA: 1s - loss: 0.0187 - acc: 0.989 - ETA: 1s - loss: 0.0196 - acc: 0.992 - ETA: 1s - loss: 0.0195 - acc: 0.993 - ETA: 1s - loss: 0.0176 - acc: 0.994 - ETA: 1s - loss: 0.0163 - acc: 0.995 - ETA: 1s - loss: 0.0152 - acc: 0.996 - ETA: 0s - loss: 0.0143 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.997 - ETA: 0s - loss: 0.0137 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - ETA: 0s - loss: 0.0133 - acc: 0.997 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0156 - acc: 0.998 - ETA: 0s - loss: 0.0172 - acc: 0.998 - ETA: 0s - loss: 0.0161 - acc: 0.998 - ETA: 0s - loss: 0.0163 - acc: 0.998 - ETA: 0s - loss: 0.0173 - acc: 0.998 - ETA: 0s - loss: 0.0176 - acc: 0.998 - 2s 66ms/step - loss: 0.0170 - acc: 0.9987 - val_loss: 0.0395 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.01823\n",
      "Epoch 131/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0853 - acc: 0.968 - ETA: 1s - loss: 0.0400 - acc: 0.989 - ETA: 1s - loss: 0.0316 - acc: 0.992 - ETA: 1s - loss: 0.0272 - acc: 0.993 - ETA: 1s - loss: 0.0248 - acc: 0.994 - ETA: 1s - loss: 0.0225 - acc: 0.995 - ETA: 1s - loss: 0.0223 - acc: 0.996 - ETA: 0s - loss: 0.0209 - acc: 0.996 - ETA: 0s - loss: 0.0249 - acc: 0.993 - ETA: 0s - loss: 0.0244 - acc: 0.994 - ETA: 0s - loss: 0.0231 - acc: 0.994 - ETA: 0s - loss: 0.0222 - acc: 0.995 - ETA: 0s - loss: 0.0227 - acc: 0.995 - ETA: 0s - loss: 0.0224 - acc: 0.995 - ETA: 0s - loss: 0.0213 - acc: 0.996 - ETA: 0s - loss: 0.0205 - acc: 0.996 - ETA: 0s - loss: 0.0203 - acc: 0.996 - ETA: 0s - loss: 0.0198 - acc: 0.996 - ETA: 0s - loss: 0.0190 - acc: 0.996 - ETA: 0s - loss: 0.0200 - acc: 0.997 - ETA: 0s - loss: 0.0212 - acc: 0.995 - ETA: 0s - loss: 0.0211 - acc: 0.995 - 2s 68ms/step - loss: 0.0225 - acc: 0.9948 - val_loss: 0.0355 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.01823\n",
      "Epoch 132/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0077 - acc: 1.000 - ETA: 1s - loss: 0.0195 - acc: 1.000 - ETA: 1s - loss: 0.0254 - acc: 0.992 - ETA: 1s - loss: 0.0240 - acc: 0.993 - ETA: 1s - loss: 0.0222 - acc: 0.994 - ETA: 1s - loss: 0.0254 - acc: 0.991 - ETA: 1s - loss: 0.0236 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.989 - ETA: 0s - loss: 0.0257 - acc: 0.990 - ETA: 0s - loss: 0.0238 - acc: 0.991 - ETA: 0s - loss: 0.0224 - acc: 0.992 - ETA: 0s - loss: 0.0211 - acc: 0.992 - ETA: 0s - loss: 0.0198 - acc: 0.993 - ETA: 0s - loss: 0.0190 - acc: 0.993 - ETA: 0s - loss: 0.0194 - acc: 0.994 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0181 - acc: 0.994 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0168 - acc: 0.995 - ETA: 0s - loss: 0.0189 - acc: 0.995 - ETA: 0s - loss: 0.0189 - acc: 0.995 - 2s 66ms/step - loss: 0.0182 - acc: 0.9961 - val_loss: 0.0443 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.01823\n",
      "Epoch 133/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0240 - acc: 1.000 - ETA: 0s - loss: 0.0155 - acc: 1.000 - ETA: 1s - loss: 0.0143 - acc: 1.000 - ETA: 1s - loss: 0.0271 - acc: 0.987 - ETA: 0s - loss: 0.0233 - acc: 0.989 - ETA: 0s - loss: 0.0235 - acc: 0.991 - ETA: 0s - loss: 0.0219 - acc: 0.992 - ETA: 0s - loss: 0.0220 - acc: 0.993 - ETA: 0s - loss: 0.0228 - acc: 0.993 - ETA: 0s - loss: 0.0211 - acc: 0.994 - ETA: 0s - loss: 0.0201 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0184 - acc: 0.995 - ETA: 0s - loss: 0.0236 - acc: 0.995 - ETA: 0s - loss: 0.0223 - acc: 0.996 - ETA: 0s - loss: 0.0215 - acc: 0.996 - ETA: 0s - loss: 0.0231 - acc: 0.994 - ETA: 0s - loss: 0.0224 - acc: 0.995 - ETA: 0s - loss: 0.0222 - acc: 0.995 - ETA: 0s - loss: 0.0223 - acc: 0.995 - ETA: 0s - loss: 0.0216 - acc: 0.995 - 2s 64ms/step - loss: 0.0215 - acc: 0.9961 - val_loss: 0.0270 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.01823\n",
      "Epoch 134/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0113 - acc: 1.000 - ETA: 1s - loss: 0.0173 - acc: 1.000 - ETA: 1s - loss: 0.0134 - acc: 1.000 - ETA: 1s - loss: 0.0154 - acc: 1.000 - ETA: 1s - loss: 0.0161 - acc: 1.000 - ETA: 1s - loss: 0.0162 - acc: 1.000 - ETA: 1s - loss: 0.0151 - acc: 1.000 - ETA: 1s - loss: 0.0154 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0182 - acc: 0.997 - ETA: 0s - loss: 0.0190 - acc: 0.997 - ETA: 0s - loss: 0.0186 - acc: 0.997 - ETA: 0s - loss: 0.0181 - acc: 0.997 - ETA: 0s - loss: 0.0180 - acc: 0.997 - ETA: 0s - loss: 0.0179 - acc: 0.998 - ETA: 0s - loss: 0.0180 - acc: 0.998 - ETA: 0s - loss: 0.0201 - acc: 0.996 - ETA: 0s - loss: 0.0234 - acc: 0.996 - ETA: 0s - loss: 0.0230 - acc: 0.996 - ETA: 0s - loss: 0.0224 - acc: 0.997 - ETA: 0s - loss: 0.0218 - acc: 0.997 - 2s 69ms/step - loss: 0.0207 - acc: 0.9974 - val_loss: 0.0280 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01823\n",
      "Epoch 135/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0040 - acc: 1.000 - ETA: 0s - loss: 0.0074 - acc: 1.000 - ETA: 1s - loss: 0.0087 - acc: 1.000 - ETA: 1s - loss: 0.0102 - acc: 1.000 - ETA: 1s - loss: 0.0104 - acc: 1.000 - ETA: 1s - loss: 0.0100 - acc: 1.000 - ETA: 0s - loss: 0.0105 - acc: 1.000 - ETA: 0s - loss: 0.0115 - acc: 1.000 - ETA: 0s - loss: 0.0128 - acc: 1.000 - ETA: 0s - loss: 0.0146 - acc: 1.000 - ETA: 0s - loss: 0.0179 - acc: 1.000 - ETA: 0s - loss: 0.0180 - acc: 1.000 - ETA: 0s - loss: 0.0171 - acc: 1.000 - ETA: 0s - loss: 0.0181 - acc: 1.000 - ETA: 0s - loss: 0.0173 - acc: 1.000 - ETA: 0s - loss: 0.0165 - acc: 1.000 - ETA: 0s - loss: 0.0174 - acc: 1.000 - ETA: 0s - loss: 0.0179 - acc: 1.000 - ETA: 0s - loss: 0.0176 - acc: 1.000 - ETA: 0s - loss: 0.0178 - acc: 1.000 - ETA: 0s - loss: 0.0171 - acc: 1.000 - ETA: 0s - loss: 0.0165 - acc: 1.000 - 2s 67ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0543 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.01823\n",
      "Epoch 136/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0052 - acc: 1.000 - ETA: 1s - loss: 0.0068 - acc: 1.000 - ETA: 1s - loss: 0.0086 - acc: 1.000 - ETA: 1s - loss: 0.0211 - acc: 0.992 - ETA: 1s - loss: 0.0172 - acc: 0.993 - ETA: 1s - loss: 0.0153 - acc: 0.994 - ETA: 1s - loss: 0.0157 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.996 - ETA: 0s - loss: 0.0171 - acc: 0.996 - ETA: 0s - loss: 0.0212 - acc: 0.993 - ETA: 0s - loss: 0.0218 - acc: 0.994 - ETA: 0s - loss: 0.0207 - acc: 0.994 - ETA: 0s - loss: 0.0213 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.995 - ETA: 0s - loss: 0.0201 - acc: 0.995 - ETA: 0s - loss: 0.0215 - acc: 0.996 - ETA: 0s - loss: 0.0209 - acc: 0.996 - ETA: 0s - loss: 0.0203 - acc: 0.996 - ETA: 0s - loss: 0.0194 - acc: 0.996 - ETA: 0s - loss: 0.0194 - acc: 0.996 - ETA: 0s - loss: 0.0208 - acc: 0.997 - ETA: 0s - loss: 0.0208 - acc: 0.997 - ETA: 0s - loss: 0.0202 - acc: 0.997 - 2s 70ms/step - loss: 0.0206 - acc: 0.9974 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.01823 to 0.00950, saving model to NN_v5.model\n",
      "Epoch 137/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0038 - acc: 1.000 - ETA: 0s - loss: 0.0130 - acc: 1.000 - ETA: 0s - loss: 0.0107 - acc: 1.000 - ETA: 0s - loss: 0.0102 - acc: 1.000 - ETA: 1s - loss: 0.0100 - acc: 1.000 - ETA: 0s - loss: 0.0206 - acc: 0.995 - ETA: 0s - loss: 0.0187 - acc: 0.996 - ETA: 0s - loss: 0.0170 - acc: 0.996 - ETA: 0s - loss: 0.0178 - acc: 0.996 - ETA: 0s - loss: 0.0173 - acc: 0.997 - ETA: 0s - loss: 0.0163 - acc: 0.997 - ETA: 0s - loss: 0.0165 - acc: 0.997 - ETA: 0s - loss: 0.0155 - acc: 0.997 - ETA: 0s - loss: 0.0151 - acc: 0.997 - ETA: 0s - loss: 0.0210 - acc: 0.996 - ETA: 0s - loss: 0.0204 - acc: 0.996 - ETA: 0s - loss: 0.0198 - acc: 0.996 - ETA: 0s - loss: 0.0198 - acc: 0.996 - ETA: 0s - loss: 0.0220 - acc: 0.995 - ETA: 0s - loss: 0.0210 - acc: 0.995 - ETA: 0s - loss: 0.0213 - acc: 0.995 - ETA: 0s - loss: 0.0205 - acc: 0.995 - 2s 64ms/step - loss: 0.0216 - acc: 0.9948 - val_loss: 0.0419 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00950\n",
      "Epoch 138/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0190 - acc: 1.000 - ETA: 0s - loss: 0.0207 - acc: 1.000 - ETA: 1s - loss: 0.0167 - acc: 1.000 - ETA: 1s - loss: 0.0207 - acc: 1.000 - ETA: 1s - loss: 0.0193 - acc: 1.000 - ETA: 0s - loss: 0.0211 - acc: 1.000 - ETA: 0s - loss: 0.0191 - acc: 1.000 - ETA: 0s - loss: 0.0175 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - ETA: 0s - loss: 0.0155 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0154 - acc: 1.000 - ETA: 0s - loss: 0.0185 - acc: 0.997 - ETA: 0s - loss: 0.0185 - acc: 0.998 - ETA: 0s - loss: 0.0179 - acc: 0.998 - ETA: 0s - loss: 0.0172 - acc: 0.998 - ETA: 0s - loss: 0.0192 - acc: 0.998 - ETA: 0s - loss: 0.0187 - acc: 0.998 - ETA: 0s - loss: 0.0182 - acc: 0.998 - ETA: 0s - loss: 0.0198 - acc: 0.997 - ETA: 0s - loss: 0.0196 - acc: 0.997 - 2s 65ms/step - loss: 0.0192 - acc: 0.9974 - val_loss: 0.0455 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00950\n",
      "Epoch 139/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 1s - loss: 0.0134 - acc: 1.000 - ETA: 1s - loss: 0.0122 - acc: 1.000 - ETA: 1s - loss: 0.0127 - acc: 1.000 - ETA: 1s - loss: 0.0113 - acc: 1.000 - ETA: 1s - loss: 0.0105 - acc: 1.000 - ETA: 0s - loss: 0.0137 - acc: 1.000 - ETA: 0s - loss: 0.0136 - acc: 1.000 - ETA: 0s - loss: 0.0139 - acc: 1.000 - ETA: 0s - loss: 0.0141 - acc: 1.000 - ETA: 0s - loss: 0.0162 - acc: 1.000 - ETA: 0s - loss: 0.0153 - acc: 1.000 - ETA: 0s - loss: 0.0165 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.998 - ETA: 0s - loss: 0.0156 - acc: 0.998 - ETA: 0s - loss: 0.0164 - acc: 0.998 - ETA: 0s - loss: 0.0163 - acc: 0.998 - ETA: 0s - loss: 0.0174 - acc: 0.998 - ETA: 0s - loss: 0.0170 - acc: 0.998 - ETA: 0s - loss: 0.0164 - acc: 0.998 - ETA: 0s - loss: 0.0161 - acc: 0.998 - 2s 74ms/step - loss: 0.0157 - acc: 0.9987 - val_loss: 0.0440 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00950\n",
      "Epoch 140/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0050 - acc: 1.000 - ETA: 1s - loss: 0.0127 - acc: 1.000 - ETA: 1s - loss: 0.0142 - acc: 1.000 - ETA: 1s - loss: 0.0133 - acc: 1.000 - ETA: 1s - loss: 0.0143 - acc: 1.000 - ETA: 1s - loss: 0.0127 - acc: 1.000 - ETA: 1s - loss: 0.0129 - acc: 1.000 - ETA: 0s - loss: 0.0120 - acc: 1.000 - ETA: 0s - loss: 0.0112 - acc: 1.000 - ETA: 0s - loss: 0.0163 - acc: 0.997 - ETA: 0s - loss: 0.0159 - acc: 0.997 - ETA: 0s - loss: 0.0153 - acc: 0.997 - ETA: 0s - loss: 0.0144 - acc: 0.997 - ETA: 0s - loss: 0.0138 - acc: 0.997 - ETA: 0s - loss: 0.0137 - acc: 0.998 - ETA: 0s - loss: 0.0132 - acc: 0.998 - ETA: 0s - loss: 0.0133 - acc: 0.998 - ETA: 0s - loss: 0.0131 - acc: 0.998 - ETA: 0s - loss: 0.0132 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0128 - acc: 0.998 - ETA: 0s - loss: 0.0129 - acc: 0.998 - 2s 70ms/step - loss: 0.0128 - acc: 0.9987 - val_loss: 0.0288 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00950\n",
      "Epoch 141/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0111 - acc: 1.000 - ETA: 1s - loss: 0.0098 - acc: 1.000 - ETA: 1s - loss: 0.0080 - acc: 1.000 - ETA: 1s - loss: 0.0091 - acc: 1.000 - ETA: 1s - loss: 0.0095 - acc: 1.000 - ETA: 0s - loss: 0.0098 - acc: 1.000 - ETA: 0s - loss: 0.0099 - acc: 1.000 - ETA: 0s - loss: 0.0091 - acc: 1.000 - ETA: 0s - loss: 0.0093 - acc: 1.000 - ETA: 0s - loss: 0.0095 - acc: 1.000 - ETA: 0s - loss: 0.0092 - acc: 1.000 - ETA: 0s - loss: 0.0116 - acc: 0.997 - ETA: 0s - loss: 0.0112 - acc: 0.997 - ETA: 0s - loss: 0.0111 - acc: 0.998 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0162 - acc: 0.997 - ETA: 0s - loss: 0.0158 - acc: 0.997 - ETA: 0s - loss: 0.0158 - acc: 0.997 - 2s 66ms/step - loss: 0.0152 - acc: 0.9974 - val_loss: 0.0265 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00950\n",
      "Epoch 142/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0144 - acc: 1.000 - ETA: 1s - loss: 0.0481 - acc: 0.979 - ETA: 1s - loss: 0.0371 - acc: 0.984 - ETA: 1s - loss: 0.0309 - acc: 0.987 - ETA: 0s - loss: 0.0328 - acc: 0.991 - ETA: 0s - loss: 0.0292 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.993 - ETA: 0s - loss: 0.0262 - acc: 0.993 - ETA: 0s - loss: 0.0245 - acc: 0.994 - ETA: 0s - loss: 0.0226 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.995 - ETA: 0s - loss: 0.0209 - acc: 0.995 - ETA: 0s - loss: 0.0203 - acc: 0.995 - ETA: 0s - loss: 0.0199 - acc: 0.996 - ETA: 0s - loss: 0.0239 - acc: 0.994 - ETA: 0s - loss: 0.0230 - acc: 0.994 - ETA: 0s - loss: 0.0234 - acc: 0.995 - ETA: 0s - loss: 0.0233 - acc: 0.995 - ETA: 0s - loss: 0.0229 - acc: 0.995 - ETA: 0s - loss: 0.0229 - acc: 0.995 - ETA: 0s - loss: 0.0221 - acc: 0.995 - 2s 65ms/step - loss: 0.0216 - acc: 0.9961 - val_loss: 0.0270 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00950\n",
      "Epoch 143/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0256 - acc: 1.000 - ETA: 1s - loss: 0.0201 - acc: 1.000 - ETA: 1s - loss: 0.0235 - acc: 1.000 - ETA: 1s - loss: 0.0208 - acc: 1.000 - ETA: 1s - loss: 0.0188 - acc: 1.000 - ETA: 0s - loss: 0.0176 - acc: 1.000 - ETA: 0s - loss: 0.0166 - acc: 1.000 - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 0s - loss: 0.0167 - acc: 1.000 - ETA: 0s - loss: 0.0161 - acc: 1.000 - ETA: 0s - loss: 0.0156 - acc: 1.000 - ETA: 0s - loss: 0.0149 - acc: 1.000 - ETA: 0s - loss: 0.0148 - acc: 1.000 - ETA: 0s - loss: 0.0142 - acc: 1.000 - ETA: 0s - loss: 0.0152 - acc: 1.000 - ETA: 0s - loss: 0.0149 - acc: 1.000 - ETA: 0s - loss: 0.0173 - acc: 0.998 - ETA: 0s - loss: 0.0172 - acc: 0.998 - ETA: 0s - loss: 0.0180 - acc: 0.998 - ETA: 0s - loss: 0.0180 - acc: 0.998 - ETA: 0s - loss: 0.0178 - acc: 0.998 - ETA: 0s - loss: 0.0175 - acc: 0.998 - 2s 66ms/step - loss: 0.0172 - acc: 0.9987 - val_loss: 0.0476 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00950\n",
      "Epoch 144/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0076 - acc: 1.000 - ETA: 1s - loss: 0.0071 - acc: 1.000 - ETA: 1s - loss: 0.0079 - acc: 1.000 - ETA: 1s - loss: 0.0103 - acc: 1.000 - ETA: 1s - loss: 0.0091 - acc: 1.000 - ETA: 1s - loss: 0.0086 - acc: 1.000 - ETA: 1s - loss: 0.0128 - acc: 0.995 - ETA: 0s - loss: 0.0141 - acc: 0.996 - ETA: 0s - loss: 0.0139 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.997 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0150 - acc: 0.997 - ETA: 0s - loss: 0.0172 - acc: 0.995 - ETA: 0s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0162 - acc: 0.996 - ETA: 0s - loss: 0.0157 - acc: 0.996 - ETA: 0s - loss: 0.0158 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - 2s 67ms/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00950\n",
      "Epoch 145/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0136 - acc: 1.000 - ETA: 1s - loss: 0.0151 - acc: 1.000 - ETA: 1s - loss: 0.0149 - acc: 1.000 - ETA: 1s - loss: 0.0168 - acc: 1.000 - ETA: 1s - loss: 0.0181 - acc: 1.000 - ETA: 1s - loss: 0.0169 - acc: 1.000 - ETA: 1s - loss: 0.0156 - acc: 1.000 - ETA: 0s - loss: 0.0175 - acc: 1.000 - ETA: 0s - loss: 0.0190 - acc: 0.997 - ETA: 0s - loss: 0.0191 - acc: 0.997 - ETA: 0s - loss: 0.0183 - acc: 0.997 - ETA: 0s - loss: 0.0181 - acc: 0.997 - ETA: 0s - loss: 0.0179 - acc: 0.997 - ETA: 0s - loss: 0.0180 - acc: 0.998 - ETA: 0s - loss: 0.0187 - acc: 0.998 - ETA: 0s - loss: 0.0188 - acc: 0.998 - ETA: 0s - loss: 0.0192 - acc: 0.998 - ETA: 0s - loss: 0.0189 - acc: 0.998 - ETA: 0s - loss: 0.0183 - acc: 0.998 - ETA: 0s - loss: 0.0182 - acc: 0.998 - ETA: 0s - loss: 0.0176 - acc: 0.998 - 2s 65ms/step - loss: 0.0169 - acc: 0.9987 - val_loss: 0.0283 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00950\n",
      "Epoch 146/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0177 - acc: 1.000 - ETA: 1s - loss: 0.0197 - acc: 0.989 - ETA: 1s - loss: 0.0176 - acc: 0.992 - ETA: 1s - loss: 0.0165 - acc: 0.993 - ETA: 1s - loss: 0.0207 - acc: 0.994 - ETA: 1s - loss: 0.0184 - acc: 0.995 - ETA: 0s - loss: 0.0252 - acc: 0.992 - ETA: 0s - loss: 0.0237 - acc: 0.993 - ETA: 0s - loss: 0.0265 - acc: 0.994 - ETA: 0s - loss: 0.0257 - acc: 0.994 - ETA: 0s - loss: 0.0256 - acc: 0.995 - ETA: 0s - loss: 0.0254 - acc: 0.995 - ETA: 0s - loss: 0.0242 - acc: 0.995 - ETA: 0s - loss: 0.0242 - acc: 0.996 - ETA: 0s - loss: 0.0244 - acc: 0.996 - ETA: 0s - loss: 0.0238 - acc: 0.996 - ETA: 0s - loss: 0.0245 - acc: 0.995 - ETA: 0s - loss: 0.0249 - acc: 0.995 - ETA: 0s - loss: 0.0247 - acc: 0.995 - ETA: 0s - loss: 0.0249 - acc: 0.995 - ETA: 0s - loss: 0.0242 - acc: 0.995 - 2s 67ms/step - loss: 0.0233 - acc: 0.9961 - val_loss: 0.0355 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00950\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0024 - acc: 1.000 - ETA: 1s - loss: 0.0206 - acc: 1.000 - ETA: 1s - loss: 0.0196 - acc: 1.000 - ETA: 1s - loss: 0.0179 - acc: 1.000 - ETA: 1s - loss: 0.0223 - acc: 1.000 - ETA: 1s - loss: 0.0294 - acc: 0.995 - ETA: 1s - loss: 0.0281 - acc: 0.996 - ETA: 1s - loss: 0.0292 - acc: 0.996 - ETA: 0s - loss: 0.0316 - acc: 0.993 - ETA: 0s - loss: 0.0332 - acc: 0.988 - ETA: 0s - loss: 0.0326 - acc: 0.989 - ETA: 0s - loss: 0.0352 - acc: 0.988 - ETA: 0s - loss: 0.0336 - acc: 0.988 - ETA: 0s - loss: 0.0357 - acc: 0.987 - ETA: 0s - loss: 0.0346 - acc: 0.988 - ETA: 0s - loss: 0.0423 - acc: 0.985 - ETA: 0s - loss: 0.0418 - acc: 0.984 - ETA: 0s - loss: 0.0400 - acc: 0.985 - ETA: 0s - loss: 0.0383 - acc: 0.986 - ETA: 0s - loss: 0.0368 - acc: 0.986 - ETA: 0s - loss: 0.0366 - acc: 0.987 - 2s 71ms/step - loss: 0.0363 - acc: 0.9879 - val_loss: 0.0455 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00950\n",
      "Epoch 148/300\n",
      "24/24 [==============================] - ETA: 1s - loss: 0.0085 - acc: 1.000 - ETA: 1s - loss: 0.0494 - acc: 0.989 - ETA: 1s - loss: 0.0386 - acc: 0.992 - ETA: 1s - loss: 0.0318 - acc: 0.993 - ETA: 1s - loss: 0.0273 - acc: 0.994 - ETA: 1s - loss: 0.0303 - acc: 0.991 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.993 - ETA: 0s - loss: 0.0264 - acc: 0.993 - ETA: 0s - loss: 0.0252 - acc: 0.994 - ETA: 0s - loss: 0.0246 - acc: 0.995 - ETA: 0s - loss: 0.0245 - acc: 0.995 - ETA: 0s - loss: 0.0234 - acc: 0.995 - ETA: 0s - loss: 0.0225 - acc: 0.996 - ETA: 0s - loss: 0.0225 - acc: 0.996 - ETA: 0s - loss: 0.0221 - acc: 0.996 - ETA: 0s - loss: 0.0224 - acc: 0.996 - ETA: 0s - loss: 0.0215 - acc: 0.996 - ETA: 0s - loss: 0.0225 - acc: 0.997 - ETA: 0s - loss: 0.0230 - acc: 0.997 - ETA: 0s - loss: 0.0223 - acc: 0.997 - 2s 68ms/step - loss: 0.0221 - acc: 0.9974 - val_loss: 0.0434 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00950\n",
      "Epoch 149/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0201 - acc: 1.000 - ETA: 1s - loss: 0.0170 - acc: 1.000 - ETA: 1s - loss: 0.0160 - acc: 1.000 - ETA: 1s - loss: 0.0437 - acc: 0.992 - ETA: 1s - loss: 0.0430 - acc: 0.993 - ETA: 1s - loss: 0.0373 - acc: 0.994 - ETA: 1s - loss: 0.0347 - acc: 0.995 - ETA: 1s - loss: 0.0319 - acc: 0.996 - ETA: 1s - loss: 0.0294 - acc: 0.996 - ETA: 0s - loss: 0.0272 - acc: 0.996 - ETA: 0s - loss: 0.0254 - acc: 0.997 - ETA: 0s - loss: 0.0249 - acc: 0.997 - ETA: 0s - loss: 0.0237 - acc: 0.997 - ETA: 0s - loss: 0.0237 - acc: 0.997 - ETA: 0s - loss: 0.0244 - acc: 0.995 - ETA: 0s - loss: 0.0240 - acc: 0.996 - ETA: 0s - loss: 0.0243 - acc: 0.996 - ETA: 0s - loss: 0.0237 - acc: 0.996 - ETA: 0s - loss: 0.0231 - acc: 0.996 - ETA: 0s - loss: 0.0239 - acc: 0.995 - ETA: 0s - loss: 0.0239 - acc: 0.995 - ETA: 0s - loss: 0.0238 - acc: 0.995 - ETA: 0s - loss: 0.0255 - acc: 0.994 - 2s 74ms/step - loss: 0.0251 - acc: 0.9948 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00950\n",
      "Epoch 150/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0028 - acc: 1.000 - ETA: 1s - loss: 0.0054 - acc: 1.000 - ETA: 1s - loss: 0.0043 - acc: 1.000 - ETA: 1s - loss: 0.0039 - acc: 1.000 - ETA: 1s - loss: 0.0041 - acc: 1.000 - ETA: 1s - loss: 0.0071 - acc: 1.000 - ETA: 1s - loss: 0.0077 - acc: 1.000 - ETA: 1s - loss: 0.0074 - acc: 1.000 - ETA: 1s - loss: 0.0083 - acc: 1.000 - ETA: 0s - loss: 0.0095 - acc: 1.000 - ETA: 0s - loss: 0.0127 - acc: 0.997 - ETA: 0s - loss: 0.0139 - acc: 0.997 - ETA: 0s - loss: 0.0142 - acc: 0.997 - ETA: 0s - loss: 0.0204 - acc: 0.992 - ETA: 0s - loss: 0.0201 - acc: 0.992 - ETA: 0s - loss: 0.0197 - acc: 0.993 - ETA: 0s - loss: 0.0192 - acc: 0.993 - ETA: 0s - loss: 0.0191 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0192 - acc: 0.994 - ETA: 0s - loss: 0.0199 - acc: 0.995 - 2s 73ms/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0207 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00950\n",
      "Epoch 151/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0066 - acc: 1.000 - ETA: 1s - loss: 0.0080 - acc: 1.000 - ETA: 1s - loss: 0.0155 - acc: 1.000 - ETA: 1s - loss: 0.0170 - acc: 1.000 - ETA: 1s - loss: 0.0165 - acc: 1.000 - ETA: 1s - loss: 0.0148 - acc: 1.000 - ETA: 1s - loss: 0.0134 - acc: 1.000 - ETA: 1s - loss: 0.0141 - acc: 1.000 - ETA: 1s - loss: 0.0140 - acc: 1.000 - ETA: 1s - loss: 0.0129 - acc: 1.000 - ETA: 0s - loss: 0.0124 - acc: 1.000 - ETA: 0s - loss: 0.0118 - acc: 1.000 - ETA: 0s - loss: 0.0115 - acc: 1.000 - ETA: 0s - loss: 0.0109 - acc: 1.000 - ETA: 0s - loss: 0.0100 - acc: 1.000 - ETA: 0s - loss: 0.0100 - acc: 1.000 - ETA: 0s - loss: 0.0096 - acc: 1.000 - ETA: 0s - loss: 0.0094 - acc: 1.000 - ETA: 0s - loss: 0.0108 - acc: 1.000 - ETA: 0s - loss: 0.0107 - acc: 1.000 - ETA: 0s - loss: 0.0114 - acc: 1.000 - ETA: 0s - loss: 0.0113 - acc: 1.000 - 2s 79ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0421 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00950\n",
      "Epoch 152/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0596 - acc: 1.000 - ETA: 1s - loss: 0.0356 - acc: 1.000 - ETA: 1s - loss: 0.0337 - acc: 1.000 - ETA: 1s - loss: 0.0321 - acc: 1.000 - ETA: 1s - loss: 0.0276 - acc: 1.000 - ETA: 1s - loss: 0.0238 - acc: 1.000 - ETA: 1s - loss: 0.0213 - acc: 1.000 - ETA: 1s - loss: 0.0199 - acc: 1.000 - ETA: 1s - loss: 0.0187 - acc: 1.000 - ETA: 1s - loss: 0.0175 - acc: 1.000 - ETA: 0s - loss: 0.0170 - acc: 1.000 - ETA: 0s - loss: 0.0157 - acc: 1.000 - ETA: 0s - loss: 0.0147 - acc: 1.000 - ETA: 0s - loss: 0.0144 - acc: 1.000 - ETA: 0s - loss: 0.0143 - acc: 1.000 - ETA: 0s - loss: 0.0145 - acc: 1.000 - ETA: 0s - loss: 0.0142 - acc: 1.000 - ETA: 0s - loss: 0.0137 - acc: 1.000 - ETA: 0s - loss: 0.0146 - acc: 1.000 - ETA: 0s - loss: 0.0144 - acc: 1.000 - ETA: 0s - loss: 0.0138 - acc: 1.000 - ETA: 0s - loss: 0.0152 - acc: 0.998 - 2s 72ms/step - loss: 0.0150 - acc: 0.9987 - val_loss: 0.0300 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00950\n",
      "Epoch 153/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0092 - acc: 1.000 - ETA: 1s - loss: 0.0088 - acc: 1.000 - ETA: 1s - loss: 0.0080 - acc: 1.000 - ETA: 1s - loss: 0.0097 - acc: 1.000 - ETA: 1s - loss: 0.0108 - acc: 1.000 - ETA: 1s - loss: 0.0102 - acc: 1.000 - ETA: 1s - loss: 0.0124 - acc: 1.000 - ETA: 1s - loss: 0.0116 - acc: 1.000 - ETA: 1s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0127 - acc: 0.996 - ETA: 0s - loss: 0.0119 - acc: 0.997 - ETA: 0s - loss: 0.0115 - acc: 0.997 - ETA: 0s - loss: 0.0139 - acc: 0.997 - ETA: 0s - loss: 0.0133 - acc: 0.997 - ETA: 0s - loss: 0.0130 - acc: 0.997 - ETA: 0s - loss: 0.0132 - acc: 0.998 - ETA: 0s - loss: 0.0122 - acc: 0.998 - ETA: 0s - loss: 0.0118 - acc: 0.998 - ETA: 0s - loss: 0.0116 - acc: 0.998 - ETA: 0s - loss: 0.0113 - acc: 0.998 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0120 - acc: 0.997 - 2s 70ms/step - loss: 0.0121 - acc: 0.9974 - val_loss: 0.0341 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00950\n",
      "Epoch 154/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0040 - acc: 1.000 - ETA: 1s - loss: 0.0145 - acc: 1.000 - ETA: 1s - loss: 0.0142 - acc: 1.000 - ETA: 1s - loss: 0.0126 - acc: 1.000 - ETA: 1s - loss: 0.0138 - acc: 1.000 - ETA: 1s - loss: 0.0131 - acc: 1.000 - ETA: 1s - loss: 0.0140 - acc: 1.000 - ETA: 1s - loss: 0.0179 - acc: 0.996 - ETA: 0s - loss: 0.0176 - acc: 0.996 - ETA: 0s - loss: 0.0172 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.997 - ETA: 0s - loss: 0.0152 - acc: 0.997 - ETA: 0s - loss: 0.0145 - acc: 0.997 - ETA: 0s - loss: 0.0139 - acc: 0.997 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0157 - acc: 0.998 - ETA: 0s - loss: 0.0203 - acc: 0.995 - ETA: 0s - loss: 0.0195 - acc: 0.996 - ETA: 0s - loss: 0.0201 - acc: 0.996 - ETA: 0s - loss: 0.0211 - acc: 0.996 - ETA: 0s - loss: 0.0207 - acc: 0.996 - ETA: 0s - loss: 0.0214 - acc: 0.996 - 2s 73ms/step - loss: 0.0209 - acc: 0.9970 - val_loss: 0.0412 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00950\n",
      "Epoch 155/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0067 - acc: 1.000 - ETA: 1s - loss: 0.0095 - acc: 1.000 - ETA: 1s - loss: 0.0078 - acc: 1.000 - ETA: 1s - loss: 0.0097 - acc: 1.000 - ETA: 1s - loss: 0.0140 - acc: 0.994 - ETA: 1s - loss: 0.0142 - acc: 0.995 - ETA: 1s - loss: 0.0143 - acc: 0.996 - ETA: 1s - loss: 0.0153 - acc: 0.996 - ETA: 0s - loss: 0.0152 - acc: 0.996 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0161 - acc: 0.997 - ETA: 0s - loss: 0.0162 - acc: 0.997 - ETA: 0s - loss: 0.0162 - acc: 0.997 - ETA: 0s - loss: 0.0164 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.998 - ETA: 0s - loss: 0.0152 - acc: 0.998 - ETA: 0s - loss: 0.0152 - acc: 0.998 - ETA: 0s - loss: 0.0153 - acc: 0.998 - ETA: 0s - loss: 0.0163 - acc: 0.998 - ETA: 0s - loss: 0.0159 - acc: 0.998 - ETA: 0s - loss: 0.0156 - acc: 0.998 - 2s 66ms/step - loss: 0.0160 - acc: 0.9987 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00950\n",
      "Epoch 156/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0105 - acc: 1.000 - ETA: 1s - loss: 0.0069 - acc: 1.000 - ETA: 1s - loss: 0.0079 - acc: 1.000 - ETA: 1s - loss: 0.0107 - acc: 1.000 - ETA: 1s - loss: 0.0092 - acc: 1.000 - ETA: 1s - loss: 0.0097 - acc: 1.000 - ETA: 0s - loss: 0.0107 - acc: 1.000 - ETA: 0s - loss: 0.0099 - acc: 1.000 - ETA: 0s - loss: 0.0096 - acc: 1.000 - ETA: 0s - loss: 0.0094 - acc: 1.000 - ETA: 0s - loss: 0.0101 - acc: 1.000 - ETA: 0s - loss: 0.0103 - acc: 1.000 - ETA: 0s - loss: 0.0098 - acc: 1.000 - ETA: 0s - loss: 0.0111 - acc: 1.000 - ETA: 0s - loss: 0.0106 - acc: 1.000 - ETA: 0s - loss: 0.0108 - acc: 1.000 - ETA: 0s - loss: 0.0108 - acc: 1.000 - ETA: 0s - loss: 0.0115 - acc: 1.000 - ETA: 0s - loss: 0.0118 - acc: 1.000 - ETA: 0s - loss: 0.0116 - acc: 1.000 - ETA: 0s - loss: 0.0113 - acc: 1.000 - 2s 63ms/step - loss: 0.0121 - acc: 0.9987 - val_loss: 0.0409 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00950\n",
      "Epoch 157/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0076 - acc: 1.000 - ETA: 0s - loss: 0.0270 - acc: 0.989 - ETA: 1s - loss: 0.0227 - acc: 0.992 - ETA: 1s - loss: 0.0191 - acc: 0.993 - ETA: 1s - loss: 0.0194 - acc: 0.994 - ETA: 1s - loss: 0.0214 - acc: 0.991 - ETA: 0s - loss: 0.0220 - acc: 0.992 - ETA: 0s - loss: 0.0236 - acc: 0.993 - ETA: 0s - loss: 0.0224 - acc: 0.993 - ETA: 0s - loss: 0.0237 - acc: 0.994 - ETA: 0s - loss: 0.0229 - acc: 0.994 - ETA: 0s - loss: 0.0218 - acc: 0.995 - ETA: 0s - loss: 0.0207 - acc: 0.995 - ETA: 0s - loss: 0.0197 - acc: 0.995 - ETA: 0s - loss: 0.0191 - acc: 0.996 - ETA: 0s - loss: 0.0182 - acc: 0.996 - ETA: 0s - loss: 0.0177 - acc: 0.996 - ETA: 0s - loss: 0.0192 - acc: 0.995 - ETA: 0s - loss: 0.0185 - acc: 0.995 - ETA: 0s - loss: 0.0185 - acc: 0.995 - ETA: 0s - loss: 0.0179 - acc: 0.995 - 2s 66ms/step - loss: 0.0173 - acc: 0.9961 - val_loss: 0.0350 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00950\n",
      "Epoch 158/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0276 - acc: 1.000 - ETA: 1s - loss: 0.0155 - acc: 1.000 - ETA: 1s - loss: 0.0134 - acc: 1.000 - ETA: 1s - loss: 0.0198 - acc: 0.993 - ETA: 1s - loss: 0.0179 - acc: 0.994 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 1s - loss: 0.0177 - acc: 0.996 - ETA: 0s - loss: 0.0166 - acc: 0.996 - ETA: 0s - loss: 0.0154 - acc: 0.996 - ETA: 0s - loss: 0.0161 - acc: 0.997 - ETA: 0s - loss: 0.0166 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.997 - ETA: 0s - loss: 0.0151 - acc: 0.997 - ETA: 0s - loss: 0.0156 - acc: 0.997 - ETA: 0s - loss: 0.0200 - acc: 0.996 - ETA: 0s - loss: 0.0198 - acc: 0.996 - ETA: 0s - loss: 0.0192 - acc: 0.996 - ETA: 0s - loss: 0.0195 - acc: 0.996 - ETA: 0s - loss: 0.0192 - acc: 0.996 - ETA: 0s - loss: 0.0187 - acc: 0.997 - ETA: 0s - loss: 0.0197 - acc: 0.995 - 2s 67ms/step - loss: 0.0192 - acc: 0.9961 - val_loss: 0.0308 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00950\n",
      "Epoch 159/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0097 - acc: 1.000 - ETA: 1s - loss: 0.0249 - acc: 0.984 - ETA: 1s - loss: 0.0207 - acc: 0.989 - ETA: 1s - loss: 0.0175 - acc: 0.992 - ETA: 1s - loss: 0.0251 - acc: 0.987 - ETA: 1s - loss: 0.0230 - acc: 0.989 - ETA: 0s - loss: 0.0209 - acc: 0.991 - ETA: 0s - loss: 0.0220 - acc: 0.992 - ETA: 0s - loss: 0.0207 - acc: 0.993 - ETA: 0s - loss: 0.0209 - acc: 0.993 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0227 - acc: 0.993 - ETA: 0s - loss: 0.0235 - acc: 0.991 - ETA: 0s - loss: 0.0224 - acc: 0.992 - ETA: 0s - loss: 0.0249 - acc: 0.990 - ETA: 0s - loss: 0.0263 - acc: 0.989 - ETA: 0s - loss: 0.0261 - acc: 0.990 - ETA: 0s - loss: 0.0251 - acc: 0.990 - ETA: 0s - loss: 0.0245 - acc: 0.991 - ETA: 0s - loss: 0.0253 - acc: 0.990 - 1s 62ms/step - loss: 0.0241 - acc: 0.9909 - val_loss: 0.0442 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00950\n",
      "Epoch 160/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0106 - acc: 1.000 - ETA: 0s - loss: 0.0191 - acc: 1.000 - ETA: 1s - loss: 0.0250 - acc: 0.992 - ETA: 1s - loss: 0.0252 - acc: 0.993 - ETA: 0s - loss: 0.0213 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.996 - ETA: 0s - loss: 0.0167 - acc: 0.996 - ETA: 0s - loss: 0.0164 - acc: 0.996 - ETA: 0s - loss: 0.0156 - acc: 0.997 - ETA: 0s - loss: 0.0160 - acc: 0.997 - ETA: 0s - loss: 0.0152 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0144 - acc: 0.997 - ETA: 0s - loss: 0.0136 - acc: 0.998 - ETA: 0s - loss: 0.0130 - acc: 0.998 - ETA: 0s - loss: 0.0140 - acc: 0.998 - ETA: 0s - loss: 0.0139 - acc: 0.998 - ETA: 0s - loss: 0.0139 - acc: 0.998 - ETA: 0s - loss: 0.0138 - acc: 0.998 - ETA: 0s - loss: 0.0144 - acc: 0.998 - ETA: 0s - loss: 0.0145 - acc: 0.998 - 1s 61ms/step - loss: 0.0149 - acc: 0.9987 - val_loss: 0.0381 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00950\n",
      "Epoch 161/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0350 - acc: 1.000 - ETA: 0s - loss: 0.0155 - acc: 1.000 - ETA: 0s - loss: 0.0128 - acc: 1.000 - ETA: 1s - loss: 0.0114 - acc: 1.000 - ETA: 0s - loss: 0.0101 - acc: 1.000 - ETA: 0s - loss: 0.0098 - acc: 1.000 - ETA: 0s - loss: 0.0096 - acc: 1.000 - ETA: 0s - loss: 0.0093 - acc: 1.000 - ETA: 0s - loss: 0.0092 - acc: 1.000 - ETA: 0s - loss: 0.0093 - acc: 1.000 - ETA: 0s - loss: 0.0096 - acc: 1.000 - ETA: 0s - loss: 0.0125 - acc: 0.997 - ETA: 0s - loss: 0.0119 - acc: 0.997 - ETA: 0s - loss: 0.0132 - acc: 0.997 - ETA: 0s - loss: 0.0135 - acc: 0.998 - ETA: 0s - loss: 0.0140 - acc: 0.998 - ETA: 0s - loss: 0.0137 - acc: 0.998 - ETA: 0s - loss: 0.0134 - acc: 0.998 - ETA: 0s - loss: 0.0153 - acc: 0.998 - ETA: 0s - loss: 0.0150 - acc: 0.998 - ETA: 0s - loss: 0.0148 - acc: 0.998 - ETA: 0s - loss: 0.0152 - acc: 0.998 - 1s 62ms/step - loss: 0.0147 - acc: 0.9987 - val_loss: 0.0291 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00950\n",
      "Epoch 162/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0036 - acc: 1.000 - ETA: 0s - loss: 0.0124 - acc: 1.000 - ETA: 0s - loss: 0.0114 - acc: 1.000 - ETA: 0s - loss: 0.0133 - acc: 1.000 - ETA: 0s - loss: 0.0124 - acc: 1.000 - ETA: 0s - loss: 0.0126 - acc: 1.000 - ETA: 0s - loss: 0.0134 - acc: 1.000 - ETA: 0s - loss: 0.0123 - acc: 1.000 - ETA: 0s - loss: 0.0116 - acc: 1.000 - ETA: 0s - loss: 0.0115 - acc: 1.000 - ETA: 0s - loss: 0.0112 - acc: 1.000 - ETA: 0s - loss: 0.0118 - acc: 1.000 - ETA: 0s - loss: 0.0122 - acc: 1.000 - ETA: 0s - loss: 0.0123 - acc: 1.000 - ETA: 0s - loss: 0.0118 - acc: 1.000 - ETA: 0s - loss: 0.0115 - acc: 1.000 - ETA: 0s - loss: 0.0111 - acc: 1.000 - ETA: 0s - loss: 0.0109 - acc: 1.000 - ETA: 0s - loss: 0.0105 - acc: 1.000 - ETA: 0s - loss: 0.0105 - acc: 1.000 - ETA: 0s - loss: 0.0104 - acc: 1.000 - ETA: 0s - loss: 0.0107 - acc: 1.000 - 1s 61ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0354 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00950\n",
      "Epoch 163/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.0169 - acc: 1.000 - ETA: 0s - loss: 0.0099 - acc: 1.000 - ETA: 0s - loss: 0.0091 - acc: 1.000 - ETA: 0s - loss: 0.0155 - acc: 1.000 - ETA: 0s - loss: 0.0149 - acc: 1.000 - ETA: 0s - loss: 0.0142 - acc: 1.000 - ETA: 0s - loss: 0.0150 - acc: 1.000 - ETA: 0s - loss: 0.0156 - acc: 1.000 - ETA: 0s - loss: 0.0158 - acc: 1.000 - ETA: 0s - loss: 0.0145 - acc: 1.000 - ETA: 0s - loss: 0.0154 - acc: 1.000 - ETA: 0s - loss: 0.0148 - acc: 1.000 - ETA: 0s - loss: 0.0165 - acc: 0.997 - ETA: 0s - loss: 0.0161 - acc: 0.997 - ETA: 0s - loss: 0.0162 - acc: 0.998 - ETA: 0s - loss: 0.0153 - acc: 0.998 - ETA: 0s - loss: 0.0158 - acc: 0.998 - ETA: 0s - loss: 0.0182 - acc: 0.996 - ETA: 0s - loss: 0.0175 - acc: 0.996 - ETA: 0s - loss: 0.0174 - acc: 0.997 - ETA: 0s - loss: 0.0169 - acc: 0.997 - ETA: 0s - loss: 0.0166 - acc: 0.997 - 1s 60ms/step - loss: 0.0162 - acc: 0.9974 - val_loss: 0.0394 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00950\n",
      "Epoch 164/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0047 - acc: 1.000 - ETA: 0s - loss: 0.0093 - acc: 1.000 - ETA: 0s - loss: 0.0093 - acc: 1.000 - ETA: 0s - loss: 0.0104 - acc: 1.000 - ETA: 0s - loss: 0.0102 - acc: 1.000 - ETA: 0s - loss: 0.0116 - acc: 1.000 - ETA: 0s - loss: 0.0116 - acc: 1.000 - ETA: 0s - loss: 0.0119 - acc: 1.000 - ETA: 0s - loss: 0.0118 - acc: 1.000 - ETA: 0s - loss: 0.0122 - acc: 1.000 - ETA: 0s - loss: 0.0127 - acc: 1.000 - ETA: 0s - loss: 0.0119 - acc: 1.000 - ETA: 0s - loss: 0.0123 - acc: 1.000 - ETA: 0s - loss: 0.0122 - acc: 1.000 - ETA: 0s - loss: 0.0122 - acc: 1.000 - ETA: 0s - loss: 0.0118 - acc: 1.000 - ETA: 0s - loss: 0.0125 - acc: 1.000 - ETA: 0s - loss: 0.0120 - acc: 1.000 - ETA: 0s - loss: 0.0117 - acc: 1.000 - ETA: 0s - loss: 0.0122 - acc: 1.000 - ETA: 0s - loss: 0.0135 - acc: 0.998 - ETA: 0s - loss: 0.0133 - acc: 0.998 - 1s 61ms/step - loss: 0.0132 - acc: 0.9987 - val_loss: 0.0188 - val_acc: 0.9940\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00950\n",
      "Epoch 165/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0052 - acc: 1.000 - ETA: 1s - loss: 0.0227 - acc: 0.989 - ETA: 0s - loss: 0.0176 - acc: 0.993 - ETA: 0s - loss: 0.0222 - acc: 0.989 - ETA: 0s - loss: 0.0219 - acc: 0.991 - ETA: 0s - loss: 0.0212 - acc: 0.992 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0199 - acc: 0.993 - ETA: 0s - loss: 0.0187 - acc: 0.994 - ETA: 0s - loss: 0.0174 - acc: 0.994 - ETA: 0s - loss: 0.0206 - acc: 0.992 - ETA: 0s - loss: 0.0203 - acc: 0.993 - ETA: 0s - loss: 0.0206 - acc: 0.993 - ETA: 0s - loss: 0.0223 - acc: 0.992 - ETA: 0s - loss: 0.0215 - acc: 0.992 - ETA: 0s - loss: 0.0208 - acc: 0.993 - ETA: 0s - loss: 0.0199 - acc: 0.993 - ETA: 0s - loss: 0.0204 - acc: 0.993 - ETA: 0s - loss: 0.0214 - acc: 0.992 - ETA: 0s - loss: 0.0206 - acc: 0.992 - ETA: 0s - loss: 0.0204 - acc: 0.993 - 1s 61ms/step - loss: 0.0199 - acc: 0.9935 - val_loss: 0.0359 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00950\n",
      "Epoch 166/300\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0091 - acc: 1.000 - ETA: 1s - loss: 0.0072 - acc: 1.000 - ETA: 1s - loss: 0.0110 - acc: 1.000 - ETA: 0s - loss: 0.0186 - acc: 0.993 - ETA: 0s - loss: 0.0166 - acc: 0.994 - ETA: 0s - loss: 0.0155 - acc: 0.994 - ETA: 0s - loss: 0.0280 - acc: 0.988 - ETA: 0s - loss: 0.0257 - acc: 0.989 - ETA: 0s - loss: 0.0250 - acc: 0.990 - ETA: 0s - loss: 0.0250 - acc: 0.991 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0250 - acc: 0.992 - ETA: 0s - loss: 0.0267 - acc: 0.991 - ETA: 0s - loss: 0.0279 - acc: 0.991 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0272 - acc: 0.992 - ETA: 0s - loss: 0.0260 - acc: 0.992 - ETA: 0s - loss: 0.0258 - acc: 0.993 - ETA: 0s - loss: 0.0249 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.993 - ETA: 0s - loss: 0.0240 - acc: 0.994 - 1s 61ms/step - loss: 0.0231 - acc: 0.9944 - val_loss: 0.0532 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00950\n",
      "Epoch 00166: early stopping\n"
     ]
    }
   ],
   "source": [
    "train_history1 = model1.fit_generator(train_generator,\n",
    "                                    steps_per_epoch=int(len(train_img)/32),\n",
    "                                    validation_data=val_generator,\n",
    "                                    validation_steps=int(len(val_img)/32),\n",
    "                                    epochs=300,\n",
    "                                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24be35559e8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_history1.history['acc'], label='Train')\n",
    "plt.plot(train_history1.history['val_acc'], label='Val')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_history1.history['loss'], label='Train')\n",
    "plt.plot(train_history1.history['val_loss'], label='Val')\n",
    "plt.title('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('NN_v5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('datasets/test.csv')\n",
    "test_id = test_df.pop('id')\n",
    "test_numerical = numerical_scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([test_img, test_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(prediction, index=test_id, columns=species_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
